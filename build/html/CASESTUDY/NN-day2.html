

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Autoencoder (AE), Variational Autoencoder (VAE) and Generative Adversarial Network (GAN) &mdash; Spatial Ecology&#39;s code documentation 0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/jupyter-sphinx.css" type="text/css" />
  <link rel="stylesheet" href="../_static/thebelab.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/thebelab-helper.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="LSTM Network" href="NN-day3.html" />
    <link rel="prev" title="Estimating nitrogen concentrations in streams and rivers using NN" href="NN-day1.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Spatial Ecology's code documentation
          

          
            
            <img src="../_static/SE_compact.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">COURSE TRAINERS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../COURSETRAINERS/trainers.html">Spatial Ecology course trainers</a></li>
</ul>
<p class="caption"><span class="caption-text">COURSES AROUND THE WORLD</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_wcsu_02-04_2021.html">Western Connecticut State University 2021</a></li>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_stock_uni_04-05_2021.html">Stockholm University 2021</a></li>
</ul>
<p class="caption"><span class="caption-text">GEO DATA</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../GEODATA/geomorpho90m/geomorpho90m.html">Geomorpho90m: technical documentation</a></li>
</ul>
<p class="caption"><span class="caption-text">LINUX VIRTUAL MACHINE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../VIRTUALMACHINE/00_Setting_Colab_for_for_Spatial_Ecology_course.html">Prepare Colab for Spatial Ecology courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../VIRTUALMACHINE/00_Setting_OSGeoLive_for_for_Spatial_Ecology_course.html">Prepare OSGeoLive for Spatial Ecology courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../VIRTUALMACHINE/00_Setting_OSGeoLive_curso_para_Ecologia_Espacial.html">Prepare OSGeoLive para el curso de ecología espacial</a></li>
</ul>
<p class="caption"><span class="caption-text">WEB SEMINARS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../WEBSEMINAR/webseminar.html">Raster/Vector Processing using GDAL/OGR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WEBSEMINAR/webseminar.html#image-processing-using-pktools">Image Processing using Pktools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WEBSEMINAR/webseminar.html#introduction-to-grass-gis">Introduction to GRASS GIS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WEBSEMINAR/webseminar.html#geocomputation-with-high-performance-computing">GeoComputation with High Performance Computing</a></li>
</ul>
<p class="caption"><span class="caption-text">BASH</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../BASH/03_bashintro_osgeo.html">Linux Operation System as a base for Spatial Ecology Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../BASH/03_bashinter_osgeo.html">Manipulate text files in bash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../BASH/03_bashxargs_osgeo.html">Multi-core bash</a></li>
</ul>
<p class="caption"><span class="caption-text">AWK</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../AWK/30_awk.html">AWK Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">GDAL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../GDAL/01_gdal_osgeo.html">Use GDAL/OGR for raster/vector operations - osgeo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GDAL/01_gdal_colab.html">Use GDAL/OGR for raster/vector operations - colab</a></li>
</ul>
<p class="caption"><span class="caption-text">PKTOOLS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../PKTOOLS/02_pktools_osgeo.html">Use PKTOOLS for raster/vector operations - osgeo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PKTOOLS/02_pktools_colab.html">Use PKTOOLS for raster/vector operations - colab</a></li>
</ul>
<p class="caption"><span class="caption-text">R</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../R/05_R_Intro.html">R Introduction</a></li>
</ul>
<p class="caption"><span class="caption-text">PYTHON</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/01_Python_Intro.html">Introduction to Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/02_Geo_Python.html">Python &amp; GeoComputation</a></li>
</ul>
<p class="caption"><span class="caption-text">GRASS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../GRASS/grass_intro.html">GRASS Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GRASS/grass_hydro.html">Using GRASS for stream-network extraction and basins delineation</a></li>
</ul>
<p class="caption"><span class="caption-text">CASE STUDY</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="101_SDM1_MWood_gecomp.html">SDM1 : Montane woodcreper - Gecomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="101_SDM1_MWood_Rmodel.html">SDM1 : Montane woodcreper - Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="101_SDM2_Vath_Rmodel.html">SDM2 : Varied Thrush - Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="110_manipulate_GSIM.html">Manipulate GSIM files</a></li>
<li class="toctree-l1"><a class="reference internal" href="120_Data_type_GTiff.html">Data type in GTiff</a></li>
<li class="toctree-l1"><a class="reference internal" href="121_temporal_interpolation.html">Temporal interpolation of landsat images</a></li>
<li class="toctree-l1"><a class="reference internal" href="123_DTW.html">Dynamic Time Warping</a></li>
<li class="toctree-l1"><a class="reference internal" href="122_pred_NP.html">Estimating nitrogen concentrations in streams and rivers using RF</a></li>
<li class="toctree-l1"><a class="reference internal" href="NN-day1.html">Estimating nitrogen concentrations in streams and rivers using NN</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Autoencoder (AE), Variational Autoencoder (VAE) and Generative Adversarial Network (GAN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="NN-day3.html">LSTM Network</a></li>
</ul>
<p class="caption"><span class="caption-text">STUDENTSPROJECTS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../STUDENTSPROJECTS/SW2021/Farzad_VahidiMayamey_sw2021_a.html">Calculating landcover distribution &amp; vegetation extraction</a></li>
</ul>
<p class="caption"><span class="caption-text">OUTDOOR</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../OUTDOOR/outdoor_orientering.html">Do not get lost in the wilderness</a></li>
</ul>
<p class="caption"><span class="caption-text">TALKS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../TALKS/intelligent_modelling.html">Intelligent modelling in time and space: combine GeoComputation and Machine Learning for  environmental application.</a></li>
</ul>
<p class="caption"><span class="caption-text">ADMIN</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ADMIN/00_pktools_gdrive_install.html">Install pktools on the gdrive and be able to use from any Colab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ADMIN/video.html">Video tips</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spatial Ecology's code documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Autoencoder (AE), Variational Autoencoder (VAE) and Generative Adversarial Network (GAN)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/CASESTUDY/NN-day2.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Autoencoder-(AE),-Variational-Autoencoder-(VAE)-and-Generative-Adversarial-Network-(GAN)">
<h1>Autoencoder (AE), Variational Autoencoder (VAE) and Generative Adversarial Network (GAN)<a class="headerlink" href="#Autoencoder-(AE),-Variational-Autoencoder-(VAE)-and-Generative-Adversarial-Network-(GAN)" title="Permalink to this headline">¶</a></h1>
<p>Antonio Fonseca</p>
<p>GeoComput &amp; ML</p>
<p>May 25th, 2021</p>
<p>Packages to be installed:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>conda install -c conda-forge umap-learn
pip install phate
conda install -c conda-forge imageio
</pre></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">codecs</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">scipy.io</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span><span class="p">,</span> <span class="n">pdist</span><span class="p">,</span> <span class="n">squareform</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">eigh</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">manifold</span>
<span class="kn">import</span> <span class="nn">phate</span>
<span class="kn">import</span> <span class="nn">umap</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">scprep</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.utils.data.sampler</span> <span class="kn">import</span> <span class="n">SubsetRandomSampler</span><span class="p">,</span><span class="n">RandomSampler</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">softmax</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span><span class="p">,</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
cpu
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Loading the dataset and create dataloaders</span>
<span class="n">mnist_train</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">mnist_test</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

</pre></div>
</div>
</div>
<div class="section" id="Implementing-an-Autoencoder">
<h2>Implementing an Autoencoder<a class="headerlink" href="#Implementing-an-Autoencoder" title="Permalink to this headline">¶</a></h2>
<p>Now that you have a basic neural network set up, we’ll go through the steps of training an autoencoder that can compress the input down to 2 dimensions, and then (attempt to) reconstruct the original image. This will be similar to your previous network with one hidden layer, but with many more. - Fill in the Autoencoder class with a stack of layers of the following shape: 784-1000-500-250-2-250- 500-1000-784 You can make use of the nn.Linear function to automatically manage the creation of
weight and bias parameters. Between each layer, use a tanh activation. - Change the activation function going to the middle (2-dim) layer to linear (keeping the rest as tanh). - Use the sigmoid activation function on the output of the last hidden layer. - Adapt your training function for the autoencoder. Use the same batch size and number of steps (128 and 5000), but use the ADAM optimizer instead of Gradient Descent. Use Mean Squared Error for your reconstruction loss. - After training your
model, plot the 2 dimensional embeddings of 1000 digits, colored by the image labels. - Produce side-by-side plots of one original and reconstructed sample of each digit (0 - 9). You can use the save_image function from torchvision.utils. - Now for something fun: locate the embeddings of two distinct images, and interpolate between them to produce some intermediate point in the latent space. Visualize this point in the 2D embedding. Then, run your decoder on this fabricated “embedding” to see if
it the output looks anything like a handwritten digit. You might try interpolating between and within several different classes.</p>
<p><img alt="drawing" class="no-scaled-link" src="../_images/AE_architecture.png" style="width: 800px;" /></p>
<div class="section" id="Section-1">
<h3>Section 1<a class="headerlink" href="#Section-1" title="Permalink to this headline">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Autoencoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># ... additional layers, plus possible nonlinearities.</span>
        <span class="k">return</span> <span class="n">z</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="c1"># ditto, but in reverse</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">z</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">## Second routine for training and evaluation (using the )</span>
<span class="c1"># Training and Evaluation routines</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is a standard training loop, which leaves some parts to be filled in.</span>
<span class="sd">    INPUT:</span>
<span class="sd">    :param model: an untrained pytorch model</span>
<span class="sd">    :param loss_fn: e.g. Cross Entropy loss of Mean Squared Error.</span>
<span class="sd">    :param optimizer: the model optimizer, initialized with a learning rate.</span>
<span class="sd">    :param training_set: The training data, in a dataloader for easy iteration.</span>
<span class="sd">    :param test_loader: The testing data, in a dataloader for easy iteration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optimizer: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimizer</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">num_epochs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># obviously, this is too many. I don&#39;t know what this author was thinking.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;n. of epochs: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="c1"># loop through each data point in the training set</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>

            <span class="c1"># run the model on the data</span>
            <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;model_input.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

            <span class="c1"># Clear gradients w.r.t. parameters</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span> <span class="c1"># The second output is the latent representation</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;out.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

            <span class="c1"># Calculate the loss</span>
            <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span> <span class="c1"># add an extra dimension to keep CrossEntropy happy.</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">model_input</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loss: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>

            <span class="c1"># Find the gradients of our loss via backpropogation</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># Adjust accordingly with the optimizer</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Give status reports every 100 epochs</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">. Progress: </span><span class="si">{</span><span class="n">epoch</span><span class="o">/</span><span class="n">num_epochs</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s2">%. &quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; Train loss: </span><span class="si">{:.4f}</span><span class="s2">. Test loss: </span><span class="si">{:.4f}</span><span class="s2">. Time: </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">train_loader</span><span class="p">,</span><span class="n">verbose</span><span class="p">),</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">test_loader</span><span class="p">,</span><span class="n">verbose</span><span class="p">),</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)))</span> <span class="c1">#TODO: implement the evaluate function to provide performance statistics during training.</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">evaluation_set</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the given model on the given dataset.</span>
<span class="sd">    Returns the percentage of correct classifications out of total classifications.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># this disables backpropogation, which makes the model run much more quickly.</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">loss_all</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">evaluation_set</span><span class="p">:</span>

            <span class="c1"># run the model on the data</span>
            <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;model_input.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
            <span class="n">out</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">model_input</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;out[:5]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">out</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
            <span class="n">loss_all</span><span class="o">+=</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_all</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">evaluation_set</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Autoencoding-MNIST">
<h2>Autoencoding MNIST<a class="headerlink" href="#Autoencoding-MNIST" title="Permalink to this headline">¶</a></h2>
<p><img alt="drawing" class="no-scaled-link" src="../_images/AE_MNIST.png" style="width: 800px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># hid_dim_range = [128,256,512]</span>
<span class="n">lr_range</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.005</span><span class="p">,</span><span class="mf">0.001</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Autoencoder - with non-linearity (tanh)&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lr_range</span><span class="p">:</span>
    <span class="k">if</span> <span class="s1">&#39;model&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting previous model&#39;</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">ADAM</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span> <span class="c1"># This is absurdly high.</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ADAM</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Autoencoder - with non-linearity (tanh)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
)
n. of epochs: 100
 EPOCH 0. Progress: 0.0%.
 Train loss: 1.0428. Test loss: 1.0423. Time: 10.0248
 EPOCH 10. Progress: 10.0%.
 Train loss: 0.8899. Test loss: 0.8877. Time: 9.6755
 EPOCH 20. Progress: 20.0%.
 Train loss: 0.8899. Test loss: 0.8877. Time: 9.5361
 EPOCH 30. Progress: 30.0%.
 Train loss: 0.8899. Test loss: 0.8877. Time: 9.4851
 EPOCH 40. Progress: 40.0%.
 Train loss: 0.8899. Test loss: 0.8877. Time: 9.6068
 EPOCH 50. Progress: 50.0%.
 Train loss: 0.8899. Test loss: 0.8877. Time: 9.5639
 EPOCH 60. Progress: 60.0%.
 Train loss: 0.8899. Test loss: 0.8877. Time: 9.5320
 EPOCH 70. Progress: 70.0%.
 Train loss: 0.8899. Test loss: 0.8877. Time: 9.5468
 EPOCH 80. Progress: 80.0%.
 Train loss: 0.8899. Test loss: 0.8877. Time: 9.5872
 EPOCH 90. Progress: 90.0%.
 Train loss: 0.8899. Test loss: 0.8877. Time: 9.5837
 EPOCH 100. Progress: 100.0%.
 Train loss: 0.8899. Test loss: 0.8877. Time: 9.5013
Deleting previous model
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 0
)
n. of epochs: 100
 EPOCH 0. Progress: 0.0%.
 Train loss: 1.1027. Test loss: 1.1086. Time: 9.4835
 EPOCH 10. Progress: 10.0%.
 Train loss: 1.1190. Test loss: 1.1250. Time: 9.5613
 EPOCH 20. Progress: 20.0%.
 Train loss: 1.1304. Test loss: 1.1366. Time: 9.3534
 EPOCH 30. Progress: 30.0%.
 Train loss: 1.1336. Test loss: 1.1397. Time: 9.6452
 EPOCH 40. Progress: 40.0%.
 Train loss: 1.1353. Test loss: 1.1414. Time: 9.4842
 EPOCH 50. Progress: 50.0%.
 Train loss: 1.1349. Test loss: 1.1411. Time: 9.4263
 EPOCH 60. Progress: 60.0%.
 Train loss: 1.1353. Test loss: 1.1414. Time: 9.5138
 EPOCH 70. Progress: 70.0%.
 Train loss: 1.1367. Test loss: 1.1428. Time: 9.5135
 EPOCH 80. Progress: 80.0%.
 Train loss: 1.1367. Test loss: 1.1428. Time: 9.5350
 EPOCH 90. Progress: 90.0%.
 Train loss: 1.1363. Test loss: 1.1425. Time: 9.4734
 EPOCH 100. Progress: 100.0%.
 Train loss: 1.1361. Test loss: 1.1422. Time: 9.5480
Deleting previous model
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
n. of epochs: 100
 EPOCH 0. Progress: 0.0%.
 Train loss: 0.0505. Test loss: 0.0503. Time: 9.5554
 EPOCH 10. Progress: 10.0%.
 Train loss: 0.0406. Test loss: 0.0405. Time: 9.4873
 EPOCH 20. Progress: 20.0%.
 Train loss: 0.0389. Test loss: 0.0389. Time: 9.5403
 EPOCH 30. Progress: 30.0%.
 Train loss: 0.0377. Test loss: 0.0378. Time: 9.2778
 EPOCH 40. Progress: 40.0%.
 Train loss: 0.0368. Test loss: 0.0370. Time: 9.5494
 EPOCH 50. Progress: 50.0%.
 Train loss: 0.0368. Test loss: 0.0369. Time: 9.5141
 EPOCH 60. Progress: 60.0%.
 Train loss: 0.0363. Test loss: 0.0366. Time: 9.5806
 EPOCH 70. Progress: 70.0%.
 Train loss: 0.0359. Test loss: 0.0362. Time: 9.5740
 EPOCH 80. Progress: 80.0%.
 Train loss: 0.0356. Test loss: 0.0359. Time: 9.5521
 EPOCH 90. Progress: 90.0%.
 Train loss: 0.0359. Test loss: 0.0362. Time: 9.4949
 EPOCH 100. Progress: 100.0%.
 Train loss: 0.0356. Test loss: 0.0359. Time: 9.5257
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Training for longer with the lr that gave best result</span>
<span class="n">lr_range</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Autoencoder - with non-linearity (tanh)&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lr_range</span><span class="p">:</span>
    <span class="k">if</span> <span class="s1">&#39;model&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting previous model&#39;</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">ADAM</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ADAM</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># # Save the trained model</span>
<span class="c1"># torch.save(model.state_dict(), &#39;./models/model_AE.pt&#39;)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Autoencoder - with non-linearity (tanh)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
n. of epochs: 500
 EPOCH 0. Progress: 0.0%.
 Train loss: 0.0498. Test loss: 0.0500. Time: 9.8799
 EPOCH 10. Progress: 2.0%.
 Train loss: 0.0413. Test loss: 0.0415. Time: 9.5658
 EPOCH 20. Progress: 4.0%.
 Train loss: 0.0390. Test loss: 0.0392. Time: 9.5774
 EPOCH 30. Progress: 6.0%.
 Train loss: 0.0380. Test loss: 0.0381. Time: 9.5850
 EPOCH 40. Progress: 8.0%.
 Train loss: 0.0371. Test loss: 0.0373. Time: 9.5798
 EPOCH 50. Progress: 10.0%.
 Train loss: 0.0368. Test loss: 0.0369. Time: 9.5161
 EPOCH 60. Progress: 12.0%.
 Train loss: 0.0363. Test loss: 0.0365. Time: 9.6184
 EPOCH 70. Progress: 14.000000000000002%.
 Train loss: 0.0359. Test loss: 0.0361. Time: 9.4047
 EPOCH 80. Progress: 16.0%.
 Train loss: 0.0357. Test loss: 0.0359. Time: 9.5648
 EPOCH 90. Progress: 18.0%.
 Train loss: 0.0356. Test loss: 0.0359. Time: 9.5302
 EPOCH 100. Progress: 20.0%.
 Train loss: 0.0355. Test loss: 0.0358. Time: 9.5004
 EPOCH 110. Progress: 22.0%.
 Train loss: 0.0354. Test loss: 0.0358. Time: 9.4044
 EPOCH 120. Progress: 24.0%.
 Train loss: 0.0356. Test loss: 0.0359. Time: 9.4904
 EPOCH 130. Progress: 26.0%.
 Train loss: 0.0350. Test loss: 0.0354. Time: 9.5713
 EPOCH 140. Progress: 28.000000000000004%.
 Train loss: 0.0350. Test loss: 0.0354. Time: 9.5394
 EPOCH 150. Progress: 30.0%.
 Train loss: 0.0350. Test loss: 0.0354. Time: 9.5370
 EPOCH 160. Progress: 32.0%.
 Train loss: 0.0354. Test loss: 0.0357. Time: 9.5828
 EPOCH 170. Progress: 34.0%.
 Train loss: 0.0347. Test loss: 0.0352. Time: 9.2973
 EPOCH 180. Progress: 36.0%.
 Train loss: 0.0346. Test loss: 0.0349. Time: 9.5753
 EPOCH 190. Progress: 38.0%.
 Train loss: 0.0352. Test loss: 0.0356. Time: 9.4743
 EPOCH 200. Progress: 40.0%.
 Train loss: 0.0349. Test loss: 0.0353. Time: 9.5419
 EPOCH 210. Progress: 42.0%.
 Train loss: 0.0345. Test loss: 0.0349. Time: 9.5709
 EPOCH 220. Progress: 44.0%.
 Train loss: 0.0345. Test loss: 0.0349. Time: 9.5569
 EPOCH 230. Progress: 46.0%.
 Train loss: 0.0343. Test loss: 0.0348. Time: 9.2725
 EPOCH 240. Progress: 48.0%.
 Train loss: 0.0343. Test loss: 0.0348. Time: 9.4752
 EPOCH 250. Progress: 50.0%.
 Train loss: 0.0348. Test loss: 0.0352. Time: 9.5952
 EPOCH 260. Progress: 52.0%.
 Train loss: 0.0342. Test loss: 0.0348. Time: 9.5204
 EPOCH 270. Progress: 54.0%.
 Train loss: 0.0342. Test loss: 0.0348. Time: 9.4831
 EPOCH 280. Progress: 56.00000000000001%.
 Train loss: 0.0341. Test loss: 0.0346. Time: 9.4748
 EPOCH 290. Progress: 57.99999999999999%.
 Train loss: 0.0340. Test loss: 0.0347. Time: 9.5644
 EPOCH 300. Progress: 60.0%.
 Train loss: 0.0343. Test loss: 0.0348. Time: 9.2511
 EPOCH 310. Progress: 62.0%.
 Train loss: 0.0340. Test loss: 0.0345. Time: 9.5455
 EPOCH 320. Progress: 64.0%.
 Train loss: 0.0339. Test loss: 0.0345. Time: 9.5771
 EPOCH 330. Progress: 66.0%.
 Train loss: 0.0339. Test loss: 0.0345. Time: 9.5510
 EPOCH 340. Progress: 68.0%.
 Train loss: 0.0343. Test loss: 0.0350. Time: 9.5394
 EPOCH 350. Progress: 70.0%.
 Train loss: 0.0342. Test loss: 0.0348. Time: 9.4558
 EPOCH 360. Progress: 72.0%.
 Train loss: 0.0341. Test loss: 0.0347. Time: 9.4981
 EPOCH 370. Progress: 74.0%.
 Train loss: 0.0337. Test loss: 0.0343. Time: 9.4733
 EPOCH 380. Progress: 76.0%.
 Train loss: 0.0338. Test loss: 0.0344. Time: 9.5276
 EPOCH 390. Progress: 78.0%.
 Train loss: 0.0340. Test loss: 0.0345. Time: 9.6180
 EPOCH 400. Progress: 80.0%.
 Train loss: 0.0339. Test loss: 0.0344. Time: 9.6086
 EPOCH 410. Progress: 82.0%.
 Train loss: 0.0337. Test loss: 0.0344. Time: 9.5417
 EPOCH 420. Progress: 84.0%.
 Train loss: 0.0336. Test loss: 0.0342. Time: 9.4374
 EPOCH 430. Progress: 86.0%.
 Train loss: 0.0336. Test loss: 0.0343. Time: 9.5644
 EPOCH 440. Progress: 88.0%.
 Train loss: 0.0336. Test loss: 0.0343. Time: 9.5794
 EPOCH 450. Progress: 90.0%.
 Train loss: 0.0334. Test loss: 0.0340. Time: 9.5730
 EPOCH 460. Progress: 92.0%.
 Train loss: 0.0337. Test loss: 0.0343. Time: 9.5895
 EPOCH 470. Progress: 94.0%.
 Train loss: 0.0336. Test loss: 0.0343. Time: 9.5377
 EPOCH 480. Progress: 96.0%.
 Train loss: 0.0335. Test loss: 0.0342. Time: 9.4630
 EPOCH 490. Progress: 98.0%.
 Train loss: 0.0340. Test loss: 0.0347. Time: 9.5456
 EPOCH 500. Progress: 100.0%.
 Train loss: 0.0337. Test loss: 0.0344. Time: 9.7685
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load the model</span>
<span class="n">model_AE</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_AE</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;./models/model_AE.pt&#39;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;All keys matched successfully&gt;
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">## Plot the embedding of 1000 digits</span>
<span class="c1"># Test</span>
<span class="n">large_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">large_batch</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;np.unique(targets): </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">targets</span><span class="p">)))</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">latentVar</span> <span class="o">=</span> <span class="n">model_AE</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">latentVar</span> <span class="o">=</span> <span class="n">latentVar</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">targets</span><span class="p">[:])</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

    <span class="n">n_points</span><span class="o">=</span><span class="mi">50</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="nb">range</span><span class="p">(</span><span class="n">n_points</span><span class="p">)):</span>

        <span class="n">label</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="c1"># this is the text</span>
                     <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="c1"># this is the point to label</span>
                     <span class="n">textcoords</span><span class="o">=</span><span class="s2">&quot;offset points&quot;</span><span class="p">,</span> <span class="c1"># how to position the text</span>
                     <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># distance from text to points (x,y)</span>
                     <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span> <span class="c1"># horizontal alignment can be left, right or center</span>

</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
targets.shape: torch.Size([1000])
np.unique(targets): [0 1 2 3 4 5 6 7 8 9]
latentVar.shape: torch.Size([1000, 2])
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_13_1.png" src="../_images/CASESTUDY_NN-day2_13_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">large_batch</span><span class="p">))</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">latentVar</span> <span class="o">=</span> <span class="n">model_AE</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
    <span class="n">latentVar</span> <span class="o">=</span> <span class="n">latentVar</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">count</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">idx1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx2</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)):</span> <span class="c1">#Looking for the digit among the labels</span>
            <span class="k">if</span> <span class="n">idx1</span><span class="o">==</span><span class="n">targets</span><span class="p">[</span><span class="n">idx2</span><span class="p">]:</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">model_input</span><span class="p">[</span><span class="n">idx2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
                <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">idx2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
                <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>
                <span class="k">break</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_14_0.png" src="../_images/CASESTUDY_NN-day2_14_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Interpolate between two images of different classes</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">large_batch</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;np.unique(targets): </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">targets</span><span class="p">)))</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">latentVar</span> <span class="o">=</span> <span class="n">model_AE</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">latentVar</span> <span class="o">=</span> <span class="n">latentVar</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">targets</span><span class="p">[:])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">n_points</span><span class="o">=</span><span class="mi">50</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="nb">range</span><span class="p">(</span><span class="n">n_points</span><span class="p">)):</span>

        <span class="n">label</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="c1"># this is the text</span>
                     <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="c1"># this is the point to label</span>
                     <span class="n">textcoords</span><span class="o">=</span><span class="s2">&quot;offset points&quot;</span><span class="p">,</span> <span class="c1"># how to position the text</span>
                     <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># distance from text to points (x,y)</span>
                     <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span> <span class="c1"># horizontal alignment can be left, right or center</span>

    <span class="c1"># Get the first two points of latentVar</span>
    <span class="n">x0</span><span class="p">,</span><span class="n">y0</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x1</span><span class="p">,</span><span class="n">y1</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">xvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">yvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x0,y0: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">y0</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x1,y1: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;xvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xvals</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;yvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">yvals</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xvals</span><span class="p">[:],</span><span class="n">yvals</span><span class="p">[:],</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
targets.shape: torch.Size([1000])
np.unique(targets): [0 1 2 3 4 5 6 7 8 9]
latentVar.shape: torch.Size([1000, 2])
targets[:20]: [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9]
latentVar[:20]: [[ 0.20221455 -0.6054991 ]
 [ 0.8239314  -0.72916347]
 [ 0.11328156  0.92810714]
 [-0.7525427  -0.9826892 ]
 [-0.5522593   0.19914472]
 [ 0.6784188   0.01163787]
 [-0.8704568  -0.46279663]
 [ 0.0295923  -0.4888633 ]
 [-0.9643847  -0.57120174]
 [-0.04333551  0.6172684 ]
 [ 0.11311392 -0.3921932 ]
 [ 0.33123577 -0.9982074 ]
 [-0.06723223 -0.314809  ]
 [ 0.45514876 -0.17654106]
 [-0.97248226 -0.63170075]
 [-0.56841725  0.92171985]
 [ 0.65297705  0.21242166]
 [-0.50694126 -0.8574053 ]
 [ 0.2849127  -0.20937435]
 [-0.43654057  0.49491724]]
x0,y0: 0.2022145539522171,-0.6054990887641907
x1,y1: 0.8239313960075378,-0.7291634678840637
xvals: [0.20221455 0.2712942  0.34037385 0.4094535  0.47853315 0.5476128
 0.61669245 0.6857721  0.75485175 0.8239314 ]
yvals: [-0.60549909 -0.61923958 -0.63298006 -0.64672055 -0.66046104 -0.67420152
 -0.68794201 -0.70168249 -0.71542298 -0.72916347]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_15_1.png" src="../_images/CASESTUDY_NN-day2_15_1.png" />
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">AE_decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AE_decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span> <span class="o">=</span> <span class="n">model_AE</span><span class="o">.</span><span class="n">dec_lin1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span> <span class="o">=</span> <span class="n">model_AE</span><span class="o">.</span><span class="n">dec_lin2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span> <span class="o">=</span> <span class="n">model_AE</span><span class="o">.</span><span class="n">dec_lin3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span> <span class="o">=</span> <span class="n">model_AE</span><span class="o">.</span><span class="n">dec_lin4</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">z</span><span class="p">):</span>
        <span class="c1"># ditto, but in reverse</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;z: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;z.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Decode the interpolated points across classes</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">count</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xvals</span><span class="p">,</span><span class="n">yvals</span><span class="p">):</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;model_input: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_input</span><span class="p">))</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AE_decoder</span><span class="p">()</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
model_input: tensor([ 0.2022, -0.6055])
z: tensor([ 0.2022, -0.6055])
z.shape: torch.Size([2])
model_input: tensor([ 0.2713, -0.6192])
z: tensor([ 0.2713, -0.6192])
z.shape: torch.Size([2])
model_input: tensor([ 0.3404, -0.6330])
z: tensor([ 0.3404, -0.6330])
z.shape: torch.Size([2])
model_input: tensor([ 0.4095, -0.6467])
z: tensor([ 0.4095, -0.6467])
z.shape: torch.Size([2])
model_input: tensor([ 0.4785, -0.6605])
z: tensor([ 0.4785, -0.6605])
z.shape: torch.Size([2])
model_input: tensor([ 0.5476, -0.6742])
z: tensor([ 0.5476, -0.6742])
z.shape: torch.Size([2])
model_input: tensor([ 0.6167, -0.6879])
z: tensor([ 0.6167, -0.6879])
z.shape: torch.Size([2])
model_input: tensor([ 0.6858, -0.7017])
z: tensor([ 0.6858, -0.7017])
z.shape: torch.Size([2])
model_input: tensor([ 0.7549, -0.7154])
z: tensor([ 0.7549, -0.7154])
z.shape: torch.Size([2])
model_input: tensor([ 0.8239, -0.7292])
z: tensor([ 0.8239, -0.7292])
z.shape: torch.Size([2])
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_17_1.png" src="../_images/CASESTUDY_NN-day2_17_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Interpolate between two images of the same class</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">large_batch</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;np.unique(targets): </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">targets</span><span class="p">)))</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">latentVar</span> <span class="o">=</span> <span class="n">model_AE</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">latentVar</span> <span class="o">=</span> <span class="n">latentVar</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">idx_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">targets</span><span class="o">==</span><span class="mi">6</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Get two &#39;6&#39;.</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">idx_</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">targets</span><span class="p">[:])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">n_points</span><span class="o">=</span><span class="mi">50</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="nb">range</span><span class="p">(</span><span class="n">n_points</span><span class="p">)):</span>

        <span class="n">label</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="c1"># this is the text</span>
                     <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="c1"># this is the point to label</span>
                     <span class="n">textcoords</span><span class="o">=</span><span class="s2">&quot;offset points&quot;</span><span class="p">,</span> <span class="c1"># how to position the text</span>
                     <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># distance from text to points (x,y)</span>
                     <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span> <span class="c1"># horizontal alignment can be left, right or center</span>

    <span class="c1"># Get the first two points of latentVar</span>
    <span class="n">x0</span><span class="p">,</span><span class="n">y0</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x1</span><span class="p">,</span><span class="n">y1</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">xvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">yvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x0,y0: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">y0</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x1,y1: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;xvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xvals</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;yvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">yvals</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xvals</span><span class="p">[:],</span><span class="n">yvals</span><span class="p">[:],</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
targets.shape: torch.Size([1000])
np.unique(targets): [0 1 2 3 4 5 6 7 8 9]
latentVar.shape: torch.Size([1000, 2])
[13 18]
targets[:20]: [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9]
latentVar[:20]: [[ 0.20221455 -0.6054991 ]
 [ 0.8239314  -0.72916347]
 [ 0.11328156  0.92810714]
 [-0.7525427  -0.9826892 ]
 [-0.5522593   0.19914472]
 [ 0.6784188   0.01163787]
 [-0.8704568  -0.46279663]
 [ 0.0295923  -0.4888633 ]
 [-0.9643847  -0.57120174]
 [-0.04333551  0.6172684 ]
 [ 0.11311392 -0.3921932 ]
 [ 0.33123577 -0.9982074 ]
 [-0.06723223 -0.314809  ]
 [ 0.45514876 -0.17654106]
 [-0.97248226 -0.63170075]
 [-0.56841725  0.92171985]
 [ 0.65297705  0.21242166]
 [-0.50694126 -0.8574053 ]
 [ 0.2849127  -0.20937435]
 [-0.43654057  0.49491724]]
x0,y0: 0.45514875650405884,-0.1765410602092743
x1,y1: 0.28491270542144775,-0.2093743532896042
xvals: [0.45514876 0.43623364 0.41731852 0.39840341 0.37948829 0.36057317
 0.34165806 0.32274294 0.30382782 0.28491271]
yvals: [-0.17654106 -0.1801892  -0.18383735 -0.18748549 -0.19113363 -0.19478178
 -0.19842992 -0.20207807 -0.20572621 -0.20937435]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_18_1.png" src="../_images/CASESTUDY_NN-day2_18_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">count</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xvals</span><span class="p">,</span><span class="n">yvals</span><span class="p">):</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;model_input: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_input</span><span class="p">))</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AE_decoder</span><span class="p">()</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
model_input: tensor([ 0.4551, -0.1765])
z: tensor([ 0.4551, -0.1765])
z.shape: torch.Size([2])
model_input: tensor([ 0.4362, -0.1802])
z: tensor([ 0.4362, -0.1802])
z.shape: torch.Size([2])
model_input: tensor([ 0.4173, -0.1838])
z: tensor([ 0.4173, -0.1838])
z.shape: torch.Size([2])
model_input: tensor([ 0.3984, -0.1875])
z: tensor([ 0.3984, -0.1875])
z.shape: torch.Size([2])
model_input: tensor([ 0.3795, -0.1911])
z: tensor([ 0.3795, -0.1911])
z.shape: torch.Size([2])
model_input: tensor([ 0.3606, -0.1948])
z: tensor([ 0.3606, -0.1948])
z.shape: torch.Size([2])
model_input: tensor([ 0.3417, -0.1984])
z: tensor([ 0.3417, -0.1984])
z.shape: torch.Size([2])
model_input: tensor([ 0.3227, -0.2021])
z: tensor([ 0.3227, -0.2021])
z.shape: torch.Size([2])
model_input: tensor([ 0.3038, -0.2057])
z: tensor([ 0.3038, -0.2057])
z.shape: torch.Size([2])
model_input: tensor([ 0.2849, -0.2094])
z: tensor([ 0.2849, -0.2094])
z.shape: torch.Size([2])
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_19_1.png" src="../_images/CASESTUDY_NN-day2_19_1.png" />
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Autoencoder - with linear activation in middle layer and non-linearity (tanh) everywhere else</span>
<span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Autoencoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">z</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="c1"># ditto, but in reverse</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">z</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">lr_range</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.005</span><span class="p">,</span><span class="mf">0.001</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Autoencoder - with linear activation in middle layer and non-linearity (tanh) everywhere else&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lr_range</span><span class="p">:</span>
    <span class="k">if</span> <span class="s1">&#39;model&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting previous model&#39;</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">ADAM</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ADAM</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Autoencoder - with linear activation in middle layer and non-linearity (tanh) everywhere else
Deleting previous model
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
)
n. of epochs: 100
 EPOCH 0. Progress: 0.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.4712
 EPOCH 10. Progress: 10.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.4638
 EPOCH 20. Progress: 20.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.5027
 EPOCH 30. Progress: 30.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.4711
 EPOCH 40. Progress: 40.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.4828
 EPOCH 50. Progress: 50.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.4912
 EPOCH 60. Progress: 60.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.5242
 EPOCH 70. Progress: 70.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.4808
 EPOCH 80. Progress: 80.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.5575
 EPOCH 90. Progress: 90.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.3911
 EPOCH 100. Progress: 100.0%.
 Train loss: 0.8617. Test loss: 0.8591. Time: 9.4259
Deleting previous model
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 0
)
n. of epochs: 100
 EPOCH 0. Progress: 0.0%.
 Train loss: 0.7290. Test loss: 0.7264. Time: 9.5176
 EPOCH 10. Progress: 10.0%.
 Train loss: 0.7941. Test loss: 0.7924. Time: 9.3252
 EPOCH 20. Progress: 20.0%.
 Train loss: 0.8117. Test loss: 0.8105. Time: 9.4485
 EPOCH 30. Progress: 30.0%.
 Train loss: 0.8164. Test loss: 0.8154. Time: 9.5031
 EPOCH 40. Progress: 40.0%.
 Train loss: 0.8180. Test loss: 0.8169. Time: 9.5295
 EPOCH 50. Progress: 50.0%.
 Train loss: 0.8197. Test loss: 0.8186. Time: 9.2573
 EPOCH 60. Progress: 60.0%.
 Train loss: 0.8220. Test loss: 0.8209. Time: 9.4746
 EPOCH 70. Progress: 70.0%.
 Train loss: 0.8244. Test loss: 0.8232. Time: 9.2388
 EPOCH 80. Progress: 80.0%.
 Train loss: 0.8251. Test loss: 0.8240. Time: 9.5341
 EPOCH 90. Progress: 90.0%.
 Train loss: 0.8253. Test loss: 0.8241. Time: 9.5734
 EPOCH 100. Progress: 100.0%.
 Train loss: 0.8251. Test loss: 0.8240. Time: 9.5021
Deleting previous model
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
n. of epochs: 100
 EPOCH 0. Progress: 0.0%.
 Train loss: 0.0465. Test loss: 0.0464. Time: 9.5317
 EPOCH 10. Progress: 10.0%.
 Train loss: 0.0375. Test loss: 0.0377. Time: 9.4616
 EPOCH 20. Progress: 20.0%.
 Train loss: 0.0356. Test loss: 0.0360. Time: 9.4783
 EPOCH 30. Progress: 30.0%.
 Train loss: 0.0352. Test loss: 0.0357. Time: 9.5708
 EPOCH 40. Progress: 40.0%.
 Train loss: 0.0347. Test loss: 0.0351. Time: 9.5585
 EPOCH 50. Progress: 50.0%.
 Train loss: 0.0341. Test loss: 0.0348. Time: 9.5080
 EPOCH 60. Progress: 60.0%.
 Train loss: 0.0339. Test loss: 0.0347. Time: 9.5499
 EPOCH 70. Progress: 70.0%.
 Train loss: 0.0335. Test loss: 0.0343. Time: 9.4573
 EPOCH 80. Progress: 80.0%.
 Train loss: 0.0336. Test loss: 0.0346. Time: 9.5612
 EPOCH 90. Progress: 90.0%.
 Train loss: 0.0332. Test loss: 0.0341. Time: 9.4910
 EPOCH 100. Progress: 100.0%.
 Train loss: 0.0331. Test loss: 0.0340. Time: 9.5090
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Training for longer with the lr that gave best result</span>
<span class="n">lr_range</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Autoencoder - with linear activation in middle layer and non-linearity (tanh) everywhere else&#39;</span><span class="p">)</span>
<span class="c1"># for hid_dim in hid_dim_range:</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lr_range</span><span class="p">:</span>
    <span class="k">if</span> <span class="s1">&#39;model&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting previous model&#39;</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">ADAM</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span>
    <span class="c1"># initialize the loss function. You don&#39;t want to use this one, so change it accordingly</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ADAM</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># # Save the trained model</span>
<span class="c1"># torch.save(model.state_dict(), &#39;./models/model_AE_linear.pt&#39;)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Autoencoder - with linear activation in middle layer and non-linearity (tanh) everywhere else
Deleting previous model
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
n. of epochs: 500
 EPOCH 0. Progress: 0.0%.
 Train loss: 0.0462. Test loss: 0.0461. Time: 9.2585
 EPOCH 10. Progress: 2.0%.
 Train loss: 0.0375. Test loss: 0.0376. Time: 9.4987
 EPOCH 20. Progress: 4.0%.
 Train loss: 0.0358. Test loss: 0.0362. Time: 9.7496
 EPOCH 30. Progress: 6.0%.
 Train loss: 0.0355. Test loss: 0.0358. Time: 9.4185
 EPOCH 40. Progress: 8.0%.
 Train loss: 0.0346. Test loss: 0.0352. Time: 9.5354
 EPOCH 50. Progress: 10.0%.
 Train loss: 0.0342. Test loss: 0.0346. Time: 9.4688
 EPOCH 60. Progress: 12.0%.
 Train loss: 0.0341. Test loss: 0.0346. Time: 9.3935
 EPOCH 70. Progress: 14.000000000000002%.
 Train loss: 0.0340. Test loss: 0.0346. Time: 9.5204
 EPOCH 80. Progress: 16.0%.
 Train loss: 0.0337. Test loss: 0.0343. Time: 9.5370
 EPOCH 90. Progress: 18.0%.
 Train loss: 0.0334. Test loss: 0.0341. Time: 9.5833
 EPOCH 100. Progress: 20.0%.
 Train loss: 0.0335. Test loss: 0.0342. Time: 9.4750
 EPOCH 110. Progress: 22.0%.
 Train loss: 0.0332. Test loss: 0.0340. Time: 9.5390
 EPOCH 120. Progress: 24.0%.
 Train loss: 0.0330. Test loss: 0.0338. Time: 9.5864
 EPOCH 130. Progress: 26.0%.
 Train loss: 0.0328. Test loss: 0.0336. Time: 9.5849
 EPOCH 140. Progress: 28.000000000000004%.
 Train loss: 0.0328. Test loss: 0.0336. Time: 9.5257
 EPOCH 150. Progress: 30.0%.
 Train loss: 0.0327. Test loss: 0.0335. Time: 9.4052
 EPOCH 160. Progress: 32.0%.
 Train loss: 0.0325. Test loss: 0.0334. Time: 9.5780
 EPOCH 170. Progress: 34.0%.
 Train loss: 0.0324. Test loss: 0.0333. Time: 9.5659
 EPOCH 180. Progress: 36.0%.
 Train loss: 0.0325. Test loss: 0.0332. Time: 9.4000
 EPOCH 190. Progress: 38.0%.
 Train loss: 0.0324. Test loss: 0.0334. Time: 9.5419
 EPOCH 200. Progress: 40.0%.
 Train loss: 0.0326. Test loss: 0.0335. Time: 9.6191
 EPOCH 210. Progress: 42.0%.
 Train loss: 0.0322. Test loss: 0.0331. Time: 9.5465
 EPOCH 220. Progress: 44.0%.
 Train loss: 0.0323. Test loss: 0.0333. Time: 9.5437
 EPOCH 230. Progress: 46.0%.
 Train loss: 0.0325. Test loss: 0.0335. Time: 9.5047
 EPOCH 240. Progress: 48.0%.
 Train loss: 0.0324. Test loss: 0.0332. Time: 9.5270
 EPOCH 250. Progress: 50.0%.
 Train loss: 0.0322. Test loss: 0.0332. Time: 9.2672
 EPOCH 260. Progress: 52.0%.
 Train loss: 0.0320. Test loss: 0.0331. Time: 9.4788
 EPOCH 270. Progress: 54.0%.
 Train loss: 0.0320. Test loss: 0.0331. Time: 9.3130
 EPOCH 280. Progress: 56.00000000000001%.
 Train loss: 0.0322. Test loss: 0.0331. Time: 9.5173
 EPOCH 290. Progress: 57.99999999999999%.
 Train loss: 0.0319. Test loss: 0.0329. Time: 9.4054
 EPOCH 300. Progress: 60.0%.
 Train loss: 0.0318. Test loss: 0.0329. Time: 9.2717
 EPOCH 310. Progress: 62.0%.
 Train loss: 0.0319. Test loss: 0.0330. Time: 9.4390
 EPOCH 320. Progress: 64.0%.
 Train loss: 0.0319. Test loss: 0.0330. Time: 9.6270
 EPOCH 330. Progress: 66.0%.
 Train loss: 0.0319. Test loss: 0.0329. Time: 9.4734
 EPOCH 340. Progress: 68.0%.
 Train loss: 0.0318. Test loss: 0.0329. Time: 9.5367
 EPOCH 350. Progress: 70.0%.
 Train loss: 0.0318. Test loss: 0.0329. Time: 9.4834
 EPOCH 360. Progress: 72.0%.
 Train loss: 0.0318. Test loss: 0.0329. Time: 9.5595
 EPOCH 370. Progress: 74.0%.
 Train loss: 0.0319. Test loss: 0.0331. Time: 9.5697
 EPOCH 380. Progress: 76.0%.
 Train loss: 0.0314. Test loss: 0.0325. Time: 9.2790
 EPOCH 390. Progress: 78.0%.
 Train loss: 0.0317. Test loss: 0.0328. Time: 9.5473
 EPOCH 400. Progress: 80.0%.
 Train loss: 0.0316. Test loss: 0.0328. Time: 9.4116
 EPOCH 410. Progress: 82.0%.
 Train loss: 0.0317. Test loss: 0.0329. Time: 9.2364
 EPOCH 420. Progress: 84.0%.
 Train loss: 0.0317. Test loss: 0.0329. Time: 9.5650
 EPOCH 430. Progress: 86.0%.
 Train loss: 0.0316. Test loss: 0.0328. Time: 9.4485
 EPOCH 440. Progress: 88.0%.
 Train loss: 0.0314. Test loss: 0.0326. Time: 9.6630
 EPOCH 450. Progress: 90.0%.
 Train loss: 0.0316. Test loss: 0.0328. Time: 9.5375
 EPOCH 460. Progress: 92.0%.
 Train loss: 0.0316. Test loss: 0.0328. Time: 9.2494
 EPOCH 470. Progress: 94.0%.
 Train loss: 0.0312. Test loss: 0.0325. Time: 9.5075
 EPOCH 480. Progress: 96.0%.
 Train loss: 0.0315. Test loss: 0.0327. Time: 9.5423
 EPOCH 490. Progress: 98.0%.
 Train loss: 0.0317. Test loss: 0.0327. Time: 9.4586
 EPOCH 500. Progress: 100.0%.
 Train loss: 0.0312. Test loss: 0.0324. Time: 9.5656
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load the model</span>
<span class="n">model_AE_linear</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_AE_linear</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;./models/model_AE_linear.pt&#39;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;All keys matched successfully&gt;
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">AE_decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s2">&quot;Bert Model : Masked LM and next sentence classification&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AE_decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span> <span class="o">=</span> <span class="n">model_AE_linear</span><span class="o">.</span><span class="n">dec_lin1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span> <span class="o">=</span> <span class="n">model_AE_linear</span><span class="o">.</span><span class="n">dec_lin2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span> <span class="o">=</span> <span class="n">model_AE_linear</span><span class="o">.</span><span class="n">dec_lin3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span> <span class="o">=</span> <span class="n">model_AE_linear</span><span class="o">.</span><span class="n">dec_lin4</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">z</span><span class="p">):</span>
        <span class="c1"># ditto, but in reverse</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;z: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;z.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">large_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">large_batch</span><span class="p">))</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">latentVar</span> <span class="o">=</span> <span class="n">model_AE_linear</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
<span class="c1">#     print(&#39;latentVar.shape: {}&#39;.format(latentVar.shape))</span>
    <span class="n">latentVar</span> <span class="o">=</span> <span class="n">latentVar</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">count</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">idx1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx2</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)):</span> <span class="c1">#Looking for the digit among the labels</span>
            <span class="k">if</span> <span class="n">idx1</span><span class="o">==</span><span class="n">targets</span><span class="p">[</span><span class="n">idx2</span><span class="p">]:</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">model_input</span><span class="p">[</span><span class="n">idx2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
                <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">idx2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
                <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>
                <span class="k">break</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_25_0.png" src="../_images/CASESTUDY_NN-day2_25_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Interpolate between two images of the same class</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">large_batch</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;np.unique(targets): </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">targets</span><span class="p">)))</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">latentVar</span> <span class="o">=</span> <span class="n">model_AE_linear</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">latentVar</span> <span class="o">=</span> <span class="n">latentVar</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">idx_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">targets</span><span class="o">==</span><span class="mi">6</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Get two &#39;6&#39;.</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">idx_</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">targets</span><span class="p">[:])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">n_points</span><span class="o">=</span><span class="mi">50</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="nb">range</span><span class="p">(</span><span class="n">n_points</span><span class="p">)):</span>

        <span class="n">label</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="c1"># this is the text</span>
                     <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="c1"># this is the point to label</span>
                     <span class="n">textcoords</span><span class="o">=</span><span class="s2">&quot;offset points&quot;</span><span class="p">,</span> <span class="c1"># how to position the text</span>
                     <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># distance from text to points (x,y)</span>
                     <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span> <span class="c1"># horizontal alignment can be left, right or center</span>

    <span class="c1"># Get the first two points of latentVar</span>
    <span class="n">x0</span><span class="p">,</span><span class="n">y0</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x1</span><span class="p">,</span><span class="n">y1</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">xvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">yvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x0,y0: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">y0</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x1,y1: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;xvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xvals</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;yvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">yvals</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xvals</span><span class="p">[:],</span><span class="n">yvals</span><span class="p">[:],</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
targets.shape: torch.Size([1000])
np.unique(targets): [0 1 2 3 4 5 6 7 8 9]
latentVar.shape: torch.Size([1000, 2])
[13 18]
targets[:20]: [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9]
latentVar[:20]: [[  0.88656074   1.5480746 ]
 [ 16.731844    -7.276616  ]
 [ -2.4016178   -6.888706  ]
 [-18.124239    19.383167  ]
 [ -7.1650925   -8.911613  ]
 [  0.5757653   23.241972  ]
 [-15.745467     6.396731  ]
 [  2.3958318    1.0412636 ]
 [-21.963514    12.419073  ]
 [ -9.068372    -1.1200742 ]
 [  3.6217616    2.202012  ]
 [ -0.54072136   0.3420033 ]
 [  4.266302     5.598924  ]
 [ 10.783974    -9.3472805 ]
 [-21.905462    10.90397   ]
 [-13.451579   -10.401498  ]
 [ -1.0420251   20.48603   ]
 [ -2.6018767    2.636466  ]
 [ 12.088468     0.17695215]
 [ -6.264395    -0.39840513]]
x0,y0: 10.783973693847656,-9.347280502319336
x1,y1: 12.088467597961426,0.17695215344429016
xvals: [10.78397369 10.92891746 11.07386123 11.218805   11.36374876 11.50869253
 11.6536363  11.79858006 11.94352383 12.0884676 ]
yvals: [-9.3472805  -8.28903243 -7.23078436 -6.17253628 -5.11428821 -4.05604014
 -2.99779207 -1.93954399 -0.88129592  0.17695215]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_26_1.png" src="../_images/CASESTUDY_NN-day2_26_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">count</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xvals</span><span class="p">,</span><span class="n">yvals</span><span class="p">):</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;model_input: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_input</span><span class="p">))</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AE_decoder</span><span class="p">()</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
model_input: tensor([10.7840, -9.3473])
z: tensor([10.7840, -9.3473])
z.shape: torch.Size([2])
model_input: tensor([10.9289, -8.2890])
z: tensor([10.9289, -8.2890])
z.shape: torch.Size([2])
model_input: tensor([11.0739, -7.2308])
z: tensor([11.0739, -7.2308])
z.shape: torch.Size([2])
model_input: tensor([11.2188, -6.1725])
z: tensor([11.2188, -6.1725])
z.shape: torch.Size([2])
model_input: tensor([11.3637, -5.1143])
z: tensor([11.3637, -5.1143])
z.shape: torch.Size([2])
model_input: tensor([11.5087, -4.0560])
z: tensor([11.5087, -4.0560])
z.shape: torch.Size([2])
model_input: tensor([11.6536, -2.9978])
z: tensor([11.6536, -2.9978])
z.shape: torch.Size([2])
model_input: tensor([11.7986, -1.9395])
z: tensor([11.7986, -1.9395])
z.shape: torch.Size([2])
model_input: tensor([11.9435, -0.8813])
z: tensor([11.9435, -0.8813])
z.shape: torch.Size([2])
model_input: tensor([12.0885,  0.1770])
z: tensor([12.0885,  0.1770])
z.shape: torch.Size([2])
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_27_1.png" src="../_images/CASESTUDY_NN-day2_27_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Interpolate between two images of different classes</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">large_batch</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;np.unique(targets): </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">targets</span><span class="p">)))</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">latentVar</span> <span class="o">=</span> <span class="n">model_AE_linear</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">latentVar</span> <span class="o">=</span> <span class="n">latentVar</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">targets</span><span class="p">[:])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">n_points</span><span class="o">=</span><span class="mi">50</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="nb">range</span><span class="p">(</span><span class="n">n_points</span><span class="p">)):</span>

        <span class="n">label</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="c1"># this is the text</span>
                     <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="c1"># this is the point to label</span>
                     <span class="n">textcoords</span><span class="o">=</span><span class="s2">&quot;offset points&quot;</span><span class="p">,</span> <span class="c1"># how to position the text</span>
                     <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># distance from text to points (x,y)</span>
                     <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span> <span class="c1"># horizontal alignment can be left, right or center</span>

    <span class="c1"># Get the first two points of latentVar</span>
    <span class="n">x0</span><span class="p">,</span><span class="n">y0</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x1</span><span class="p">,</span><span class="n">y1</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">xvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">yvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x0,y0: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">y0</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x1,y1: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;xvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xvals</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;yvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">yvals</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xvals</span><span class="p">[:],</span><span class="n">yvals</span><span class="p">[:],</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
targets.shape: torch.Size([1000])
np.unique(targets): [0 1 2 3 4 5 6 7 8 9]
latentVar.shape: torch.Size([1000, 2])
targets[:20]: [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9]
latentVar[:20]: [[  0.88656074   1.5480746 ]
 [ 16.731844    -7.276616  ]
 [ -2.4016178   -6.888706  ]
 [-18.124239    19.383167  ]
 [ -7.1650925   -8.911613  ]
 [  0.5757653   23.241972  ]
 [-15.745467     6.396731  ]
 [  2.3958318    1.0412636 ]
 [-21.963514    12.419073  ]
 [ -9.068372    -1.1200742 ]
 [  3.6217616    2.202012  ]
 [ -0.54072136   0.3420033 ]
 [  4.266302     5.598924  ]
 [ 10.783974    -9.3472805 ]
 [-21.905462    10.90397   ]
 [-13.451579   -10.401498  ]
 [ -1.0420251   20.48603   ]
 [ -2.6018767    2.636466  ]
 [ 12.088468     0.17695215]
 [ -6.264395    -0.39840513]]
x0,y0: 0.8865607380867004,1.5480746030807495
x1,y1: 16.731843948364258,-7.276616096496582
xvals: [ 0.88656074  2.64714776  4.40773478  6.16832181  7.92890883  9.68949585
 11.45008288 13.2106699  14.97125693 16.73184395]
yvals: [ 1.5480746   0.56755341 -0.41296777 -1.39348896 -2.37401015 -3.35453134
 -4.33505253 -5.31557372 -6.29609491 -7.2766161 ]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_28_1.png" src="../_images/CASESTUDY_NN-day2_28_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">count</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xvals</span><span class="p">,</span><span class="n">yvals</span><span class="p">):</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;model_input: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_input</span><span class="p">))</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AE_decoder</span><span class="p">()</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
model_input: tensor([0.8866, 1.5481])
z: tensor([0.8866, 1.5481])
z.shape: torch.Size([2])
model_input: tensor([2.6471, 0.5676])
z: tensor([2.6471, 0.5676])
z.shape: torch.Size([2])
model_input: tensor([ 4.4077, -0.4130])
z: tensor([ 4.4077, -0.4130])
z.shape: torch.Size([2])
model_input: tensor([ 6.1683, -1.3935])
z: tensor([ 6.1683, -1.3935])
z.shape: torch.Size([2])
model_input: tensor([ 7.9289, -2.3740])
z: tensor([ 7.9289, -2.3740])
z.shape: torch.Size([2])
model_input: tensor([ 9.6895, -3.3545])
z: tensor([ 9.6895, -3.3545])
z.shape: torch.Size([2])
model_input: tensor([11.4501, -4.3351])
z: tensor([11.4501, -4.3351])
z.shape: torch.Size([2])
model_input: tensor([13.2107, -5.3156])
z: tensor([13.2107, -5.3156])
z.shape: torch.Size([2])
model_input: tensor([14.9713, -6.2961])
z: tensor([14.9713, -6.2961])
z.shape: torch.Size([2])
model_input: tensor([16.7318, -7.2766])
z: tensor([16.7318, -7.2766])
z.shape: torch.Size([2])
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_29_1.png" src="../_images/CASESTUDY_NN-day2_29_1.png" />
</div>
</div>
</div>
<div class="section" id="Question-1.1.">
<h2>Question 1.1.<a class="headerlink" href="#Question-1.1." title="Permalink to this headline">¶</a></h2>
<p><strong>Do the colors easily separate, or are they all clumped together? Which numbers are frequently embedded close together, and what does this mean?</strong></p>
</div>
<div class="section" id="Question-1.2.">
<h2>Question 1.2.<a class="headerlink" href="#Question-1.2." title="Permalink to this headline">¶</a></h2>
<p><strong>How realistic were the images you generated by interpolating between points in the latent space? Can you think of a better way to generate images with an autoencoder?</strong></p>
</div>
<div class="section" id="Section-2">
<h2>Section 2<a class="headerlink" href="#Section-2" title="Permalink to this headline">¶</a></h2>
<p>Now that we have an autoencoder working on MNIST, let’s use this model to visualize some geodata. For the next section we will use the SAT-6 (<a class="reference external" href="https://csc.lsu.edu/~saikat/deepsat/">https://csc.lsu.edu/~saikat/deepsat/</a>)</p>
<p>SAT-6 consists of a total of 405,000 image patches each of size 28x28 and covering 6 landcover classes - barren land, trees, grassland, roads, buildings and water bodies. 324,000 images (comprising of four-fifths of the total dataset) were chosen as the training dataset and 81,000 (one fifths) were chosen as the testing dataset. Similar to SAT-4, the training and test sets were selected from disjoint NAIP tiles. Once generated, the images in the dataset were randomized in the same way as that
for SAT-4. The specifications for the various landcover classes of SAT-4 and SAT-6 were adopted from those used in the National Land Cover Data (NLCD) algorithm.</p>
<p>The datasets are encoded as MATLAB .mat files that can be read using the standard load command in MATLAB. Each sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared. The training and test labels are 1x4 and 1x6 vectors for SAT-4 and SAT-6 respectively having a single 1 indexing a particular class from 0 through 4 or 6 and 0 values at all other indices.</p>
<p>The MAT file for the SAT-6 dataset contains the following variables:</p>
<ul class="simple">
<li><p>train_x 28x28x4x324000 uint8 (containing 324000 training samples of 28x28 images each with 4 channels)</p></li>
<li><p>train_y 324000x6 uint8 (containing 6x1 vectors having labels for the 324000 training samples)</p></li>
<li><p>test_x 28x28x4x81000 uint8 (containing 81000 test samples of 28x28 images each with 4 channels)</p></li>
<li><p>test_y 81000x6 uint8 (containing 6x1 vectors having labels for the 81000 test samples)</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.io</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span><span class="p">,</span> <span class="n">nn</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
cpu
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Using the satelite images dataset</span>
<span class="c1">###############################################################################</span>
<span class="c1">#load the data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">loadmat</span><span class="p">(</span><span class="s2">&quot;./geodata/sat-6-full.mat&quot;</span><span class="p">)</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;train_x&#39;</span><span class="p">]</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;train_y&#39;</span><span class="p">]</span>

<span class="n">test_images</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;test_x&#39;</span><span class="p">]</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;test_y&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">####################################################################</span>
<span class="c1">#Checkout the data</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training data shape : &#39;</span><span class="p">,</span> <span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Testing data shape : &#39;</span><span class="p">,</span> <span class="n">test_images</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Training data shape :  (28, 28, 4, 324000) (6, 324000)
Testing data shape :  (28, 28, 4, 81000) (6, 81000)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">#Change the dimension to fit into the model</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">train_images</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">t_train</span> <span class="o">=</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="c1"># x_test = test_images.transpose(3,0,1,2)</span>
<span class="c1"># t_test = test_labels.transpose()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training data shape : &#39;</span><span class="p">,</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">t_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># print(&#39;Testing data shape : &#39;, x_test.shape, t_test.shape)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Training data shape :  (324000, 28, 28, 4) (324000, 6)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">#Check what is in each channel</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">list_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="k">for</span> <span class="n">count</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">list_idx</span><span class="p">):</span>
<span class="c1">#     print(idx)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;count, t_train[count,:]: </span><span class="si">{}</span><span class="s1">, </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">t_train</span><span class="p">[</span><span class="n">count</span><span class="p">,:]))</span>
<span class="c1">#     print(x_train[idx,:,:,0:3])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">count</span><span class="p">,:,:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t_train</span><span class="p">[</span><span class="n">count</span><span class="p">,:])))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
count, t_train[count,:]: 0, [0 0 1 0 0 0]
count, t_train[count,:]: 1, [0 1 0 0 0 0]
count, t_train[count,:]: 2, [0 0 0 0 0 1]
count, t_train[count,:]: 3, [0 0 0 0 0 1]
count, t_train[count,:]: 4, [0 0 0 0 0 1]
count, t_train[count,:]: 5, [1 0 0 0 0 0]
count, t_train[count,:]: 6, [1 0 0 0 0 0]
count, t_train[count,:]: 7, [0 0 0 0 0 1]
count, t_train[count,:]: 8, [0 1 0 0 0 0]
count, t_train[count,:]: 9, [0 0 1 0 0 0]
count, t_train[count,:]: 10, [0 0 0 0 0 1]
count, t_train[count,:]: 11, [0 1 0 0 0 0]
count, t_train[count,:]: 12, [0 1 0 0 0 0]
count, t_train[count,:]: 13, [0 0 0 0 1 0]
count, t_train[count,:]: 14, [0 0 0 0 0 1]
count, t_train[count,:]: 15, [0 0 1 0 0 0]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_38_1.png" src="../_images/CASESTUDY_NN-day2_38_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># split in training and testing</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.utils.data.sampler</span> <span class="kn">import</span> <span class="n">SubsetRandomSampler</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">scipy.ndimage</span> <span class="kn">import</span> <span class="n">zoom</span>


<span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;data.dtype: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;target.dtype: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>


    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x_train.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">MyDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:</span><span class="n">n_samples</span><span class="p">,:,:,:],</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t_train</span><span class="p">[:</span><span class="n">n_samples</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="k">del</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span>
<span class="n">dataset_size</span>  <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dataset_size: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dataset_size</span><span class="p">))</span>
<span class="n">test_split</span><span class="o">=</span><span class="mf">0.2</span>

<span class="c1"># Number of frames in the sequence (in this case, same as number of tokens). Maybe I can make this number much bigger, like 4 times bigger, and then do the batches of batches...</span>
<span class="c1"># For example, when classifying, I can test if the first and the second chunk are sequence vs the first and third</span>
<span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span> <span class="c1">#Originally 16 frames... can I do 128 and then split in 4 chunks of 32</span>

<span class="c1"># -- split dataset</span>
<span class="n">indices</span>       <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">dataset_size</span><span class="p">))</span>
<span class="n">split</span>         <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">test_split</span><span class="o">*</span><span class="n">dataset_size</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;split: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">split</span><span class="p">))</span>
<span class="c1"># np.random.shuffle(indices) # Randomizing the indices is not a good idea if you want to model the sequence</span>
<span class="n">train_indices</span><span class="p">,</span> <span class="n">val_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">split</span><span class="p">:],</span> <span class="n">indices</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>

<span class="c1"># -- create dataloaders</span>
<span class="c1"># #Original</span>
<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="n">train_indices</span><span class="p">)</span>
<span class="n">valid_sampler</span> <span class="o">=</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="n">val_indices</span><span class="p">)</span>

<span class="n">dataloaders</span>   <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">),</span>
    <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">valid_sampler</span><span class="p">),</span>
    <span class="s1">&#39;all&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span>  <span class="n">batch_size</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x_train.shape: (324000, 28, 28, 4)
data.dtype: uint8
target.dtype: int64
dataset_size: 50000
split: 10000
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/home/user/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Linear activation in the middle (instead of an activation function)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Autoencoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3136</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">3136</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">z</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="c1"># ditto, but in reverse</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">z</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">lr_range</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.005</span><span class="p">,</span><span class="mf">0.001</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Autoencoder - with linear activation in middle layer and non-linearity (tanh) everywhere else&#39;</span><span class="p">)</span>
<span class="c1"># for hid_dim in hid_dim_range:</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lr_range</span><span class="p">:</span>
<span class="c1">#         print(&#39;\nhid_dim: {}, lr: {}&#39;.format(hid_dim, lr))</span>
    <span class="k">if</span> <span class="s1">&#39;model&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting previous model&#39;</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">ADAM</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span> <span class="c1"># This is absurdly high.</span>
    <span class="c1"># initialize the loss function. You don&#39;t want to use this one, so change it accordingly</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ADAM</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Linear activation in the middle (instead of an activation function and relus)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Autoencoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3136</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">3136</span><span class="p">)</span>

<span class="c1">#         self.tanh = nn.Tanh()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_lin4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">z</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="c1"># ditto, but in reverse</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_lin4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">z</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">lr_range</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.005</span><span class="p">,</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Autoencoder - with linear activation in middle layer and non-linearity (relu) everywhere else&#39;</span><span class="p">)</span>
<span class="c1"># for hid_dim in hid_dim_range:</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lr_range</span><span class="p">:</span>
<span class="c1">#         print(&#39;\nhid_dim: {}, lr: {}&#39;.format(hid_dim, lr))</span>
    <span class="k">if</span> <span class="s1">&#39;model&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting previous model&#39;</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">ADAM</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span> <span class="c1"># This is absurdly high.</span>
    <span class="c1"># initialize the loss function. You don&#39;t want to use this one, so change it accordingly</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ADAM</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Train the best config for longer</span>
<span class="n">lr_range</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Autoencoder - with linear activation in middle layer and non-linearity (relu) everywhere else&#39;</span><span class="p">)</span>
<span class="c1"># for hid_dim in hid_dim_range:</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lr_range</span><span class="p">:</span>
<span class="c1">#         print(&#39;\nhid_dim: {}, lr: {}&#39;.format(hid_dim, lr))</span>
    <span class="k">if</span> <span class="s1">&#39;model&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting previous model&#39;</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">ADAM</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span> <span class="c1"># This is absurdly high.</span>
    <span class="c1"># initialize the loss function. You don&#39;t want to use this one, so change it accordingly</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ADAM</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># #Save this model</span>
<span class="c1"># torch.save(model.state_dict(), &#39;./models/model_AE_sat6_v2.pt&#39;)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">if</span> <span class="s1">&#39;model&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting &quot;model&quot;&#39;</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">model</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;./models/model_AE_sat6.pt&#39;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;All keys matched successfully&gt;
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">if</span> <span class="s1">&#39;model_input&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting &quot;model_input&quot;&#39;</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">model_input</span>

<span class="k">if</span> <span class="s1">&#39;out&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting &quot;out&quot;&#39;</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">out</span>

<span class="k">if</span> <span class="s1">&#39;latentVar&#39;</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Deleting &quot;latentVar&quot;&#39;</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">latentVar</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Interpolate between two images of different classes</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">]))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;np.unique(targets): </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">targets</span><span class="p">)))</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">latentVar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">out</span><span class="p">,</span> <span class="n">model_input</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">latentVar</span> <span class="o">=</span> <span class="n">latentVar</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">targets</span><span class="p">[:],</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Set1&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">26</span><span class="p">))</span>
    <span class="n">n_points</span><span class="o">=</span><span class="mi">50</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="nb">range</span><span class="p">(</span><span class="n">n_points</span><span class="p">)):</span>

        <span class="n">label</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="c1"># this is the text</span>
                     <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="c1"># this is the point to label</span>
                     <span class="n">textcoords</span><span class="o">=</span><span class="s2">&quot;offset points&quot;</span><span class="p">,</span> <span class="c1"># how to position the text</span>
                     <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span>
                     <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># distance from text to points (x,y)</span>
                     <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span> <span class="c1"># horizontal alignment can be left, right or center</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
targets.shape: torch.Size([5000])
np.unique(targets): [0. 1. 2. 3. 4. 5.]
latentVar.shape: torch.Size([5000, 2])
targets[:20]: [2. 1. 5. 5. 5. 0. 0. 5. 1. 2. 5. 1. 1. 4. 5. 2. 5. 3. 3. 1.]
latentVar[:20]: [[-22.65415     5.3563433]
 [-38.309322   13.542638 ]
 [ -4.8976493  10.544079 ]
 [ -8.371047    8.8982525]
 [-13.901408   17.608837 ]
 [-46.51722    23.859837 ]
 [ -2.5415459  34.010395 ]
 [ -4.0781636   8.349136 ]
 [-34.823757   10.22914  ]
 [-80.44063   -24.848303 ]
 [ -7.9433665   8.423912 ]
 [-35.214314    9.43365  ]
 [-38.87878    12.459227 ]
 [-22.8948     16.725973 ]
 [ -5.7261662  10.49354  ]
 [-54.75082    -9.278481 ]
 [ -8.728979   10.757269 ]
 [-67.66766   -12.463643 ]
 [-88.861664  -26.455889 ]
 [-35.756615   13.15131  ]]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_49_1.png" src="../_images/CASESTUDY_NN-day2_49_1.png" />
</div>
</div>
</div>
<div class="section" id="Question-2.1.">
<h2>Question 2.1.<a class="headerlink" href="#Question-2.1." title="Permalink to this headline">¶</a></h2>
<p><strong>How many clusters are visible in the embedding? Do they correspond to the cluster labels?</strong></p>
</div>
<div class="section" id="Section-3---Generative-Models">
<h2>Section 3 - Generative Models<a class="headerlink" href="#Section-3---Generative-Models" title="Permalink to this headline">¶</a></h2>
<p>Now, let’s try something more interesting: generating data. In this section, you’ll implement a variation of the autoencoder (called a “Variational Autoencoder”) and a Generative Adversiarial Network, and will employ both to create never-before seen handwritten digits.</p>
</div>
<div class="section" id="Section-3.1---Variational-Autoencoder">
<h2>Section 3.1 - Variational Autoencoder<a class="headerlink" href="#Section-3.1---Variational-Autoencoder" title="Permalink to this headline">¶</a></h2>
<p>Autoencoders are great, but their latent spaces can be messy. You may have noticed previously that the AE’s embedding of MNIST clumped each digit into separate islands, with some overlap but also large empty regions. As you saw, the points in these empty parts of the embedding don’t correspond well to real digits.</p>
<p>Embedding generated by AE:</p>
<p><img alt="drawing" class="no-scaled-link" src="../_images/AE_embed_example.png" style="width: 800px;" /></p>
<p>This is the founding idea of the Variational Autoencoder, which makes two modifications to make interpolation within the latent space more meaningful. The first modification is the strangest: instead of encoding points in a latent space, the encoder creates a gaussian probability distribution around the encoded point, with a mean and squared variance unique to each point. The decoder is then passed a random sample from this distribution. This encourages similar points in the latent space to
correspond to similar outputs, since the decoder only gets to choose a point close to the encoded original.</p>
<p><img alt="drawing" class="no-scaled-link" src="../_images/VAE_architecture.png" style="width: 800px;" /></p>
<p>If the first of these regularizations encourages similar latent representations within clusters, the second enforces proximity between clusters. This is achieved with the Kullback Leibler (KL) divergence, which tabulates the dissimilarity of the previously generated gaussian with a standard normal distribution; measuring, in effect, how much the varaince and mean differ from a variance of one and mean of zero. This prevents any class of embeddings from drifting too far away from the others. The
KL divergence between two normal distributions is given by:</p>
<p><span class="math notranslate nohighlight">\(D_{KL}[N(\mu,\sigma)||N(0,1)] = (1/2)\sum{1 + log\sigma^2-\mu^2-\sigma^2}\)</span></p>
<p>where the sum is taken over each dimension in the latent space.</p>
<p><img alt="drawing" class="no-scaled-link" src="../_images/VAE_KLDiv.png" style="width: 800px;" /> <img alt="drawing" class="no-scaled-link" src="../_images/VAE_Loss.png" style="width: 800px;" /> <img alt="drawing" class="no-scaled-link" src="../_images/VAE_embed_example.png" style="width: 800px;" /></p>
<p>An excellent and highly entertaining introduction to Variational Autoencoders may be found in David Foster’s book, “Generative Deep Learning”. Additionally, the mathematically inclined may enjoy Kingma and Welling’s 2013 paper “Auto-encoding Variational Bayes” (<a class="reference external" href="https://arxiv.org/pdf/1312.6114">https://arxiv.org/pdf/1312.6114</a>) which first presented the theoretical foundations for the Variational Autoencoder.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Loading the dataset and create dataloaders</span>
<span class="n">mnist_train</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">mnist_test</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">Ref: https://github.com/pytorch/examples/blob/master/vae/main.py</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">save_image</span>

<span class="n">path_to_save</span> <span class="o">=</span> <span class="s1">&#39;./plots_VAE&#39;</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path_to_save</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path_to_save</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Args</span><span class="p">:</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">no_cuda</span><span class="o">=</span><span class="kc">False</span>
    <span class="n">log_interval</span><span class="o">=</span><span class="mi">100</span>

<span class="n">args</span><span class="o">=</span><span class="n">Args</span><span class="p">()</span>

<span class="n">args</span><span class="o">.</span><span class="n">cuda</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">no_cuda</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">cuda</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="c1"># Use NVIDIA CUDA GPU if available</span>

<span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;pin_memory&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">cuda</span> <span class="k">else</span> <span class="p">{}</span>


<span class="k">class</span> <span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VAE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc21</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc22</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc21</span><span class="p">(</span><span class="n">h1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc22</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">logvar</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="o">*</span><span class="n">std</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">h3</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">h3</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="c1"># loss_MSE = nn.MSELoss().to(device)</span>

<span class="k">def</span> <span class="nf">VAE_loss_function</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
    <span class="n">recon_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">784</span><span class="p">),</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>

    <span class="c1"># Compute the KLD</span>
    <span class="n">KLD</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">recon_loss</span><span class="p">,</span> <span class="n">KLD</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_KLD</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">train_recon_loss</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">recon_batch</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">recon_loss</span><span class="p">,</span> <span class="n">KLD</span> <span class="o">=</span> <span class="n">VAE_loss_function</span><span class="p">(</span><span class="n">recon_batch</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="n">KLD</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">train_KLD</span> <span class="o">+=</span> <span class="n">KLD</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">train_recon_loss</span> <span class="o">+=</span> <span class="n">recon_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Epoch: </span><span class="si">{}</span><span class="s1"> [</span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1"> (</span><span class="si">{:.0f}</span><span class="s1">%)]</span><span class="se">\t</span><span class="s1">Loss: </span><span class="si">{:.6f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span>
                <span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_idx</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
<span class="c1">#                 recon_loss.item() / len(data),</span>
<span class="c1">#                 KLD.item() / len(data)))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;====&gt; Epoch: </span><span class="si">{}</span><span class="s1"> Average loss: </span><span class="si">{:.6f}</span><span class="s1"> (Loss_recon: </span><span class="si">{:.6f}</span><span class="s1">, Loss_KLD: </span><span class="si">{:.6f}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="n">epoch</span><span class="p">,</span> <span class="n">train_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span> <span class="n">train_recon_loss</span><span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span>
                <span class="n">train_KLD</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)))</span>


<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">test_KLD</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">test_recon_loss</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">recon_batch</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="c1">#             test_loss += VAE_loss_function(recon_batch, data, mu, logvar).item()</span>
            <span class="n">recon_loss</span><span class="p">,</span> <span class="n">KLD</span> <span class="o">=</span> <span class="n">VAE_loss_function</span><span class="p">(</span><span class="n">recon_batch</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="n">KLD</span>
            <span class="n">test_recon_loss</span> <span class="o">+=</span> <span class="n">recon_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">test_KLD</span> <span class="o">+=</span> <span class="n">KLD</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">n</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">8</span><span class="p">)</span>
                <span class="n">comparison</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span>
                                      <span class="n">recon_batch</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)[:</span><span class="n">n</span><span class="p">]])</span>
                <span class="n">save_image</span><span class="p">(</span><span class="n">comparison</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="s1">&#39;./plots_VAE/reconstruction_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;.png&#39;</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

    <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;====&gt; Test set loss: </span><span class="si">{:.6f}</span><span class="s1"> (Loss_recon: </span><span class="si">{:.6f}</span><span class="s1">, Loss_KLD: </span><span class="si">{:.6f}</span><span class="s1">)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_recon_loss</span><span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span>
                <span class="n">test_KLD</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)))</span>

</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        <span class="n">test</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="n">save_image</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span>
                       <span class="s1">&#39;./plots_VAE/sample_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;.png&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># # Save the model</span>
<span class="c1"># torch.save(model.state_dict(), &#39;./models/model_VAE.pt&#39;)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load the model</span>
<span class="n">model_VAE</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_VAE</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;./models/model_VAE.pt&#39;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;All keys matched successfully&gt;
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load some of the images</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">img0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_VAE/sample_1.png&#39;</span><span class="p">))</span>
<span class="n">img10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_VAE/sample_10.png&#39;</span><span class="p">))</span>
<span class="n">img25</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_VAE/sample_25.png&#39;</span><span class="p">))</span>
<span class="n">img50</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_VAE/sample_50.png&#39;</span><span class="p">))</span>
<span class="c1"># img100 = np.array(Image.open(&#39;./plots_VAE/sample_40.png&#39;))</span>
<span class="c1"># img250 = np.array(Image.open(&#39;./plots_VAE/gen_img250.png&#39;))</span>
<span class="c1"># img500 = np.array(Image.open(&#39;./plots_VAE/gen_img500.png&#39;))</span>
<span class="c1"># img750 = np.array(Image.open(&#39;./plots_VAE/gen_img750.png&#39;))</span>
<span class="c1"># img800 = np.array(Image.open(&#39;./plots_VAE/gen_img800.png&#39;))</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img0</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;sample_1&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img10</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;sample_10&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img25</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;sample_25&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;sample_50&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([]);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_58_0.png" src="../_images/CASESTUDY_NN-day2_58_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Visualize the 2D space</span>
<span class="c1"># Should we use PCA to embeded the 20D to 2D?</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;default&#39;</span><span class="p">)</span>

<span class="n">large_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="mi">60000</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model_VAE</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">large_batch</span><span class="p">))</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">recon_batch</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model_VAE</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1">#Reduce dimensions to 2D</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">latentVar</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">targets</span><span class="p">[:],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">26</span><span class="p">))</span>
    <span class="n">n_points</span><span class="o">=</span><span class="mi">100</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="nb">range</span><span class="p">(</span><span class="n">n_points</span><span class="p">)):</span>

        <span class="n">label</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="c1"># this is the text</span>
                     <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="c1"># this is the point to label</span>
                     <span class="n">textcoords</span><span class="o">=</span><span class="s2">&quot;offset points&quot;</span><span class="p">,</span> <span class="c1"># how to position the text</span>
                     <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span>
                     <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># distance from text to points (x,y)</span>
                     <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span> <span class="c1"># horizontal alignment can be left, right or center</span>

    <span class="c1"># Get the first two points of latentVar</span>
    <span class="n">x0</span><span class="p">,</span><span class="n">y0</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x1</span><span class="p">,</span><span class="n">y1</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">xvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">yvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x0,y0: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">y0</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x1,y1: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;xvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xvals</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;yvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">yvals</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xvals</span><span class="p">[:],</span><span class="n">yvals</span><span class="p">[:],</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
targets[:20]: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]
latentVar[:20]: [[ 0.26706883 -1.9736019 ]
 [-1.8611817   1.8440641 ]
 [-2.5281484  -1.0331651 ]
 [ 1.4196528   1.3110398 ]
 [ 1.4876976   0.21686505]
 [-2.5829864  -1.5002542 ]
 [ 0.29228047 -0.887662  ]
 [ 0.30350858 -0.6865011 ]
 [ 1.3719988   0.20367995]
 [ 1.4738213  -1.3410944 ]
 [ 0.83528453  2.032341  ]
 [ 0.15110765  0.95467454]
 [ 2.0726109  -2.5019965 ]
 [ 1.7359124   1.2086135 ]
 [-2.1024263  -0.9454553 ]
 [-0.84691584  1.5131922 ]
 [ 1.9012048  -0.801138  ]
 [ 0.5894465  -1.1062578 ]
 [-0.8419465   0.6534402 ]
 [ 0.93832934 -1.0562824 ]]
x0,y0: 0.2670688331127167,-1.9736019372940063
x1,y1: -1.8611817359924316,1.8440641164779663
xvals: [ 0.26706883  0.03059655 -0.20587574 -0.44234802 -0.67882031 -0.91529259
 -1.15176488 -1.38823717 -1.62470945 -1.86118174]
yvals: [-1.97360194 -1.54941682 -1.1252317  -0.70104659 -0.27686147  0.14732365
  0.57150877  0.99569388  1.419879    1.84406412]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_59_1.png" src="../_images/CASESTUDY_NN-day2_59_1.png" />
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># ## Visualizing the embedding using different algorithms</span>
<span class="c1"># # reducer = phate.PHATE(n_components=2, random_state=1, knn=5, t=2)#, n_neighbors=8)</span>
<span class="c1"># reducer = umap.UMAP(n_components=2)</span>
<span class="c1"># # reducer = PCA(n_components=2,random_state=np.random.RandomState(42))</span>

<span class="c1"># with torch.no_grad():</span>
<span class="c1">#     model_VAE.eval()</span>
<span class="c1">#     data, targets = next(iter(large_batch))</span>
<span class="c1">#     targets = targets.numpy()</span>
<span class="c1">#     data = data.to(device)</span>
<span class="c1">#     recon_batch, mu, logvar = model_VAE(data)</span>

<span class="c1">#     #Reduce dimensions to 2D</span>
<span class="c1">#     latentVar = reducer.fit_transform(mu.cpu().numpy())</span>

<span class="c1">#     fig,ax = plt.subplots(1,1,figsize=(10,10))</span>
<span class="c1">#     plt.scatter(latentVar[:,0],latentVar[:,1],c=targets[:], alpha=0.5)</span>
<span class="c1">#     print(&#39;targets[:20]: {}&#39;.format(targets[:20]))</span>
<span class="c1">#     print(&#39;latentVar[:20]: {}&#39;.format(latentVar[:20]))</span>

<span class="c1">#     plt.colorbar(ticks=range(26))</span>
<span class="c1">#     n_points=100</span>
<span class="c1">#     for x,y,i in zip(latentVar[:n_points,0],latentVar[:n_points,1],range(n_points)):</span>

<span class="c1">#         label = targets[i]</span>

<span class="c1">#         plt.annotate(label, # this is the text</span>
<span class="c1">#                      (x,y), # this is the point to label</span>
<span class="c1">#                      textcoords=&quot;offset points&quot;, # how to position the text</span>
<span class="c1">#                      c=&#39;r&#39;,</span>
<span class="c1">#                      xytext=(0,0), # distance from text to points (x,y)</span>
<span class="c1">#                      ha=&#39;center&#39;) # horizontal alignment can be left, right or center</span>

<span class="c1">#     # Get the first two points of latentVar</span>
<span class="c1">#     x0,y0 = latentVar[0,0],latentVar[0,1]</span>
<span class="c1">#     x1,y1 = latentVar[1,0],latentVar[1,1]</span>
<span class="c1">#     xvals = np.array(np.linspace(x0, x1, 10))</span>
<span class="c1">#     yvals = np.array(np.linspace(y0, y1, 10))</span>
<span class="c1">#     print(&#39;x0,y0: {},{}&#39;.format(x0,y0))</span>
<span class="c1">#     print(&#39;x1,y1: {},{}&#39;.format(x1,y1))</span>
<span class="c1">#     print(&#39;xvals: {}&#39;.format(xvals))</span>
<span class="c1">#     print(&#39;yvals: {}&#39;.format(yvals))</span>
<span class="c1">#     plt.plot(xvals[:],yvals[:],c=&#39;r&#39;,marker=&#39;*&#39;)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">logvar</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logvar</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logvar</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logvar</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor(-0.4241)
tensor(-2.4555)
tensor(-4.5016)
torch.Size([10000, 20])
torch.Size([10000, 20])
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">VAE_decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VAE_decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">model_VAE</span><span class="o">.</span><span class="n">fc1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc21</span> <span class="o">=</span> <span class="n">model_VAE</span><span class="o">.</span><span class="n">fc21</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc22</span> <span class="o">=</span> <span class="n">model_VAE</span><span class="o">.</span><span class="n">fc22</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">model_VAE</span><span class="o">.</span><span class="n">fc3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">model_VAE</span><span class="o">.</span><span class="n">fc4</span>

    <span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">logvar</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="o">*</span><span class="n">std</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">h3</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">h3</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
<span class="c1">#         mu, logvar = self.encode(x.view(-1, 784))</span>
        <span class="n">logvar</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">2.5</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;z.shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">VAE_decoder</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">count</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xvals</span><span class="p">,</span><span class="n">yvals</span><span class="p">):</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span> <span class="c1">#Take it back to 20D</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="c1">#         print(&#39;model_input: {}&#39;.format(model_input))</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_63_1.png" src="../_images/CASESTUDY_NN-day2_63_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Redo for digits in the same class</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model_VAE</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">large_batch</span><span class="p">))</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">idx_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">targets</span><span class="o">==</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Get two &#39;6&#39;.</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">idx_</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">recon_batch</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model_VAE</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1">#Reduce dimensions to 2D</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">latentVar</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">targets</span><span class="p">[:],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;latentVar[:20]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="mi">20</span><span class="p">]))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">26</span><span class="p">))</span>
    <span class="n">n_points</span><span class="o">=</span><span class="mi">100</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[:</span><span class="n">n_points</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="nb">range</span><span class="p">(</span><span class="n">n_points</span><span class="p">)):</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="c1"># this is the text</span>
                     <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="c1"># this is the point to label</span>
                     <span class="n">textcoords</span><span class="o">=</span><span class="s2">&quot;offset points&quot;</span><span class="p">,</span> <span class="c1"># how to position the text</span>
                     <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span>
                     <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># distance from text to points (x,y)</span>
                     <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span> <span class="c1"># horizontal alignment can be left, right or center</span>

    <span class="c1"># Get the first two points of latentVar</span>
    <span class="n">x0</span><span class="p">,</span><span class="n">y0</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x1</span><span class="p">,</span><span class="n">y1</span> <span class="o">=</span> <span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">0</span><span class="p">],</span><span class="n">latentVar</span><span class="p">[</span><span class="n">idx_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">xvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">yvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x0,y0: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">y0</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x1,y1: </span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;xvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xvals</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;yvals: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">yvals</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xvals</span><span class="p">[:],</span><span class="n">yvals</span><span class="p">[:],</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">VAE_decoder</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">count</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xvals</span><span class="p">,</span><span class="n">yvals</span><span class="p">):</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span> <span class="c1">#Take it back to 20D</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="c1">#         print(&#39;model_input: {}&#39;.format(model_input))</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Digit: 8&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Digit: 1&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2 5]
targets[:20]: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]
latentVar[:20]: [[ 0.4045419  -2.033641  ]
 [-1.7656242   1.7947903 ]
 [-2.4364967  -1.1195807 ]
 [ 1.4608481   1.354135  ]
 [ 1.4224052   0.25367513]
 [-2.459871   -1.6010041 ]
 [ 0.2546648  -0.7847284 ]
 [ 0.20147283 -0.7330895 ]
 [ 1.6117115   0.11941535]
 [ 1.6162555  -1.352225  ]
 [ 0.727913    2.0899346 ]
 [ 0.0062574   1.0858725 ]
 [ 2.0874546  -2.4241602 ]
 [ 1.6282545   1.2073023 ]
 [-2.029297   -0.9550152 ]
 [-1.0435832   1.5274566 ]
 [ 1.8745112  -0.7466577 ]
 [ 0.6685767  -1.2008247 ]
 [-0.81140006  0.52554303]
 [ 0.83150834 -0.9141133 ]]
x0,y0: -2.4364967346191406,-1.1195807456970215
x1,y1: -2.4598710536956787,-1.6010041236877441
xvals: [-2.43649673 -2.43909388 -2.44169103 -2.44428817 -2.44688532 -2.44948247
 -2.45207961 -2.45467676 -2.45727391 -2.45987105]
yvals: [-1.11958075 -1.17307223 -1.22656372 -1.28005521 -1.33354669 -1.38703818
 -1.44052966 -1.49402115 -1.54751264 -1.60100412]
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
z.shape: torch.Size([20])
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_64_1.png" src="../_images/CASESTUDY_NN-day2_64_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_64_2.png" src="../_images/CASESTUDY_NN-day2_64_2.png" />
</div>
</div>
</div>
<div class="section" id="Question-3.1.1.">
<h2>Question 3.1.1.<a class="headerlink" href="#Question-3.1.1." title="Permalink to this headline">¶</a></h2>
<p><strong>How does the VAE’s latent space compare to the latent space of your previous autoencoder? Do the generated images have more clarity? Is this most noticeable between or within classes?</strong></p>
</div>
<div class="section" id="Question-3.1.2.">
<h2>Question 3.1.2.<a class="headerlink" href="#Question-3.1.2." title="Permalink to this headline">¶</a></h2>
<p><strong>In what situations would a VAE be more useful than a vanilla autoencoder, and when would you prefer a vanilla autoencoder to a VAE?</strong></p>
</div>
<div class="section" id="Question-3.1.3.">
<h2>Question 3.1.3.<a class="headerlink" href="#Question-3.1.3." title="Permalink to this headline">¶</a></h2>
<p><strong>The distance between embeddings in your first autoencoder provided some measure of the similarity between digits. To what extent is this preserved, or improved, by the VAE?</strong></p>
</div>
<div class="section" id="Section-3.2---GANS">
<h2>Section 3.2 - GANS<a class="headerlink" href="#Section-3.2---GANS" title="Permalink to this headline">¶</a></h2>
<p>Whereas the VAE was tweaked to allow small perturbations in the latent space to produce reasonable decodings, the Generative Adversarial Network was designed to generate novel samples. A GAN is really two networks in one: the generator network produces fake images, while the discriminator guesses if they are fake. Initially, both networks perform horribly, but with time (and luck) they force each other to improve until the generator’s images are indistinguishable from the real thing. In this
part, you’ll build your own GAN in PyTorch, and test it on the MNIST dataset.</p>
<p><img alt="drawing" class="no-scaled-link" src="../_images/GANs_architecture1.png" style="width: 800px;" /> <img alt="drawing" class="no-scaled-link" src="../_images/GANS_architecture2.png" style="width: 800px;" /> <img alt="drawing" class="no-scaled-link" src="../_images/GANs_Loss.png" style="width: 800px;" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">imageio</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span><span class="p">,</span> <span class="n">save_image</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;default&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nz</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Generator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nz</span> <span class="o">=</span> <span class="n">nz</span> <span class="c1"># the dimension of the random noise used to seed the Generator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">main</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span> <span class="c1"># nn.sequential is a handy way of combining multiple layers.</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nz</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span> <span class="c1"># Original: Tanh. Use a sigmoid</span>
        <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Discriminator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_input</span> <span class="o">=</span> <span class="mi">784</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">main</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_input</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
        <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_discriminator</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">real_data</span><span class="p">,</span> <span class="n">fake_data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Train the discriminator on a minibatch of data.</span>
<span class="sd">    INPUTS</span>
<span class="sd">        :param optimizer: the optimizer used for training</span>
<span class="sd">        :param real_data: the batch of training data</span>
<span class="sd">        :param fake_data: the data generated by the generator from random noise</span>
<span class="sd">    The discriminator will incur two losses: one from trying to classify the real data, and another from classifying the fake data.</span>
<span class="sd">    TODO: Fill in this function.</span>
<span class="sd">    It should</span>
<span class="sd">    1. Run the discriminator on the real_data and the fake_data</span>
<span class="sd">    2. Compute and sum the respective loss terms (described in the assignment)</span>
<span class="sd">    3. Backpropogate the loss (e.g. loss.backward()), and perform optimization (e.g. optimizer.step()).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 1.1 Train on Real Data</span>
    <span class="n">prediction_real</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">(</span><span class="n">real_data</span><span class="p">)</span>
    <span class="n">y_real</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">prediction_real</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">D_real_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">prediction_real</span><span class="p">,</span> <span class="n">y_real</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>


    <span class="c1"># 1.2 Train on Fake Data</span>
    <span class="n">prediction_fake</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">(</span><span class="n">fake_data</span><span class="p">)</span>
    <span class="n">y_fake</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">prediction_fake</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">D_fake_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">prediction_fake</span><span class="p">,</span> <span class="n">y_fake</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

    <span class="n">D_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">D_real_loss</span> <span class="o">+</span> <span class="n">D_fake_loss</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">D_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># we&#39;ll return the loss for book-keeping purposes. (E.g. if you want to make plots of the loss.)</span>
    <span class="k">return</span> <span class="n">D_loss</span>

<span class="k">def</span> <span class="nf">train_generator</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">fake_data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a single training step on the generator.</span>
<span class="sd">    :param optimizer: the optimizer</span>
<span class="sd">    :param fake_data: forgeries, created by the generator from random noise. (Done before calling this function.)</span>
<span class="sd">    :return:  the generator&#39;s loss</span>
<span class="sd">    TODO: Fill in this function</span>
<span class="sd">    It should</span>
<span class="sd">    1. Run the discriminator on the fake_data</span>
<span class="sd">    2. compute the resultant loss for the generator (as described in the assignment)</span>
<span class="sd">    3. Backpropagate the loss, and perform optimization</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">(</span><span class="n">fake_data</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">prediction</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">G_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">G_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">G_loss</span><span class="o">/</span><span class="mi">2</span>


</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[36]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">path_to_save</span> <span class="o">=</span> <span class="s1">&#39;./plots_GANs&#39;</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path_to_save</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path_to_save</span><span class="p">)</span>


<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">nz</span> <span class="o">=</span> <span class="mi">100</span><span class="c1"># dimension of random noise</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">nz</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Optimizers (notice the use of &#39;discriminator&#39;&lt;-Object class)</span>
<span class="n">d_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">discriminator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">)</span>
<span class="n">g_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">generator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">)</span> <span class="c1"># 1: 3e-4; 2: 1e-5</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[45]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># For each epoch, you&#39;ll</span>
<span class="c1"># 1. Loop through the training data. For each batch, feed random noise into the generator to generate fake_data of the corresponding size.</span>
<span class="c1"># 2. Feed the fake data and real data into the train_discriminator and train_generator functions</span>
<span class="c1"># At the end of each epoch, use the below functions to save a grid of generated images.</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_d_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_g_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

        <span class="c1"># 1. Train Discriminator</span>
        <span class="n">real_data</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Generate fake data</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nz</span><span class="p">))))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">fake_data</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="c1"># Because we don&#39;t want the generator being trained while the discriminator is being trained</span>

        <span class="c1"># Train D</span>
        <span class="n">d_error</span> <span class="o">=</span> <span class="n">train_discriminator</span><span class="p">(</span><span class="n">d_optimizer</span><span class="p">,</span> <span class="n">real_data</span><span class="p">,</span> <span class="n">fake_data</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="c1">#, discriminator, criterion)</span>


        <span class="c1"># 2. Train Generator</span>
        <span class="c1"># Generate fake data</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nz</span><span class="p">))))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">generated_img</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

        <span class="c1"># Train G</span>
        <span class="n">g_error</span> <span class="o">=</span> <span class="n">train_generator</span><span class="p">(</span><span class="n">g_optimizer</span><span class="p">,</span> <span class="n">generated_img</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">d_error</span> <span class="o">+</span> <span class="n">g_error</span>
        <span class="n">train_d_error</span> <span class="o">+=</span> <span class="n">d_error</span>
        <span class="n">train_g_error</span> <span class="o">+=</span> <span class="n">g_error</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;====&gt; Epoch: </span><span class="si">{}</span><span class="s1"> Average loss: </span><span class="si">{:.6f}</span><span class="s1"> (D_error: </span><span class="si">{:.6f}</span><span class="s1">, G_error: </span><span class="si">{:.6f}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
      <span class="n">epoch</span><span class="p">,</span> <span class="n">train_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span> <span class="n">train_d_error</span><span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span>
            <span class="n">train_g_error</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)))</span>

<span class="c1">#     if epoch == 100:</span>
<span class="c1">#         torch.save(generator.state_dict(), &#39;./models/generator_100epochs.pt&#39;)</span>
<span class="c1">#         torch.save(discriminator.state_dict(), &#39;./models/discriminator_100epochs.pt&#39;)</span>

    <span class="c1"># reshape the image tensors into a grid</span>
    <span class="n">generated_img</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">generated_img</span><span class="p">)</span>
    <span class="c1"># save the generated torch tensor images</span>
    <span class="n">save_image</span><span class="p">(</span><span class="n">generated_img</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;./plots_GANs/gen_img</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
====&gt; Epoch: 0 Average loss: 0.009112 (D_error: 0.005233, G_error: 0.003878)
====&gt; Epoch: 1 Average loss: 0.011252 (D_error: 0.004452, G_error: 0.006800)
====&gt; Epoch: 2 Average loss: 0.016729 (D_error: 0.004336, G_error: 0.012393)
====&gt; Epoch: 3 Average loss: 0.013045 (D_error: 0.003964, G_error: 0.009081)
====&gt; Epoch: 4 Average loss: 0.012556 (D_error: 0.003082, G_error: 0.009474)
====&gt; Epoch: 5 Average loss: 0.012022 (D_error: 0.002859, G_error: 0.009164)
====&gt; Epoch: 6 Average loss: 0.011667 (D_error: 0.002569, G_error: 0.009097)
====&gt; Epoch: 7 Average loss: 0.012540 (D_error: 0.003023, G_error: 0.009517)
====&gt; Epoch: 8 Average loss: 0.011756 (D_error: 0.002783, G_error: 0.008973)
====&gt; Epoch: 9 Average loss: 0.011427 (D_error: 0.002824, G_error: 0.008603)
====&gt; Epoch: 10 Average loss: 0.011226 (D_error: 0.002858, G_error: 0.008368)
====&gt; Epoch: 11 Average loss: 0.011279 (D_error: 0.002739, G_error: 0.008540)
====&gt; Epoch: 12 Average loss: 0.011694 (D_error: 0.002860, G_error: 0.008834)
====&gt; Epoch: 13 Average loss: 0.012288 (D_error: 0.002805, G_error: 0.009483)
====&gt; Epoch: 14 Average loss: 0.012043 (D_error: 0.002977, G_error: 0.009066)
====&gt; Epoch: 15 Average loss: 0.011299 (D_error: 0.002731, G_error: 0.008568)
====&gt; Epoch: 16 Average loss: 0.010944 (D_error: 0.002850, G_error: 0.008095)
====&gt; Epoch: 17 Average loss: 0.010834 (D_error: 0.002759, G_error: 0.008074)
====&gt; Epoch: 18 Average loss: 0.010840 (D_error: 0.002983, G_error: 0.007857)
====&gt; Epoch: 19 Average loss: 0.011458 (D_error: 0.002909, G_error: 0.008549)
====&gt; Epoch: 20 Average loss: 0.012937 (D_error: 0.003202, G_error: 0.009735)
====&gt; Epoch: 21 Average loss: 0.011743 (D_error: 0.002804, G_error: 0.008939)
====&gt; Epoch: 22 Average loss: 0.011122 (D_error: 0.002759, G_error: 0.008364)
====&gt; Epoch: 23 Average loss: 0.010848 (D_error: 0.002680, G_error: 0.008168)
====&gt; Epoch: 24 Average loss: 0.011261 (D_error: 0.002618, G_error: 0.008642)
====&gt; Epoch: 25 Average loss: 0.011152 (D_error: 0.002671, G_error: 0.008482)
====&gt; Epoch: 26 Average loss: 0.010925 (D_error: 0.002673, G_error: 0.008252)
====&gt; Epoch: 27 Average loss: 0.011445 (D_error: 0.002541, G_error: 0.008903)
====&gt; Epoch: 28 Average loss: 0.012191 (D_error: 0.002650, G_error: 0.009541)
====&gt; Epoch: 29 Average loss: 0.012264 (D_error: 0.002669, G_error: 0.009595)
====&gt; Epoch: 30 Average loss: 0.012541 (D_error: 0.002559, G_error: 0.009982)
====&gt; Epoch: 31 Average loss: 0.011871 (D_error: 0.002520, G_error: 0.009351)
====&gt; Epoch: 32 Average loss: 0.011711 (D_error: 0.002538, G_error: 0.009173)
====&gt; Epoch: 33 Average loss: 0.011661 (D_error: 0.002421, G_error: 0.009240)
====&gt; Epoch: 34 Average loss: 0.011732 (D_error: 0.002540, G_error: 0.009192)
====&gt; Epoch: 35 Average loss: 0.011645 (D_error: 0.002627, G_error: 0.009018)
====&gt; Epoch: 36 Average loss: 0.011363 (D_error: 0.002604, G_error: 0.008759)
====&gt; Epoch: 37 Average loss: 0.011924 (D_error: 0.002564, G_error: 0.009360)
====&gt; Epoch: 38 Average loss: 0.011954 (D_error: 0.002563, G_error: 0.009392)
====&gt; Epoch: 39 Average loss: 0.011776 (D_error: 0.002527, G_error: 0.009248)
====&gt; Epoch: 40 Average loss: 0.011560 (D_error: 0.002481, G_error: 0.009079)
====&gt; Epoch: 41 Average loss: 0.011478 (D_error: 0.002448, G_error: 0.009030)
====&gt; Epoch: 42 Average loss: 0.011517 (D_error: 0.002457, G_error: 0.009060)
====&gt; Epoch: 43 Average loss: 0.012042 (D_error: 0.002379, G_error: 0.009663)
====&gt; Epoch: 44 Average loss: 0.012851 (D_error: 0.002175, G_error: 0.010675)
====&gt; Epoch: 45 Average loss: 0.012954 (D_error: 0.002152, G_error: 0.010802)
====&gt; Epoch: 46 Average loss: 0.012342 (D_error: 0.002193, G_error: 0.010149)
====&gt; Epoch: 47 Average loss: 0.012484 (D_error: 0.002040, G_error: 0.010444)
====&gt; Epoch: 48 Average loss: 0.013092 (D_error: 0.002028, G_error: 0.011064)
====&gt; Epoch: 49 Average loss: 0.013174 (D_error: 0.001903, G_error: 0.011270)
====&gt; Epoch: 50 Average loss: 0.013456 (D_error: 0.001871, G_error: 0.011586)
====&gt; Epoch: 51 Average loss: 0.013044 (D_error: 0.001979, G_error: 0.011065)
====&gt; Epoch: 52 Average loss: 0.013525 (D_error: 0.001852, G_error: 0.011672)
====&gt; Epoch: 53 Average loss: 0.013525 (D_error: 0.001875, G_error: 0.011650)
====&gt; Epoch: 54 Average loss: 0.013212 (D_error: 0.001817, G_error: 0.011395)
====&gt; Epoch: 55 Average loss: 0.013245 (D_error: 0.001821, G_error: 0.011425)
====&gt; Epoch: 56 Average loss: 0.013492 (D_error: 0.001801, G_error: 0.011691)
====&gt; Epoch: 57 Average loss: 0.013587 (D_error: 0.001761, G_error: 0.011826)
====&gt; Epoch: 58 Average loss: 0.013946 (D_error: 0.001675, G_error: 0.012272)
====&gt; Epoch: 59 Average loss: 0.014265 (D_error: 0.001521, G_error: 0.012744)
====&gt; Epoch: 60 Average loss: 0.014652 (D_error: 0.001627, G_error: 0.013025)
====&gt; Epoch: 61 Average loss: 0.014539 (D_error: 0.001506, G_error: 0.013033)
====&gt; Epoch: 62 Average loss: 0.014445 (D_error: 0.001540, G_error: 0.012905)
====&gt; Epoch: 63 Average loss: 0.014385 (D_error: 0.001506, G_error: 0.012879)
====&gt; Epoch: 64 Average loss: 0.014808 (D_error: 0.001486, G_error: 0.013322)
====&gt; Epoch: 65 Average loss: 0.014533 (D_error: 0.001507, G_error: 0.013026)
====&gt; Epoch: 66 Average loss: 0.014668 (D_error: 0.001443, G_error: 0.013226)
====&gt; Epoch: 67 Average loss: 0.014753 (D_error: 0.001462, G_error: 0.013291)
====&gt; Epoch: 68 Average loss: 0.014728 (D_error: 0.001522, G_error: 0.013206)
====&gt; Epoch: 69 Average loss: 0.015186 (D_error: 0.001461, G_error: 0.013725)
====&gt; Epoch: 70 Average loss: 0.014812 (D_error: 0.001473, G_error: 0.013340)
====&gt; Epoch: 71 Average loss: 0.015048 (D_error: 0.001436, G_error: 0.013612)
====&gt; Epoch: 72 Average loss: 0.015458 (D_error: 0.001345, G_error: 0.014113)
====&gt; Epoch: 73 Average loss: 0.015654 (D_error: 0.001328, G_error: 0.014327)
====&gt; Epoch: 74 Average loss: 0.015795 (D_error: 0.001309, G_error: 0.014486)
====&gt; Epoch: 75 Average loss: 0.015665 (D_error: 0.001308, G_error: 0.014357)
====&gt; Epoch: 76 Average loss: 0.015843 (D_error: 0.001342, G_error: 0.014500)
====&gt; Epoch: 77 Average loss: 0.015952 (D_error: 0.001293, G_error: 0.014660)
====&gt; Epoch: 78 Average loss: 0.015914 (D_error: 0.001269, G_error: 0.014644)
====&gt; Epoch: 79 Average loss: 0.016237 (D_error: 0.001226, G_error: 0.015011)
====&gt; Epoch: 80 Average loss: 0.016566 (D_error: 0.001248, G_error: 0.015318)
====&gt; Epoch: 81 Average loss: 0.016607 (D_error: 0.001260, G_error: 0.015347)
====&gt; Epoch: 82 Average loss: 0.016465 (D_error: 0.001172, G_error: 0.015293)
====&gt; Epoch: 83 Average loss: 0.016949 (D_error: 0.001194, G_error: 0.015755)
====&gt; Epoch: 84 Average loss: 0.016940 (D_error: 0.001156, G_error: 0.015784)
====&gt; Epoch: 85 Average loss: 0.016884 (D_error: 0.001140, G_error: 0.015743)
====&gt; Epoch: 86 Average loss: 0.017056 (D_error: 0.001098, G_error: 0.015959)
====&gt; Epoch: 87 Average loss: 0.017155 (D_error: 0.001112, G_error: 0.016043)
====&gt; Epoch: 88 Average loss: 0.017173 (D_error: 0.001101, G_error: 0.016071)
====&gt; Epoch: 89 Average loss: 0.017179 (D_error: 0.001102, G_error: 0.016077)
====&gt; Epoch: 90 Average loss: 0.016958 (D_error: 0.001089, G_error: 0.015869)
====&gt; Epoch: 91 Average loss: 0.017053 (D_error: 0.001102, G_error: 0.015952)
====&gt; Epoch: 92 Average loss: 0.017441 (D_error: 0.001068, G_error: 0.016373)
====&gt; Epoch: 93 Average loss: 0.017577 (D_error: 0.001114, G_error: 0.016463)
====&gt; Epoch: 94 Average loss: 0.017384 (D_error: 0.001056, G_error: 0.016328)
====&gt; Epoch: 95 Average loss: 0.017796 (D_error: 0.001058, G_error: 0.016738)
====&gt; Epoch: 96 Average loss: 0.017723 (D_error: 0.001064, G_error: 0.016659)
====&gt; Epoch: 97 Average loss: 0.017897 (D_error: 0.001017, G_error: 0.016880)
====&gt; Epoch: 98 Average loss: 0.017764 (D_error: 0.001019, G_error: 0.016745)
====&gt; Epoch: 99 Average loss: 0.017495 (D_error: 0.001073, G_error: 0.016422)
====&gt; Epoch: 100 Average loss: 0.017784 (D_error: 0.001026, G_error: 0.016758)
====&gt; Epoch: 101 Average loss: 0.018088 (D_error: 0.001039, G_error: 0.017049)
====&gt; Epoch: 102 Average loss: 0.017699 (D_error: 0.001063, G_error: 0.016637)
====&gt; Epoch: 103 Average loss: 0.017511 (D_error: 0.001040, G_error: 0.016472)
====&gt; Epoch: 104 Average loss: 0.018074 (D_error: 0.001026, G_error: 0.017048)
====&gt; Epoch: 105 Average loss: 0.017617 (D_error: 0.001044, G_error: 0.016573)
====&gt; Epoch: 106 Average loss: 0.018061 (D_error: 0.001004, G_error: 0.017057)
====&gt; Epoch: 107 Average loss: 0.017484 (D_error: 0.001044, G_error: 0.016440)
====&gt; Epoch: 108 Average loss: 0.018194 (D_error: 0.001112, G_error: 0.017082)
====&gt; Epoch: 109 Average loss: 0.017651 (D_error: 0.001020, G_error: 0.016631)
====&gt; Epoch: 110 Average loss: 0.018023 (D_error: 0.001002, G_error: 0.017021)
====&gt; Epoch: 111 Average loss: 0.018217 (D_error: 0.000966, G_error: 0.017252)
====&gt; Epoch: 112 Average loss: 0.018138 (D_error: 0.000990, G_error: 0.017148)
====&gt; Epoch: 113 Average loss: 0.017834 (D_error: 0.001016, G_error: 0.016818)
====&gt; Epoch: 114 Average loss: 0.017741 (D_error: 0.001058, G_error: 0.016683)
====&gt; Epoch: 115 Average loss: 0.017715 (D_error: 0.001004, G_error: 0.016711)
====&gt; Epoch: 116 Average loss: 0.018108 (D_error: 0.001013, G_error: 0.017095)
====&gt; Epoch: 117 Average loss: 0.018262 (D_error: 0.001062, G_error: 0.017200)
====&gt; Epoch: 118 Average loss: 0.018157 (D_error: 0.001024, G_error: 0.017133)
====&gt; Epoch: 119 Average loss: 0.018061 (D_error: 0.001019, G_error: 0.017042)
====&gt; Epoch: 120 Average loss: 0.017999 (D_error: 0.000985, G_error: 0.017014)
====&gt; Epoch: 121 Average loss: 0.018171 (D_error: 0.000986, G_error: 0.017185)
====&gt; Epoch: 122 Average loss: 0.018332 (D_error: 0.001029, G_error: 0.017302)
====&gt; Epoch: 123 Average loss: 0.017983 (D_error: 0.001024, G_error: 0.016959)
====&gt; Epoch: 124 Average loss: 0.018170 (D_error: 0.001000, G_error: 0.017170)
====&gt; Epoch: 125 Average loss: 0.018097 (D_error: 0.000997, G_error: 0.017100)
====&gt; Epoch: 126 Average loss: 0.018026 (D_error: 0.001057, G_error: 0.016969)
====&gt; Epoch: 127 Average loss: 0.018347 (D_error: 0.001038, G_error: 0.017309)
====&gt; Epoch: 128 Average loss: 0.018174 (D_error: 0.001000, G_error: 0.017174)
====&gt; Epoch: 129 Average loss: 0.017822 (D_error: 0.000997, G_error: 0.016826)
====&gt; Epoch: 130 Average loss: 0.017821 (D_error: 0.000979, G_error: 0.016842)
====&gt; Epoch: 131 Average loss: 0.018226 (D_error: 0.001065, G_error: 0.017160)
====&gt; Epoch: 132 Average loss: 0.017687 (D_error: 0.001032, G_error: 0.016655)
====&gt; Epoch: 133 Average loss: 0.017993 (D_error: 0.000984, G_error: 0.017008)
====&gt; Epoch: 134 Average loss: 0.018018 (D_error: 0.001004, G_error: 0.017013)
====&gt; Epoch: 135 Average loss: 0.018900 (D_error: 0.000971, G_error: 0.017929)
====&gt; Epoch: 136 Average loss: 0.018362 (D_error: 0.000984, G_error: 0.017378)
====&gt; Epoch: 137 Average loss: 0.018550 (D_error: 0.000976, G_error: 0.017574)
====&gt; Epoch: 138 Average loss: 0.018233 (D_error: 0.000991, G_error: 0.017242)
====&gt; Epoch: 139 Average loss: 0.018189 (D_error: 0.000951, G_error: 0.017238)
====&gt; Epoch: 140 Average loss: 0.017951 (D_error: 0.000990, G_error: 0.016961)
====&gt; Epoch: 141 Average loss: 0.018234 (D_error: 0.001059, G_error: 0.017174)
====&gt; Epoch: 142 Average loss: 0.018046 (D_error: 0.001062, G_error: 0.016983)
====&gt; Epoch: 143 Average loss: 0.017858 (D_error: 0.000990, G_error: 0.016868)
====&gt; Epoch: 144 Average loss: 0.017907 (D_error: 0.001038, G_error: 0.016869)
====&gt; Epoch: 145 Average loss: 0.018020 (D_error: 0.001020, G_error: 0.017000)
====&gt; Epoch: 146 Average loss: 0.018373 (D_error: 0.000989, G_error: 0.017383)
====&gt; Epoch: 147 Average loss: 0.017580 (D_error: 0.000995, G_error: 0.016585)
====&gt; Epoch: 148 Average loss: 0.018428 (D_error: 0.001060, G_error: 0.017369)
====&gt; Epoch: 149 Average loss: 0.017944 (D_error: 0.000997, G_error: 0.016948)
====&gt; Epoch: 150 Average loss: 0.018333 (D_error: 0.000998, G_error: 0.017335)
====&gt; Epoch: 151 Average loss: 0.018717 (D_error: 0.001053, G_error: 0.017665)
====&gt; Epoch: 152 Average loss: 0.017669 (D_error: 0.001059, G_error: 0.016610)
====&gt; Epoch: 153 Average loss: 0.017882 (D_error: 0.001045, G_error: 0.016837)
====&gt; Epoch: 154 Average loss: 0.017780 (D_error: 0.001012, G_error: 0.016767)
====&gt; Epoch: 155 Average loss: 0.018159 (D_error: 0.000991, G_error: 0.017169)
====&gt; Epoch: 156 Average loss: 0.018246 (D_error: 0.000998, G_error: 0.017248)
====&gt; Epoch: 157 Average loss: 0.018200 (D_error: 0.001069, G_error: 0.017131)
====&gt; Epoch: 158 Average loss: 0.018493 (D_error: 0.001030, G_error: 0.017464)
====&gt; Epoch: 159 Average loss: 0.018440 (D_error: 0.001029, G_error: 0.017411)
====&gt; Epoch: 160 Average loss: 0.018309 (D_error: 0.001022, G_error: 0.017288)
====&gt; Epoch: 161 Average loss: 0.017720 (D_error: 0.000995, G_error: 0.016725)
====&gt; Epoch: 162 Average loss: 0.018151 (D_error: 0.001003, G_error: 0.017148)
====&gt; Epoch: 163 Average loss: 0.017964 (D_error: 0.001059, G_error: 0.016905)
====&gt; Epoch: 164 Average loss: 0.017532 (D_error: 0.001039, G_error: 0.016493)
====&gt; Epoch: 165 Average loss: 0.017897 (D_error: 0.001079, G_error: 0.016819)
====&gt; Epoch: 166 Average loss: 0.017851 (D_error: 0.001018, G_error: 0.016833)
====&gt; Epoch: 167 Average loss: 0.017842 (D_error: 0.001033, G_error: 0.016809)
====&gt; Epoch: 168 Average loss: 0.018644 (D_error: 0.001032, G_error: 0.017612)
====&gt; Epoch: 169 Average loss: 0.018260 (D_error: 0.000983, G_error: 0.017277)
====&gt; Epoch: 170 Average loss: 0.017722 (D_error: 0.001107, G_error: 0.016615)
====&gt; Epoch: 171 Average loss: 0.018053 (D_error: 0.001041, G_error: 0.017011)
====&gt; Epoch: 172 Average loss: 0.017790 (D_error: 0.001013, G_error: 0.016777)
====&gt; Epoch: 173 Average loss: 0.018498 (D_error: 0.000984, G_error: 0.017514)
====&gt; Epoch: 174 Average loss: 0.018073 (D_error: 0.001057, G_error: 0.017016)
====&gt; Epoch: 175 Average loss: 0.017807 (D_error: 0.001027, G_error: 0.016779)
====&gt; Epoch: 176 Average loss: 0.017806 (D_error: 0.001072, G_error: 0.016734)
====&gt; Epoch: 177 Average loss: 0.018517 (D_error: 0.000970, G_error: 0.017547)
====&gt; Epoch: 178 Average loss: 0.018171 (D_error: 0.000997, G_error: 0.017175)
====&gt; Epoch: 179 Average loss: 0.017929 (D_error: 0.001048, G_error: 0.016881)
====&gt; Epoch: 180 Average loss: 0.017955 (D_error: 0.001097, G_error: 0.016858)
====&gt; Epoch: 181 Average loss: 0.018304 (D_error: 0.001061, G_error: 0.017243)
====&gt; Epoch: 182 Average loss: 0.017778 (D_error: 0.001041, G_error: 0.016737)
====&gt; Epoch: 183 Average loss: 0.017688 (D_error: 0.001036, G_error: 0.016652)
====&gt; Epoch: 184 Average loss: 0.017846 (D_error: 0.001065, G_error: 0.016781)
====&gt; Epoch: 185 Average loss: 0.017920 (D_error: 0.001108, G_error: 0.016812)
====&gt; Epoch: 186 Average loss: 0.018004 (D_error: 0.001048, G_error: 0.016957)
====&gt; Epoch: 187 Average loss: 0.017944 (D_error: 0.001073, G_error: 0.016871)
====&gt; Epoch: 188 Average loss: 0.017355 (D_error: 0.001093, G_error: 0.016262)
====&gt; Epoch: 189 Average loss: 0.017901 (D_error: 0.001086, G_error: 0.016815)
====&gt; Epoch: 190 Average loss: 0.017511 (D_error: 0.001059, G_error: 0.016452)
====&gt; Epoch: 191 Average loss: 0.017985 (D_error: 0.001154, G_error: 0.016831)
====&gt; Epoch: 192 Average loss: 0.017780 (D_error: 0.001048, G_error: 0.016732)
====&gt; Epoch: 193 Average loss: 0.017622 (D_error: 0.001070, G_error: 0.016552)
====&gt; Epoch: 194 Average loss: 0.017696 (D_error: 0.001064, G_error: 0.016632)
====&gt; Epoch: 195 Average loss: 0.017417 (D_error: 0.001067, G_error: 0.016351)
====&gt; Epoch: 196 Average loss: 0.017242 (D_error: 0.001111, G_error: 0.016131)
====&gt; Epoch: 197 Average loss: 0.017231 (D_error: 0.001124, G_error: 0.016107)
====&gt; Epoch: 198 Average loss: 0.017553 (D_error: 0.001063, G_error: 0.016490)
====&gt; Epoch: 199 Average loss: 0.017563 (D_error: 0.001060, G_error: 0.016503)
====&gt; Epoch: 200 Average loss: 0.017822 (D_error: 0.001096, G_error: 0.016726)
====&gt; Epoch: 201 Average loss: 0.017558 (D_error: 0.001161, G_error: 0.016397)
====&gt; Epoch: 202 Average loss: 0.017061 (D_error: 0.001184, G_error: 0.015877)
====&gt; Epoch: 203 Average loss: 0.017147 (D_error: 0.001083, G_error: 0.016064)
====&gt; Epoch: 204 Average loss: 0.017509 (D_error: 0.001176, G_error: 0.016333)
====&gt; Epoch: 205 Average loss: 0.017155 (D_error: 0.001233, G_error: 0.015922)
====&gt; Epoch: 206 Average loss: 0.016801 (D_error: 0.001159, G_error: 0.015642)
====&gt; Epoch: 207 Average loss: 0.016895 (D_error: 0.001097, G_error: 0.015799)
====&gt; Epoch: 208 Average loss: 0.016882 (D_error: 0.001155, G_error: 0.015726)
====&gt; Epoch: 209 Average loss: 0.017318 (D_error: 0.001147, G_error: 0.016172)
====&gt; Epoch: 210 Average loss: 0.016862 (D_error: 0.001137, G_error: 0.015724)
====&gt; Epoch: 211 Average loss: 0.017346 (D_error: 0.001077, G_error: 0.016269)
====&gt; Epoch: 212 Average loss: 0.017130 (D_error: 0.001149, G_error: 0.015981)
====&gt; Epoch: 213 Average loss: 0.016911 (D_error: 0.001188, G_error: 0.015723)
====&gt; Epoch: 214 Average loss: 0.017228 (D_error: 0.001162, G_error: 0.016065)
====&gt; Epoch: 215 Average loss: 0.017178 (D_error: 0.001199, G_error: 0.015979)
====&gt; Epoch: 216 Average loss: 0.016764 (D_error: 0.001163, G_error: 0.015601)
====&gt; Epoch: 217 Average loss: 0.016852 (D_error: 0.001116, G_error: 0.015736)
====&gt; Epoch: 218 Average loss: 0.017045 (D_error: 0.001155, G_error: 0.015890)
====&gt; Epoch: 219 Average loss: 0.016768 (D_error: 0.001199, G_error: 0.015569)
====&gt; Epoch: 220 Average loss: 0.016737 (D_error: 0.001214, G_error: 0.015524)
====&gt; Epoch: 221 Average loss: 0.016350 (D_error: 0.001173, G_error: 0.015177)
====&gt; Epoch: 222 Average loss: 0.017150 (D_error: 0.001188, G_error: 0.015962)
====&gt; Epoch: 223 Average loss: 0.017012 (D_error: 0.001174, G_error: 0.015838)
====&gt; Epoch: 224 Average loss: 0.016598 (D_error: 0.001194, G_error: 0.015405)
====&gt; Epoch: 225 Average loss: 0.016938 (D_error: 0.001188, G_error: 0.015750)
====&gt; Epoch: 226 Average loss: 0.016965 (D_error: 0.001151, G_error: 0.015814)
====&gt; Epoch: 227 Average loss: 0.016500 (D_error: 0.001196, G_error: 0.015304)
====&gt; Epoch: 228 Average loss: 0.016640 (D_error: 0.001176, G_error: 0.015465)
====&gt; Epoch: 229 Average loss: 0.016735 (D_error: 0.001165, G_error: 0.015570)
====&gt; Epoch: 230 Average loss: 0.016396 (D_error: 0.001177, G_error: 0.015219)
====&gt; Epoch: 231 Average loss: 0.016588 (D_error: 0.001199, G_error: 0.015389)
====&gt; Epoch: 232 Average loss: 0.016171 (D_error: 0.001178, G_error: 0.014993)
====&gt; Epoch: 233 Average loss: 0.016857 (D_error: 0.001184, G_error: 0.015673)
====&gt; Epoch: 234 Average loss: 0.016385 (D_error: 0.001223, G_error: 0.015162)
====&gt; Epoch: 235 Average loss: 0.016281 (D_error: 0.001203, G_error: 0.015078)
====&gt; Epoch: 236 Average loss: 0.016657 (D_error: 0.001229, G_error: 0.015428)
====&gt; Epoch: 237 Average loss: 0.016402 (D_error: 0.001183, G_error: 0.015219)
====&gt; Epoch: 238 Average loss: 0.016270 (D_error: 0.001184, G_error: 0.015087)
====&gt; Epoch: 239 Average loss: 0.016581 (D_error: 0.001357, G_error: 0.015224)
====&gt; Epoch: 240 Average loss: 0.016483 (D_error: 0.001215, G_error: 0.015268)
====&gt; Epoch: 241 Average loss: 0.016505 (D_error: 0.001177, G_error: 0.015328)
====&gt; Epoch: 242 Average loss: 0.016770 (D_error: 0.001177, G_error: 0.015593)
====&gt; Epoch: 243 Average loss: 0.016541 (D_error: 0.001218, G_error: 0.015324)
====&gt; Epoch: 244 Average loss: 0.016170 (D_error: 0.001218, G_error: 0.014953)
====&gt; Epoch: 245 Average loss: 0.016386 (D_error: 0.001240, G_error: 0.015147)
====&gt; Epoch: 246 Average loss: 0.016368 (D_error: 0.001256, G_error: 0.015112)
====&gt; Epoch: 247 Average loss: 0.016179 (D_error: 0.001239, G_error: 0.014940)
====&gt; Epoch: 248 Average loss: 0.016053 (D_error: 0.001268, G_error: 0.014785)
====&gt; Epoch: 249 Average loss: 0.016328 (D_error: 0.001238, G_error: 0.015090)
====&gt; Epoch: 250 Average loss: 0.016136 (D_error: 0.001186, G_error: 0.014950)
====&gt; Epoch: 251 Average loss: 0.015947 (D_error: 0.001270, G_error: 0.014677)
====&gt; Epoch: 252 Average loss: 0.016476 (D_error: 0.001263, G_error: 0.015213)
====&gt; Epoch: 253 Average loss: 0.015914 (D_error: 0.001263, G_error: 0.014651)
====&gt; Epoch: 254 Average loss: 0.016209 (D_error: 0.001244, G_error: 0.014965)
====&gt; Epoch: 255 Average loss: 0.016175 (D_error: 0.001171, G_error: 0.015004)
====&gt; Epoch: 256 Average loss: 0.016128 (D_error: 0.001235, G_error: 0.014893)
====&gt; Epoch: 257 Average loss: 0.016194 (D_error: 0.001287, G_error: 0.014908)
====&gt; Epoch: 258 Average loss: 0.015860 (D_error: 0.001353, G_error: 0.014506)
====&gt; Epoch: 259 Average loss: 0.016071 (D_error: 0.001307, G_error: 0.014764)
====&gt; Epoch: 260 Average loss: 0.015988 (D_error: 0.001271, G_error: 0.014717)
====&gt; Epoch: 261 Average loss: 0.015868 (D_error: 0.001264, G_error: 0.014604)
====&gt; Epoch: 262 Average loss: 0.016019 (D_error: 0.001310, G_error: 0.014710)
====&gt; Epoch: 263 Average loss: 0.016025 (D_error: 0.001268, G_error: 0.014757)
====&gt; Epoch: 264 Average loss: 0.015705 (D_error: 0.001244, G_error: 0.014461)
====&gt; Epoch: 265 Average loss: 0.016080 (D_error: 0.001312, G_error: 0.014768)
====&gt; Epoch: 266 Average loss: 0.015585 (D_error: 0.001289, G_error: 0.014296)
====&gt; Epoch: 267 Average loss: 0.015792 (D_error: 0.001350, G_error: 0.014441)
====&gt; Epoch: 268 Average loss: 0.015391 (D_error: 0.001279, G_error: 0.014112)
====&gt; Epoch: 269 Average loss: 0.015868 (D_error: 0.001375, G_error: 0.014493)
====&gt; Epoch: 270 Average loss: 0.016020 (D_error: 0.001372, G_error: 0.014648)
====&gt; Epoch: 271 Average loss: 0.015958 (D_error: 0.001264, G_error: 0.014693)
====&gt; Epoch: 272 Average loss: 0.016112 (D_error: 0.001304, G_error: 0.014808)
====&gt; Epoch: 273 Average loss: 0.015959 (D_error: 0.001315, G_error: 0.014644)
====&gt; Epoch: 274 Average loss: 0.015679 (D_error: 0.001366, G_error: 0.014314)
====&gt; Epoch: 275 Average loss: 0.015810 (D_error: 0.001424, G_error: 0.014386)
====&gt; Epoch: 276 Average loss: 0.015403 (D_error: 0.001333, G_error: 0.014070)
====&gt; Epoch: 277 Average loss: 0.015598 (D_error: 0.001320, G_error: 0.014277)
====&gt; Epoch: 278 Average loss: 0.015462 (D_error: 0.001289, G_error: 0.014172)
====&gt; Epoch: 279 Average loss: 0.015748 (D_error: 0.001338, G_error: 0.014410)
====&gt; Epoch: 280 Average loss: 0.015447 (D_error: 0.001395, G_error: 0.014052)
====&gt; Epoch: 281 Average loss: 0.015357 (D_error: 0.001344, G_error: 0.014012)
====&gt; Epoch: 282 Average loss: 0.015365 (D_error: 0.001393, G_error: 0.013972)
====&gt; Epoch: 283 Average loss: 0.015927 (D_error: 0.001402, G_error: 0.014525)
====&gt; Epoch: 284 Average loss: 0.015367 (D_error: 0.001389, G_error: 0.013978)
====&gt; Epoch: 285 Average loss: 0.015229 (D_error: 0.001419, G_error: 0.013810)
====&gt; Epoch: 286 Average loss: 0.015224 (D_error: 0.001409, G_error: 0.013816)
====&gt; Epoch: 287 Average loss: 0.015178 (D_error: 0.001403, G_error: 0.013775)
====&gt; Epoch: 288 Average loss: 0.015372 (D_error: 0.001360, G_error: 0.014012)
====&gt; Epoch: 289 Average loss: 0.015191 (D_error: 0.001384, G_error: 0.013807)
====&gt; Epoch: 290 Average loss: 0.015517 (D_error: 0.001336, G_error: 0.014181)
====&gt; Epoch: 291 Average loss: 0.015122 (D_error: 0.001433, G_error: 0.013689)
====&gt; Epoch: 292 Average loss: 0.015545 (D_error: 0.001395, G_error: 0.014150)
====&gt; Epoch: 293 Average loss: 0.015370 (D_error: 0.001383, G_error: 0.013987)
====&gt; Epoch: 294 Average loss: 0.015584 (D_error: 0.001380, G_error: 0.014204)
====&gt; Epoch: 295 Average loss: 0.015309 (D_error: 0.001356, G_error: 0.013953)
====&gt; Epoch: 296 Average loss: 0.015436 (D_error: 0.001451, G_error: 0.013985)
====&gt; Epoch: 297 Average loss: 0.015418 (D_error: 0.001401, G_error: 0.014017)
====&gt; Epoch: 298 Average loss: 0.015201 (D_error: 0.001447, G_error: 0.013754)
====&gt; Epoch: 299 Average loss: 0.014956 (D_error: 0.001412, G_error: 0.013544)
====&gt; Epoch: 300 Average loss: 0.015159 (D_error: 0.001457, G_error: 0.013702)
====&gt; Epoch: 301 Average loss: 0.015092 (D_error: 0.001473, G_error: 0.013619)
====&gt; Epoch: 302 Average loss: 0.015128 (D_error: 0.001479, G_error: 0.013649)
====&gt; Epoch: 303 Average loss: 0.015223 (D_error: 0.001493, G_error: 0.013730)
====&gt; Epoch: 304 Average loss: 0.015166 (D_error: 0.001415, G_error: 0.013751)
====&gt; Epoch: 305 Average loss: 0.015243 (D_error: 0.001493, G_error: 0.013750)
====&gt; Epoch: 306 Average loss: 0.014879 (D_error: 0.001448, G_error: 0.013431)
====&gt; Epoch: 307 Average loss: 0.014668 (D_error: 0.001417, G_error: 0.013251)
====&gt; Epoch: 308 Average loss: 0.014855 (D_error: 0.001423, G_error: 0.013432)
====&gt; Epoch: 309 Average loss: 0.015383 (D_error: 0.001408, G_error: 0.013974)
====&gt; Epoch: 310 Average loss: 0.015006 (D_error: 0.001357, G_error: 0.013649)
====&gt; Epoch: 311 Average loss: 0.015426 (D_error: 0.001411, G_error: 0.014016)
====&gt; Epoch: 312 Average loss: 0.015688 (D_error: 0.001383, G_error: 0.014306)
====&gt; Epoch: 313 Average loss: 0.014836 (D_error: 0.001448, G_error: 0.013388)
====&gt; Epoch: 314 Average loss: 0.014978 (D_error: 0.001423, G_error: 0.013555)
====&gt; Epoch: 315 Average loss: 0.014942 (D_error: 0.001465, G_error: 0.013478)
====&gt; Epoch: 316 Average loss: 0.015017 (D_error: 0.001480, G_error: 0.013537)
====&gt; Epoch: 317 Average loss: 0.014991 (D_error: 0.001418, G_error: 0.013573)
====&gt; Epoch: 318 Average loss: 0.015105 (D_error: 0.001436, G_error: 0.013669)
====&gt; Epoch: 319 Average loss: 0.014994 (D_error: 0.001417, G_error: 0.013577)
====&gt; Epoch: 320 Average loss: 0.015052 (D_error: 0.001487, G_error: 0.013565)
====&gt; Epoch: 321 Average loss: 0.014928 (D_error: 0.001435, G_error: 0.013492)
====&gt; Epoch: 322 Average loss: 0.014828 (D_error: 0.001418, G_error: 0.013410)
====&gt; Epoch: 323 Average loss: 0.014982 (D_error: 0.001380, G_error: 0.013602)
====&gt; Epoch: 324 Average loss: 0.015062 (D_error: 0.001507, G_error: 0.013556)
====&gt; Epoch: 325 Average loss: 0.014901 (D_error: 0.001509, G_error: 0.013392)
====&gt; Epoch: 326 Average loss: 0.014984 (D_error: 0.001428, G_error: 0.013556)
====&gt; Epoch: 327 Average loss: 0.015089 (D_error: 0.001503, G_error: 0.013587)
====&gt; Epoch: 328 Average loss: 0.014990 (D_error: 0.001411, G_error: 0.013579)
====&gt; Epoch: 329 Average loss: 0.014750 (D_error: 0.001426, G_error: 0.013324)
====&gt; Epoch: 330 Average loss: 0.014836 (D_error: 0.001417, G_error: 0.013419)
====&gt; Epoch: 331 Average loss: 0.015190 (D_error: 0.001452, G_error: 0.013738)
====&gt; Epoch: 332 Average loss: 0.014941 (D_error: 0.001504, G_error: 0.013438)
====&gt; Epoch: 333 Average loss: 0.014728 (D_error: 0.001460, G_error: 0.013268)
====&gt; Epoch: 334 Average loss: 0.015295 (D_error: 0.001499, G_error: 0.013796)
====&gt; Epoch: 335 Average loss: 0.014584 (D_error: 0.001515, G_error: 0.013069)
====&gt; Epoch: 336 Average loss: 0.014439 (D_error: 0.001533, G_error: 0.012905)
====&gt; Epoch: 337 Average loss: 0.014579 (D_error: 0.001510, G_error: 0.013069)
====&gt; Epoch: 338 Average loss: 0.014695 (D_error: 0.001536, G_error: 0.013159)
====&gt; Epoch: 339 Average loss: 0.014522 (D_error: 0.001516, G_error: 0.013006)
====&gt; Epoch: 340 Average loss: 0.014517 (D_error: 0.001499, G_error: 0.013018)
====&gt; Epoch: 341 Average loss: 0.014375 (D_error: 0.001467, G_error: 0.012909)
====&gt; Epoch: 342 Average loss: 0.014443 (D_error: 0.001541, G_error: 0.012902)
====&gt; Epoch: 343 Average loss: 0.014374 (D_error: 0.001564, G_error: 0.012810)
====&gt; Epoch: 344 Average loss: 0.014628 (D_error: 0.001611, G_error: 0.013017)
====&gt; Epoch: 345 Average loss: 0.014329 (D_error: 0.001514, G_error: 0.012815)
====&gt; Epoch: 346 Average loss: 0.014761 (D_error: 0.001496, G_error: 0.013264)
====&gt; Epoch: 347 Average loss: 0.014423 (D_error: 0.001582, G_error: 0.012841)
====&gt; Epoch: 348 Average loss: 0.014474 (D_error: 0.001624, G_error: 0.012850)
====&gt; Epoch: 349 Average loss: 0.014240 (D_error: 0.001524, G_error: 0.012716)
====&gt; Epoch: 350 Average loss: 0.014535 (D_error: 0.001544, G_error: 0.012990)
====&gt; Epoch: 351 Average loss: 0.014521 (D_error: 0.001549, G_error: 0.012972)
====&gt; Epoch: 352 Average loss: 0.014344 (D_error: 0.001592, G_error: 0.012752)
====&gt; Epoch: 353 Average loss: 0.014387 (D_error: 0.001545, G_error: 0.012842)
====&gt; Epoch: 354 Average loss: 0.014202 (D_error: 0.001493, G_error: 0.012708)
====&gt; Epoch: 355 Average loss: 0.014562 (D_error: 0.001534, G_error: 0.013029)
====&gt; Epoch: 356 Average loss: 0.014602 (D_error: 0.001616, G_error: 0.012986)
====&gt; Epoch: 357 Average loss: 0.014454 (D_error: 0.001541, G_error: 0.012913)
====&gt; Epoch: 358 Average loss: 0.014258 (D_error: 0.001549, G_error: 0.012709)
====&gt; Epoch: 359 Average loss: 0.014409 (D_error: 0.001511, G_error: 0.012898)
====&gt; Epoch: 360 Average loss: 0.014557 (D_error: 0.001529, G_error: 0.013029)
====&gt; Epoch: 361 Average loss: 0.014501 (D_error: 0.001598, G_error: 0.012902)
====&gt; Epoch: 362 Average loss: 0.014388 (D_error: 0.001546, G_error: 0.012842)
====&gt; Epoch: 363 Average loss: 0.014590 (D_error: 0.001586, G_error: 0.013004)
====&gt; Epoch: 364 Average loss: 0.014447 (D_error: 0.001545, G_error: 0.012902)
====&gt; Epoch: 365 Average loss: 0.014197 (D_error: 0.001530, G_error: 0.012667)
====&gt; Epoch: 366 Average loss: 0.014313 (D_error: 0.001527, G_error: 0.012787)
====&gt; Epoch: 367 Average loss: 0.014332 (D_error: 0.001579, G_error: 0.012754)
====&gt; Epoch: 368 Average loss: 0.014527 (D_error: 0.001533, G_error: 0.012994)
====&gt; Epoch: 369 Average loss: 0.014434 (D_error: 0.001608, G_error: 0.012826)
====&gt; Epoch: 370 Average loss: 0.014244 (D_error: 0.001568, G_error: 0.012676)
====&gt; Epoch: 371 Average loss: 0.014258 (D_error: 0.001595, G_error: 0.012663)
====&gt; Epoch: 372 Average loss: 0.014149 (D_error: 0.001563, G_error: 0.012587)
====&gt; Epoch: 373 Average loss: 0.014575 (D_error: 0.001645, G_error: 0.012930)
====&gt; Epoch: 374 Average loss: 0.013924 (D_error: 0.001614, G_error: 0.012310)
====&gt; Epoch: 375 Average loss: 0.014026 (D_error: 0.001602, G_error: 0.012424)
====&gt; Epoch: 376 Average loss: 0.014480 (D_error: 0.001564, G_error: 0.012916)
====&gt; Epoch: 377 Average loss: 0.014509 (D_error: 0.001601, G_error: 0.012908)
====&gt; Epoch: 378 Average loss: 0.014110 (D_error: 0.001549, G_error: 0.012560)
====&gt; Epoch: 379 Average loss: 0.014406 (D_error: 0.001603, G_error: 0.012804)
====&gt; Epoch: 380 Average loss: 0.014465 (D_error: 0.001547, G_error: 0.012917)
====&gt; Epoch: 381 Average loss: 0.014178 (D_error: 0.001558, G_error: 0.012620)
====&gt; Epoch: 382 Average loss: 0.014526 (D_error: 0.001520, G_error: 0.013005)
====&gt; Epoch: 383 Average loss: 0.014409 (D_error: 0.001558, G_error: 0.012851)
====&gt; Epoch: 384 Average loss: 0.014226 (D_error: 0.001657, G_error: 0.012569)
====&gt; Epoch: 385 Average loss: 0.014230 (D_error: 0.001590, G_error: 0.012640)
====&gt; Epoch: 386 Average loss: 0.014101 (D_error: 0.001555, G_error: 0.012546)
====&gt; Epoch: 387 Average loss: 0.014285 (D_error: 0.001624, G_error: 0.012661)
====&gt; Epoch: 388 Average loss: 0.014216 (D_error: 0.001589, G_error: 0.012628)
====&gt; Epoch: 389 Average loss: 0.014078 (D_error: 0.001646, G_error: 0.012432)
====&gt; Epoch: 390 Average loss: 0.014209 (D_error: 0.001612, G_error: 0.012597)
====&gt; Epoch: 391 Average loss: 0.014223 (D_error: 0.001546, G_error: 0.012677)
====&gt; Epoch: 392 Average loss: 0.014129 (D_error: 0.001601, G_error: 0.012529)
====&gt; Epoch: 393 Average loss: 0.014173 (D_error: 0.001602, G_error: 0.012571)
====&gt; Epoch: 394 Average loss: 0.014058 (D_error: 0.001660, G_error: 0.012398)
====&gt; Epoch: 395 Average loss: 0.014296 (D_error: 0.001647, G_error: 0.012650)
====&gt; Epoch: 396 Average loss: 0.014177 (D_error: 0.001732, G_error: 0.012446)
====&gt; Epoch: 397 Average loss: 0.013838 (D_error: 0.001646, G_error: 0.012192)
====&gt; Epoch: 398 Average loss: 0.014210 (D_error: 0.001644, G_error: 0.012566)
====&gt; Epoch: 399 Average loss: 0.014097 (D_error: 0.001610, G_error: 0.012487)
====&gt; Epoch: 400 Average loss: 0.014241 (D_error: 0.001632, G_error: 0.012609)
====&gt; Epoch: 401 Average loss: 0.014015 (D_error: 0.001646, G_error: 0.012369)
====&gt; Epoch: 402 Average loss: 0.014387 (D_error: 0.001570, G_error: 0.012817)
====&gt; Epoch: 403 Average loss: 0.014274 (D_error: 0.001607, G_error: 0.012667)
====&gt; Epoch: 404 Average loss: 0.013935 (D_error: 0.001537, G_error: 0.012398)
====&gt; Epoch: 405 Average loss: 0.014065 (D_error: 0.001597, G_error: 0.012468)
====&gt; Epoch: 406 Average loss: 0.014022 (D_error: 0.001703, G_error: 0.012319)
====&gt; Epoch: 407 Average loss: 0.013988 (D_error: 0.001639, G_error: 0.012349)
====&gt; Epoch: 408 Average loss: 0.014017 (D_error: 0.001587, G_error: 0.012429)
====&gt; Epoch: 409 Average loss: 0.014066 (D_error: 0.001616, G_error: 0.012450)
====&gt; Epoch: 410 Average loss: 0.013996 (D_error: 0.001685, G_error: 0.012311)
====&gt; Epoch: 411 Average loss: 0.014050 (D_error: 0.001670, G_error: 0.012380)
====&gt; Epoch: 412 Average loss: 0.013808 (D_error: 0.001638, G_error: 0.012169)
====&gt; Epoch: 413 Average loss: 0.013839 (D_error: 0.001656, G_error: 0.012183)
====&gt; Epoch: 414 Average loss: 0.013976 (D_error: 0.001582, G_error: 0.012394)
====&gt; Epoch: 415 Average loss: 0.013994 (D_error: 0.001665, G_error: 0.012329)
====&gt; Epoch: 416 Average loss: 0.014012 (D_error: 0.001644, G_error: 0.012369)
====&gt; Epoch: 417 Average loss: 0.014007 (D_error: 0.001640, G_error: 0.012367)
====&gt; Epoch: 418 Average loss: 0.014129 (D_error: 0.001676, G_error: 0.012453)
====&gt; Epoch: 419 Average loss: 0.013578 (D_error: 0.001643, G_error: 0.011935)
====&gt; Epoch: 420 Average loss: 0.013978 (D_error: 0.001625, G_error: 0.012353)
====&gt; Epoch: 421 Average loss: 0.013923 (D_error: 0.001605, G_error: 0.012318)
====&gt; Epoch: 422 Average loss: 0.013856 (D_error: 0.001599, G_error: 0.012257)
====&gt; Epoch: 423 Average loss: 0.013854 (D_error: 0.001658, G_error: 0.012196)
====&gt; Epoch: 424 Average loss: 0.013854 (D_error: 0.001606, G_error: 0.012248)
====&gt; Epoch: 425 Average loss: 0.013801 (D_error: 0.001653, G_error: 0.012148)
====&gt; Epoch: 426 Average loss: 0.013836 (D_error: 0.001614, G_error: 0.012222)
====&gt; Epoch: 427 Average loss: 0.013948 (D_error: 0.001604, G_error: 0.012344)
====&gt; Epoch: 428 Average loss: 0.014152 (D_error: 0.001633, G_error: 0.012519)
====&gt; Epoch: 429 Average loss: 0.014050 (D_error: 0.001620, G_error: 0.012430)
====&gt; Epoch: 430 Average loss: 0.013888 (D_error: 0.001638, G_error: 0.012249)
====&gt; Epoch: 431 Average loss: 0.013855 (D_error: 0.001584, G_error: 0.012271)
====&gt; Epoch: 432 Average loss: 0.013797 (D_error: 0.001696, G_error: 0.012102)
====&gt; Epoch: 433 Average loss: 0.013956 (D_error: 0.001640, G_error: 0.012316)
====&gt; Epoch: 434 Average loss: 0.013690 (D_error: 0.001622, G_error: 0.012068)
====&gt; Epoch: 435 Average loss: 0.013930 (D_error: 0.001603, G_error: 0.012327)
====&gt; Epoch: 436 Average loss: 0.013798 (D_error: 0.001607, G_error: 0.012191)
====&gt; Epoch: 437 Average loss: 0.014027 (D_error: 0.001670, G_error: 0.012357)
====&gt; Epoch: 438 Average loss: 0.013797 (D_error: 0.001600, G_error: 0.012197)
====&gt; Epoch: 439 Average loss: 0.013830 (D_error: 0.001630, G_error: 0.012200)
====&gt; Epoch: 440 Average loss: 0.013873 (D_error: 0.001650, G_error: 0.012224)
====&gt; Epoch: 441 Average loss: 0.013973 (D_error: 0.001636, G_error: 0.012337)
====&gt; Epoch: 442 Average loss: 0.013823 (D_error: 0.001668, G_error: 0.012156)
====&gt; Epoch: 443 Average loss: 0.013736 (D_error: 0.001628, G_error: 0.012108)
====&gt; Epoch: 444 Average loss: 0.013915 (D_error: 0.001590, G_error: 0.012325)
====&gt; Epoch: 445 Average loss: 0.013827 (D_error: 0.001645, G_error: 0.012182)
====&gt; Epoch: 446 Average loss: 0.013795 (D_error: 0.001619, G_error: 0.012176)
====&gt; Epoch: 447 Average loss: 0.013879 (D_error: 0.001663, G_error: 0.012216)
====&gt; Epoch: 448 Average loss: 0.013891 (D_error: 0.001733, G_error: 0.012158)
====&gt; Epoch: 449 Average loss: 0.013862 (D_error: 0.001665, G_error: 0.012197)
====&gt; Epoch: 450 Average loss: 0.013562 (D_error: 0.001656, G_error: 0.011906)
====&gt; Epoch: 451 Average loss: 0.013881 (D_error: 0.001623, G_error: 0.012258)
====&gt; Epoch: 452 Average loss: 0.013728 (D_error: 0.001621, G_error: 0.012107)
====&gt; Epoch: 453 Average loss: 0.013858 (D_error: 0.001703, G_error: 0.012155)
====&gt; Epoch: 454 Average loss: 0.013732 (D_error: 0.001646, G_error: 0.012086)
====&gt; Epoch: 455 Average loss: 0.013796 (D_error: 0.001645, G_error: 0.012151)
====&gt; Epoch: 456 Average loss: 0.013638 (D_error: 0.001598, G_error: 0.012040)
====&gt; Epoch: 457 Average loss: 0.013717 (D_error: 0.001671, G_error: 0.012046)
====&gt; Epoch: 458 Average loss: 0.013785 (D_error: 0.001632, G_error: 0.012153)
====&gt; Epoch: 459 Average loss: 0.013601 (D_error: 0.001648, G_error: 0.011953)
====&gt; Epoch: 460 Average loss: 0.013746 (D_error: 0.001650, G_error: 0.012096)
====&gt; Epoch: 461 Average loss: 0.013733 (D_error: 0.001606, G_error: 0.012127)
====&gt; Epoch: 462 Average loss: 0.013782 (D_error: 0.001668, G_error: 0.012113)
====&gt; Epoch: 463 Average loss: 0.013782 (D_error: 0.001617, G_error: 0.012165)
====&gt; Epoch: 464 Average loss: 0.013820 (D_error: 0.001630, G_error: 0.012190)
====&gt; Epoch: 465 Average loss: 0.013920 (D_error: 0.001666, G_error: 0.012254)
====&gt; Epoch: 466 Average loss: 0.013713 (D_error: 0.001661, G_error: 0.012052)
====&gt; Epoch: 467 Average loss: 0.013631 (D_error: 0.001709, G_error: 0.011922)
====&gt; Epoch: 468 Average loss: 0.013788 (D_error: 0.001671, G_error: 0.012117)
====&gt; Epoch: 469 Average loss: 0.013529 (D_error: 0.001687, G_error: 0.011842)
====&gt; Epoch: 470 Average loss: 0.013756 (D_error: 0.001626, G_error: 0.012130)
====&gt; Epoch: 471 Average loss: 0.014106 (D_error: 0.001698, G_error: 0.012408)
====&gt; Epoch: 472 Average loss: 0.013695 (D_error: 0.001659, G_error: 0.012036)
====&gt; Epoch: 473 Average loss: 0.013840 (D_error: 0.001612, G_error: 0.012229)
====&gt; Epoch: 474 Average loss: 0.013716 (D_error: 0.001676, G_error: 0.012041)
====&gt; Epoch: 475 Average loss: 0.013523 (D_error: 0.001648, G_error: 0.011875)
====&gt; Epoch: 476 Average loss: 0.013715 (D_error: 0.001661, G_error: 0.012054)
====&gt; Epoch: 477 Average loss: 0.013583 (D_error: 0.001719, G_error: 0.011864)
====&gt; Epoch: 478 Average loss: 0.013650 (D_error: 0.001657, G_error: 0.011992)
====&gt; Epoch: 479 Average loss: 0.013698 (D_error: 0.001663, G_error: 0.012035)
====&gt; Epoch: 480 Average loss: 0.013530 (D_error: 0.001661, G_error: 0.011869)
====&gt; Epoch: 481 Average loss: 0.013920 (D_error: 0.001723, G_error: 0.012196)
====&gt; Epoch: 482 Average loss: 0.013565 (D_error: 0.001701, G_error: 0.011864)
====&gt; Epoch: 483 Average loss: 0.013689 (D_error: 0.001653, G_error: 0.012037)
====&gt; Epoch: 484 Average loss: 0.013661 (D_error: 0.001712, G_error: 0.011949)
====&gt; Epoch: 485 Average loss: 0.013557 (D_error: 0.001680, G_error: 0.011877)
====&gt; Epoch: 486 Average loss: 0.013780 (D_error: 0.001608, G_error: 0.012172)
====&gt; Epoch: 487 Average loss: 0.013752 (D_error: 0.001715, G_error: 0.012036)
====&gt; Epoch: 488 Average loss: 0.013755 (D_error: 0.001712, G_error: 0.012043)
====&gt; Epoch: 489 Average loss: 0.013765 (D_error: 0.001689, G_error: 0.012076)
====&gt; Epoch: 490 Average loss: 0.013540 (D_error: 0.001635, G_error: 0.011905)
====&gt; Epoch: 491 Average loss: 0.013563 (D_error: 0.001626, G_error: 0.011937)
====&gt; Epoch: 492 Average loss: 0.013565 (D_error: 0.001675, G_error: 0.011890)
====&gt; Epoch: 493 Average loss: 0.013520 (D_error: 0.001695, G_error: 0.011825)
====&gt; Epoch: 494 Average loss: 0.013668 (D_error: 0.001692, G_error: 0.011976)
====&gt; Epoch: 495 Average loss: 0.013609 (D_error: 0.001693, G_error: 0.011916)
====&gt; Epoch: 496 Average loss: 0.013689 (D_error: 0.001714, G_error: 0.011975)
====&gt; Epoch: 497 Average loss: 0.013565 (D_error: 0.001682, G_error: 0.011883)
====&gt; Epoch: 498 Average loss: 0.013632 (D_error: 0.001696, G_error: 0.011936)
====&gt; Epoch: 499 Average loss: 0.013600 (D_error: 0.001650, G_error: 0.011950)
====&gt; Epoch: 500 Average loss: 0.013482 (D_error: 0.001688, G_error: 0.011794)
====&gt; Epoch: 501 Average loss: 0.013612 (D_error: 0.001676, G_error: 0.011937)
====&gt; Epoch: 502 Average loss: 0.013633 (D_error: 0.001668, G_error: 0.011965)
====&gt; Epoch: 503 Average loss: 0.013634 (D_error: 0.001674, G_error: 0.011960)
====&gt; Epoch: 504 Average loss: 0.013638 (D_error: 0.001667, G_error: 0.011971)
====&gt; Epoch: 505 Average loss: 0.013429 (D_error: 0.001661, G_error: 0.011768)
====&gt; Epoch: 506 Average loss: 0.013602 (D_error: 0.001728, G_error: 0.011874)
====&gt; Epoch: 507 Average loss: 0.013445 (D_error: 0.001673, G_error: 0.011772)
====&gt; Epoch: 508 Average loss: 0.013440 (D_error: 0.001728, G_error: 0.011713)
====&gt; Epoch: 509 Average loss: 0.013543 (D_error: 0.001679, G_error: 0.011864)
====&gt; Epoch: 510 Average loss: 0.013521 (D_error: 0.001713, G_error: 0.011808)
====&gt; Epoch: 511 Average loss: 0.013368 (D_error: 0.001661, G_error: 0.011707)
====&gt; Epoch: 512 Average loss: 0.013628 (D_error: 0.001763, G_error: 0.011865)
====&gt; Epoch: 513 Average loss: 0.013586 (D_error: 0.001663, G_error: 0.011923)
====&gt; Epoch: 514 Average loss: 0.013650 (D_error: 0.001709, G_error: 0.011942)
====&gt; Epoch: 515 Average loss: 0.013591 (D_error: 0.001644, G_error: 0.011947)
====&gt; Epoch: 516 Average loss: 0.013187 (D_error: 0.001708, G_error: 0.011479)
====&gt; Epoch: 517 Average loss: 0.013277 (D_error: 0.001704, G_error: 0.011573)
====&gt; Epoch: 518 Average loss: 0.013403 (D_error: 0.001654, G_error: 0.011749)
====&gt; Epoch: 519 Average loss: 0.013734 (D_error: 0.001705, G_error: 0.012028)
====&gt; Epoch: 520 Average loss: 0.013533 (D_error: 0.001703, G_error: 0.011830)
====&gt; Epoch: 521 Average loss: 0.013453 (D_error: 0.001717, G_error: 0.011736)
====&gt; Epoch: 522 Average loss: 0.013606 (D_error: 0.001674, G_error: 0.011932)
====&gt; Epoch: 523 Average loss: 0.013598 (D_error: 0.001662, G_error: 0.011936)
====&gt; Epoch: 524 Average loss: 0.013622 (D_error: 0.001686, G_error: 0.011936)
====&gt; Epoch: 525 Average loss: 0.013336 (D_error: 0.001662, G_error: 0.011673)
====&gt; Epoch: 526 Average loss: 0.013422 (D_error: 0.001692, G_error: 0.011730)
====&gt; Epoch: 527 Average loss: 0.013542 (D_error: 0.001730, G_error: 0.011812)
====&gt; Epoch: 528 Average loss: 0.013355 (D_error: 0.001690, G_error: 0.011665)
====&gt; Epoch: 529 Average loss: 0.013467 (D_error: 0.001751, G_error: 0.011716)
====&gt; Epoch: 530 Average loss: 0.013426 (D_error: 0.001681, G_error: 0.011745)
====&gt; Epoch: 531 Average loss: 0.013438 (D_error: 0.001665, G_error: 0.011773)
====&gt; Epoch: 532 Average loss: 0.013554 (D_error: 0.001719, G_error: 0.011835)
====&gt; Epoch: 533 Average loss: 0.013472 (D_error: 0.001716, G_error: 0.011756)
====&gt; Epoch: 534 Average loss: 0.013447 (D_error: 0.001705, G_error: 0.011743)
====&gt; Epoch: 535 Average loss: 0.013238 (D_error: 0.001674, G_error: 0.011564)
====&gt; Epoch: 536 Average loss: 0.013529 (D_error: 0.001744, G_error: 0.011784)
====&gt; Epoch: 537 Average loss: 0.013578 (D_error: 0.001671, G_error: 0.011907)
====&gt; Epoch: 538 Average loss: 0.013473 (D_error: 0.001759, G_error: 0.011714)
====&gt; Epoch: 539 Average loss: 0.013471 (D_error: 0.001726, G_error: 0.011746)
====&gt; Epoch: 540 Average loss: 0.013452 (D_error: 0.001675, G_error: 0.011778)
====&gt; Epoch: 541 Average loss: 0.013306 (D_error: 0.001699, G_error: 0.011607)
====&gt; Epoch: 542 Average loss: 0.013439 (D_error: 0.001704, G_error: 0.011735)
====&gt; Epoch: 543 Average loss: 0.013351 (D_error: 0.001731, G_error: 0.011620)
====&gt; Epoch: 544 Average loss: 0.013415 (D_error: 0.001714, G_error: 0.011701)
====&gt; Epoch: 545 Average loss: 0.013364 (D_error: 0.001732, G_error: 0.011633)
====&gt; Epoch: 546 Average loss: 0.013385 (D_error: 0.001725, G_error: 0.011660)
====&gt; Epoch: 547 Average loss: 0.013357 (D_error: 0.001667, G_error: 0.011690)
====&gt; Epoch: 548 Average loss: 0.013420 (D_error: 0.001739, G_error: 0.011682)
====&gt; Epoch: 549 Average loss: 0.013522 (D_error: 0.001732, G_error: 0.011790)
====&gt; Epoch: 550 Average loss: 0.013482 (D_error: 0.001703, G_error: 0.011779)
====&gt; Epoch: 551 Average loss: 0.013559 (D_error: 0.001721, G_error: 0.011838)
====&gt; Epoch: 552 Average loss: 0.013305 (D_error: 0.001670, G_error: 0.011635)
====&gt; Epoch: 553 Average loss: 0.013604 (D_error: 0.001687, G_error: 0.011917)
====&gt; Epoch: 554 Average loss: 0.013652 (D_error: 0.001711, G_error: 0.011941)
====&gt; Epoch: 555 Average loss: 0.013271 (D_error: 0.001650, G_error: 0.011621)
====&gt; Epoch: 556 Average loss: 0.013538 (D_error: 0.001679, G_error: 0.011858)
====&gt; Epoch: 557 Average loss: 0.013520 (D_error: 0.001691, G_error: 0.011829)
====&gt; Epoch: 558 Average loss: 0.013243 (D_error: 0.001702, G_error: 0.011541)
====&gt; Epoch: 559 Average loss: 0.013447 (D_error: 0.001669, G_error: 0.011778)
====&gt; Epoch: 560 Average loss: 0.013294 (D_error: 0.001701, G_error: 0.011593)
====&gt; Epoch: 561 Average loss: 0.013444 (D_error: 0.001733, G_error: 0.011710)
====&gt; Epoch: 562 Average loss: 0.013419 (D_error: 0.001692, G_error: 0.011728)
====&gt; Epoch: 563 Average loss: 0.013380 (D_error: 0.001686, G_error: 0.011694)
====&gt; Epoch: 564 Average loss: 0.013535 (D_error: 0.001669, G_error: 0.011865)
====&gt; Epoch: 565 Average loss: 0.013367 (D_error: 0.001751, G_error: 0.011616)
====&gt; Epoch: 566 Average loss: 0.013283 (D_error: 0.001669, G_error: 0.011614)
====&gt; Epoch: 567 Average loss: 0.013540 (D_error: 0.001712, G_error: 0.011828)
====&gt; Epoch: 568 Average loss: 0.013230 (D_error: 0.001738, G_error: 0.011492)
====&gt; Epoch: 569 Average loss: 0.013371 (D_error: 0.001655, G_error: 0.011716)
====&gt; Epoch: 570 Average loss: 0.013608 (D_error: 0.001730, G_error: 0.011878)
====&gt; Epoch: 571 Average loss: 0.013442 (D_error: 0.001720, G_error: 0.011722)
====&gt; Epoch: 572 Average loss: 0.013358 (D_error: 0.001701, G_error: 0.011657)
====&gt; Epoch: 573 Average loss: 0.013210 (D_error: 0.001778, G_error: 0.011432)
====&gt; Epoch: 574 Average loss: 0.013414 (D_error: 0.001727, G_error: 0.011687)
====&gt; Epoch: 575 Average loss: 0.013144 (D_error: 0.001763, G_error: 0.011381)
====&gt; Epoch: 576 Average loss: 0.013243 (D_error: 0.001728, G_error: 0.011514)
====&gt; Epoch: 577 Average loss: 0.013425 (D_error: 0.001693, G_error: 0.011732)
====&gt; Epoch: 578 Average loss: 0.013412 (D_error: 0.001735, G_error: 0.011678)
====&gt; Epoch: 579 Average loss: 0.013367 (D_error: 0.001708, G_error: 0.011659)
====&gt; Epoch: 580 Average loss: 0.013370 (D_error: 0.001728, G_error: 0.011642)
====&gt; Epoch: 581 Average loss: 0.013265 (D_error: 0.001723, G_error: 0.011542)
====&gt; Epoch: 582 Average loss: 0.013338 (D_error: 0.001706, G_error: 0.011631)
====&gt; Epoch: 583 Average loss: 0.013178 (D_error: 0.001714, G_error: 0.011464)
====&gt; Epoch: 584 Average loss: 0.013383 (D_error: 0.001725, G_error: 0.011658)
====&gt; Epoch: 585 Average loss: 0.013475 (D_error: 0.001669, G_error: 0.011806)
====&gt; Epoch: 586 Average loss: 0.013252 (D_error: 0.001693, G_error: 0.011559)
====&gt; Epoch: 587 Average loss: 0.013290 (D_error: 0.001714, G_error: 0.011576)
====&gt; Epoch: 588 Average loss: 0.013657 (D_error: 0.001671, G_error: 0.011986)
====&gt; Epoch: 589 Average loss: 0.013250 (D_error: 0.001727, G_error: 0.011523)
====&gt; Epoch: 590 Average loss: 0.013395 (D_error: 0.001699, G_error: 0.011696)
====&gt; Epoch: 591 Average loss: 0.013517 (D_error: 0.001681, G_error: 0.011836)
====&gt; Epoch: 592 Average loss: 0.013280 (D_error: 0.001707, G_error: 0.011574)
====&gt; Epoch: 593 Average loss: 0.013280 (D_error: 0.001753, G_error: 0.011527)
====&gt; Epoch: 594 Average loss: 0.013132 (D_error: 0.001704, G_error: 0.011428)
====&gt; Epoch: 595 Average loss: 0.013450 (D_error: 0.001685, G_error: 0.011765)
====&gt; Epoch: 596 Average loss: 0.013438 (D_error: 0.001719, G_error: 0.011720)
====&gt; Epoch: 597 Average loss: 0.013404 (D_error: 0.001705, G_error: 0.011699)
====&gt; Epoch: 598 Average loss: 0.013185 (D_error: 0.001724, G_error: 0.011461)
====&gt; Epoch: 599 Average loss: 0.013518 (D_error: 0.001756, G_error: 0.011762)
====&gt; Epoch: 600 Average loss: 0.013311 (D_error: 0.001719, G_error: 0.011593)
====&gt; Epoch: 601 Average loss: 0.013359 (D_error: 0.001710, G_error: 0.011649)
====&gt; Epoch: 602 Average loss: 0.013390 (D_error: 0.001695, G_error: 0.011695)
====&gt; Epoch: 603 Average loss: 0.013242 (D_error: 0.001675, G_error: 0.011567)
====&gt; Epoch: 604 Average loss: 0.013363 (D_error: 0.001697, G_error: 0.011666)
====&gt; Epoch: 605 Average loss: 0.013200 (D_error: 0.001719, G_error: 0.011481)
====&gt; Epoch: 606 Average loss: 0.013475 (D_error: 0.001719, G_error: 0.011757)
====&gt; Epoch: 607 Average loss: 0.013204 (D_error: 0.001705, G_error: 0.011500)
====&gt; Epoch: 608 Average loss: 0.013203 (D_error: 0.001678, G_error: 0.011525)
====&gt; Epoch: 609 Average loss: 0.013351 (D_error: 0.001740, G_error: 0.011611)
====&gt; Epoch: 610 Average loss: 0.013353 (D_error: 0.001753, G_error: 0.011600)
====&gt; Epoch: 611 Average loss: 0.013240 (D_error: 0.001663, G_error: 0.011577)
====&gt; Epoch: 612 Average loss: 0.013509 (D_error: 0.001688, G_error: 0.011822)
====&gt; Epoch: 613 Average loss: 0.013301 (D_error: 0.001726, G_error: 0.011575)
====&gt; Epoch: 614 Average loss: 0.013296 (D_error: 0.001697, G_error: 0.011599)
====&gt; Epoch: 615 Average loss: 0.013328 (D_error: 0.001720, G_error: 0.011608)
====&gt; Epoch: 616 Average loss: 0.013242 (D_error: 0.001725, G_error: 0.011517)
====&gt; Epoch: 617 Average loss: 0.013162 (D_error: 0.001714, G_error: 0.011448)
====&gt; Epoch: 618 Average loss: 0.013525 (D_error: 0.001693, G_error: 0.011832)
====&gt; Epoch: 619 Average loss: 0.013356 (D_error: 0.001695, G_error: 0.011661)
====&gt; Epoch: 620 Average loss: 0.013360 (D_error: 0.001688, G_error: 0.011672)
====&gt; Epoch: 621 Average loss: 0.013326 (D_error: 0.001647, G_error: 0.011679)
====&gt; Epoch: 622 Average loss: 0.013248 (D_error: 0.001696, G_error: 0.011552)
====&gt; Epoch: 623 Average loss: 0.013318 (D_error: 0.001671, G_error: 0.011647)
====&gt; Epoch: 624 Average loss: 0.013492 (D_error: 0.001737, G_error: 0.011755)
====&gt; Epoch: 625 Average loss: 0.013423 (D_error: 0.001692, G_error: 0.011732)
====&gt; Epoch: 626 Average loss: 0.013281 (D_error: 0.001698, G_error: 0.011583)
====&gt; Epoch: 627 Average loss: 0.013325 (D_error: 0.001713, G_error: 0.011612)
====&gt; Epoch: 628 Average loss: 0.013262 (D_error: 0.001689, G_error: 0.011573)
====&gt; Epoch: 629 Average loss: 0.013139 (D_error: 0.001674, G_error: 0.011465)
====&gt; Epoch: 630 Average loss: 0.013384 (D_error: 0.001676, G_error: 0.011708)
====&gt; Epoch: 631 Average loss: 0.013430 (D_error: 0.001636, G_error: 0.011794)
====&gt; Epoch: 632 Average loss: 0.013355 (D_error: 0.001730, G_error: 0.011625)
====&gt; Epoch: 633 Average loss: 0.013394 (D_error: 0.001661, G_error: 0.011733)
====&gt; Epoch: 634 Average loss: 0.013381 (D_error: 0.001665, G_error: 0.011716)
====&gt; Epoch: 635 Average loss: 0.013445 (D_error: 0.001703, G_error: 0.011743)
====&gt; Epoch: 636 Average loss: 0.013374 (D_error: 0.001696, G_error: 0.011678)
====&gt; Epoch: 637 Average loss: 0.013519 (D_error: 0.001651, G_error: 0.011868)
====&gt; Epoch: 638 Average loss: 0.013385 (D_error: 0.001663, G_error: 0.011722)
====&gt; Epoch: 639 Average loss: 0.013368 (D_error: 0.001624, G_error: 0.011743)
====&gt; Epoch: 640 Average loss: 0.013265 (D_error: 0.001728, G_error: 0.011537)
====&gt; Epoch: 641 Average loss: 0.013366 (D_error: 0.001679, G_error: 0.011687)
====&gt; Epoch: 642 Average loss: 0.013424 (D_error: 0.001621, G_error: 0.011803)
====&gt; Epoch: 643 Average loss: 0.013419 (D_error: 0.001697, G_error: 0.011722)
====&gt; Epoch: 644 Average loss: 0.013435 (D_error: 0.001681, G_error: 0.011754)
====&gt; Epoch: 645 Average loss: 0.013402 (D_error: 0.001688, G_error: 0.011714)
====&gt; Epoch: 646 Average loss: 0.013487 (D_error: 0.001632, G_error: 0.011855)
====&gt; Epoch: 647 Average loss: 0.013617 (D_error: 0.001645, G_error: 0.011972)
====&gt; Epoch: 648 Average loss: 0.013466 (D_error: 0.001663, G_error: 0.011803)
====&gt; Epoch: 649 Average loss: 0.013365 (D_error: 0.001625, G_error: 0.011740)
====&gt; Epoch: 650 Average loss: 0.013409 (D_error: 0.001635, G_error: 0.011774)
====&gt; Epoch: 651 Average loss: 0.013515 (D_error: 0.001618, G_error: 0.011897)
====&gt; Epoch: 652 Average loss: 0.013717 (D_error: 0.001670, G_error: 0.012047)
====&gt; Epoch: 653 Average loss: 0.013469 (D_error: 0.001617, G_error: 0.011852)
====&gt; Epoch: 654 Average loss: 0.013504 (D_error: 0.001673, G_error: 0.011831)
====&gt; Epoch: 655 Average loss: 0.013522 (D_error: 0.001614, G_error: 0.011908)
====&gt; Epoch: 656 Average loss: 0.013514 (D_error: 0.001628, G_error: 0.011886)
====&gt; Epoch: 657 Average loss: 0.013596 (D_error: 0.001643, G_error: 0.011953)
====&gt; Epoch: 658 Average loss: 0.013569 (D_error: 0.001595, G_error: 0.011974)
====&gt; Epoch: 659 Average loss: 0.013442 (D_error: 0.001621, G_error: 0.011820)
====&gt; Epoch: 660 Average loss: 0.013509 (D_error: 0.001628, G_error: 0.011881)
====&gt; Epoch: 661 Average loss: 0.013289 (D_error: 0.001585, G_error: 0.011704)
====&gt; Epoch: 662 Average loss: 0.013677 (D_error: 0.001649, G_error: 0.012028)
====&gt; Epoch: 663 Average loss: 0.013589 (D_error: 0.001604, G_error: 0.011985)
====&gt; Epoch: 664 Average loss: 0.013490 (D_error: 0.001602, G_error: 0.011888)
====&gt; Epoch: 665 Average loss: 0.013557 (D_error: 0.001584, G_error: 0.011974)
====&gt; Epoch: 666 Average loss: 0.013571 (D_error: 0.001595, G_error: 0.011976)
====&gt; Epoch: 667 Average loss: 0.013801 (D_error: 0.001559, G_error: 0.012242)
====&gt; Epoch: 668 Average loss: 0.013703 (D_error: 0.001598, G_error: 0.012104)
====&gt; Epoch: 669 Average loss: 0.013708 (D_error: 0.001605, G_error: 0.012103)
====&gt; Epoch: 670 Average loss: 0.013523 (D_error: 0.001584, G_error: 0.011939)
====&gt; Epoch: 671 Average loss: 0.013702 (D_error: 0.001597, G_error: 0.012106)
====&gt; Epoch: 672 Average loss: 0.013747 (D_error: 0.001572, G_error: 0.012175)
====&gt; Epoch: 673 Average loss: 0.013533 (D_error: 0.001550, G_error: 0.011984)
====&gt; Epoch: 674 Average loss: 0.013707 (D_error: 0.001582, G_error: 0.012125)
====&gt; Epoch: 675 Average loss: 0.013882 (D_error: 0.001596, G_error: 0.012286)
====&gt; Epoch: 676 Average loss: 0.013763 (D_error: 0.001576, G_error: 0.012187)
====&gt; Epoch: 677 Average loss: 0.013808 (D_error: 0.001546, G_error: 0.012262)
====&gt; Epoch: 678 Average loss: 0.013840 (D_error: 0.001583, G_error: 0.012257)
====&gt; Epoch: 679 Average loss: 0.013682 (D_error: 0.001541, G_error: 0.012142)
====&gt; Epoch: 680 Average loss: 0.013788 (D_error: 0.001553, G_error: 0.012235)
====&gt; Epoch: 681 Average loss: 0.013749 (D_error: 0.001510, G_error: 0.012239)
====&gt; Epoch: 682 Average loss: 0.013939 (D_error: 0.001522, G_error: 0.012417)
====&gt; Epoch: 683 Average loss: 0.013843 (D_error: 0.001553, G_error: 0.012290)
====&gt; Epoch: 684 Average loss: 0.013812 (D_error: 0.001519, G_error: 0.012293)
====&gt; Epoch: 685 Average loss: 0.013880 (D_error: 0.001523, G_error: 0.012357)
====&gt; Epoch: 686 Average loss: 0.014072 (D_error: 0.001514, G_error: 0.012558)
====&gt; Epoch: 687 Average loss: 0.013866 (D_error: 0.001523, G_error: 0.012342)
====&gt; Epoch: 688 Average loss: 0.013808 (D_error: 0.001506, G_error: 0.012302)
====&gt; Epoch: 689 Average loss: 0.013857 (D_error: 0.001508, G_error: 0.012349)
====&gt; Epoch: 690 Average loss: 0.014058 (D_error: 0.001502, G_error: 0.012557)
====&gt; Epoch: 691 Average loss: 0.013750 (D_error: 0.001536, G_error: 0.012215)
====&gt; Epoch: 692 Average loss: 0.013974 (D_error: 0.001504, G_error: 0.012470)
====&gt; Epoch: 693 Average loss: 0.013898 (D_error: 0.001504, G_error: 0.012394)
====&gt; Epoch: 694 Average loss: 0.013857 (D_error: 0.001519, G_error: 0.012337)
====&gt; Epoch: 695 Average loss: 0.013942 (D_error: 0.001483, G_error: 0.012459)
====&gt; Epoch: 696 Average loss: 0.014011 (D_error: 0.001488, G_error: 0.012524)
====&gt; Epoch: 697 Average loss: 0.013937 (D_error: 0.001497, G_error: 0.012440)
====&gt; Epoch: 698 Average loss: 0.013865 (D_error: 0.001526, G_error: 0.012339)
====&gt; Epoch: 699 Average loss: 0.013891 (D_error: 0.001453, G_error: 0.012438)
====&gt; Epoch: 700 Average loss: 0.014199 (D_error: 0.001451, G_error: 0.012748)
====&gt; Epoch: 701 Average loss: 0.014089 (D_error: 0.001472, G_error: 0.012617)
====&gt; Epoch: 702 Average loss: 0.014220 (D_error: 0.001450, G_error: 0.012770)
====&gt; Epoch: 703 Average loss: 0.014080 (D_error: 0.001474, G_error: 0.012605)
====&gt; Epoch: 704 Average loss: 0.014179 (D_error: 0.001441, G_error: 0.012738)
====&gt; Epoch: 705 Average loss: 0.014287 (D_error: 0.001416, G_error: 0.012871)
====&gt; Epoch: 706 Average loss: 0.014290 (D_error: 0.001429, G_error: 0.012861)
====&gt; Epoch: 707 Average loss: 0.014085 (D_error: 0.001454, G_error: 0.012630)
====&gt; Epoch: 708 Average loss: 0.014298 (D_error: 0.001433, G_error: 0.012865)
====&gt; Epoch: 709 Average loss: 0.014353 (D_error: 0.001440, G_error: 0.012913)
====&gt; Epoch: 710 Average loss: 0.014177 (D_error: 0.001415, G_error: 0.012762)
====&gt; Epoch: 711 Average loss: 0.014209 (D_error: 0.001455, G_error: 0.012755)
====&gt; Epoch: 712 Average loss: 0.014204 (D_error: 0.001434, G_error: 0.012770)
====&gt; Epoch: 713 Average loss: 0.014392 (D_error: 0.001405, G_error: 0.012988)
====&gt; Epoch: 714 Average loss: 0.014396 (D_error: 0.001405, G_error: 0.012991)
====&gt; Epoch: 715 Average loss: 0.014436 (D_error: 0.001416, G_error: 0.013020)
====&gt; Epoch: 716 Average loss: 0.014416 (D_error: 0.001387, G_error: 0.013029)
====&gt; Epoch: 717 Average loss: 0.014447 (D_error: 0.001399, G_error: 0.013048)
====&gt; Epoch: 718 Average loss: 0.014485 (D_error: 0.001394, G_error: 0.013091)
====&gt; Epoch: 719 Average loss: 0.014473 (D_error: 0.001365, G_error: 0.013108)
====&gt; Epoch: 720 Average loss: 0.014690 (D_error: 0.001360, G_error: 0.013330)
====&gt; Epoch: 721 Average loss: 0.014643 (D_error: 0.001368, G_error: 0.013275)
====&gt; Epoch: 722 Average loss: 0.014388 (D_error: 0.001399, G_error: 0.012989)
====&gt; Epoch: 723 Average loss: 0.014497 (D_error: 0.001400, G_error: 0.013097)
====&gt; Epoch: 724 Average loss: 0.014574 (D_error: 0.001350, G_error: 0.013224)
====&gt; Epoch: 725 Average loss: 0.014448 (D_error: 0.001393, G_error: 0.013055)
====&gt; Epoch: 726 Average loss: 0.014553 (D_error: 0.001358, G_error: 0.013195)
====&gt; Epoch: 727 Average loss: 0.014523 (D_error: 0.001376, G_error: 0.013147)
====&gt; Epoch: 728 Average loss: 0.014590 (D_error: 0.001383, G_error: 0.013207)
====&gt; Epoch: 729 Average loss: 0.014556 (D_error: 0.001350, G_error: 0.013206)
====&gt; Epoch: 730 Average loss: 0.014765 (D_error: 0.001330, G_error: 0.013435)
====&gt; Epoch: 731 Average loss: 0.014749 (D_error: 0.001339, G_error: 0.013410)
====&gt; Epoch: 732 Average loss: 0.014796 (D_error: 0.001338, G_error: 0.013458)
====&gt; Epoch: 733 Average loss: 0.014697 (D_error: 0.001340, G_error: 0.013357)
====&gt; Epoch: 734 Average loss: 0.014638 (D_error: 0.001351, G_error: 0.013287)
====&gt; Epoch: 735 Average loss: 0.014850 (D_error: 0.001331, G_error: 0.013518)
====&gt; Epoch: 736 Average loss: 0.014852 (D_error: 0.001330, G_error: 0.013521)
====&gt; Epoch: 737 Average loss: 0.014876 (D_error: 0.001307, G_error: 0.013569)
====&gt; Epoch: 738 Average loss: 0.014879 (D_error: 0.001317, G_error: 0.013562)
====&gt; Epoch: 739 Average loss: 0.015139 (D_error: 0.001282, G_error: 0.013857)
====&gt; Epoch: 740 Average loss: 0.014895 (D_error: 0.001327, G_error: 0.013568)
====&gt; Epoch: 741 Average loss: 0.014995 (D_error: 0.001299, G_error: 0.013696)
====&gt; Epoch: 742 Average loss: 0.014841 (D_error: 0.001316, G_error: 0.013525)
====&gt; Epoch: 743 Average loss: 0.015200 (D_error: 0.001275, G_error: 0.013925)
====&gt; Epoch: 744 Average loss: 0.015199 (D_error: 0.001285, G_error: 0.013915)
====&gt; Epoch: 745 Average loss: 0.015198 (D_error: 0.001271, G_error: 0.013927)
====&gt; Epoch: 746 Average loss: 0.014930 (D_error: 0.001289, G_error: 0.013642)
====&gt; Epoch: 747 Average loss: 0.015105 (D_error: 0.001273, G_error: 0.013832)
====&gt; Epoch: 748 Average loss: 0.015055 (D_error: 0.001294, G_error: 0.013762)
====&gt; Epoch: 749 Average loss: 0.015075 (D_error: 0.001294, G_error: 0.013781)
====&gt; Epoch: 750 Average loss: 0.015312 (D_error: 0.001263, G_error: 0.014049)
====&gt; Epoch: 751 Average loss: 0.015407 (D_error: 0.001249, G_error: 0.014158)
====&gt; Epoch: 752 Average loss: 0.015486 (D_error: 0.001233, G_error: 0.014253)
====&gt; Epoch: 753 Average loss: 0.015895 (D_error: 0.001203, G_error: 0.014692)
====&gt; Epoch: 754 Average loss: 0.015332 (D_error: 0.001254, G_error: 0.014078)
====&gt; Epoch: 755 Average loss: 0.015254 (D_error: 0.001243, G_error: 0.014011)
====&gt; Epoch: 756 Average loss: 0.015404 (D_error: 0.001244, G_error: 0.014160)
====&gt; Epoch: 757 Average loss: 0.015602 (D_error: 0.001209, G_error: 0.014393)
====&gt; Epoch: 758 Average loss: 0.015407 (D_error: 0.001240, G_error: 0.014166)
====&gt; Epoch: 759 Average loss: 0.015421 (D_error: 0.001235, G_error: 0.014185)
====&gt; Epoch: 760 Average loss: 0.015651 (D_error: 0.001224, G_error: 0.014427)
====&gt; Epoch: 761 Average loss: 0.015658 (D_error: 0.001215, G_error: 0.014443)
====&gt; Epoch: 762 Average loss: 0.015679 (D_error: 0.001209, G_error: 0.014470)
====&gt; Epoch: 763 Average loss: 0.015333 (D_error: 0.001244, G_error: 0.014089)
====&gt; Epoch: 764 Average loss: 0.015789 (D_error: 0.001219, G_error: 0.014570)
====&gt; Epoch: 765 Average loss: 0.015636 (D_error: 0.001212, G_error: 0.014424)
====&gt; Epoch: 766 Average loss: 0.015594 (D_error: 0.001237, G_error: 0.014357)
====&gt; Epoch: 767 Average loss: 0.015680 (D_error: 0.001217, G_error: 0.014463)
====&gt; Epoch: 768 Average loss: 0.015805 (D_error: 0.001187, G_error: 0.014617)
====&gt; Epoch: 769 Average loss: 0.015955 (D_error: 0.001178, G_error: 0.014777)
====&gt; Epoch: 770 Average loss: 0.015906 (D_error: 0.001189, G_error: 0.014718)
====&gt; Epoch: 771 Average loss: 0.015778 (D_error: 0.001182, G_error: 0.014596)
====&gt; Epoch: 772 Average loss: 0.015872 (D_error: 0.001180, G_error: 0.014692)
====&gt; Epoch: 773 Average loss: 0.016202 (D_error: 0.001142, G_error: 0.015061)
====&gt; Epoch: 774 Average loss: 0.016031 (D_error: 0.001188, G_error: 0.014843)
====&gt; Epoch: 775 Average loss: 0.016045 (D_error: 0.001166, G_error: 0.014879)
====&gt; Epoch: 776 Average loss: 0.015848 (D_error: 0.001176, G_error: 0.014673)
====&gt; Epoch: 777 Average loss: 0.015637 (D_error: 0.001197, G_error: 0.014439)
====&gt; Epoch: 778 Average loss: 0.015905 (D_error: 0.001172, G_error: 0.014733)
====&gt; Epoch: 779 Average loss: 0.016018 (D_error: 0.001151, G_error: 0.014867)
====&gt; Epoch: 780 Average loss: 0.016412 (D_error: 0.001130, G_error: 0.015282)
====&gt; Epoch: 781 Average loss: 0.016134 (D_error: 0.001148, G_error: 0.014986)
====&gt; Epoch: 782 Average loss: 0.016164 (D_error: 0.001139, G_error: 0.015025)
====&gt; Epoch: 783 Average loss: 0.016369 (D_error: 0.001129, G_error: 0.015239)
====&gt; Epoch: 784 Average loss: 0.016364 (D_error: 0.001119, G_error: 0.015245)
====&gt; Epoch: 785 Average loss: 0.016257 (D_error: 0.001141, G_error: 0.015116)
====&gt; Epoch: 786 Average loss: 0.016758 (D_error: 0.001078, G_error: 0.015680)
====&gt; Epoch: 787 Average loss: 0.016103 (D_error: 0.001149, G_error: 0.014954)
====&gt; Epoch: 788 Average loss: 0.016461 (D_error: 0.001123, G_error: 0.015337)
====&gt; Epoch: 789 Average loss: 0.016193 (D_error: 0.001107, G_error: 0.015086)
====&gt; Epoch: 790 Average loss: 0.016474 (D_error: 0.001108, G_error: 0.015366)
====&gt; Epoch: 791 Average loss: 0.016513 (D_error: 0.001108, G_error: 0.015405)
====&gt; Epoch: 792 Average loss: 0.016725 (D_error: 0.001069, G_error: 0.015656)
====&gt; Epoch: 793 Average loss: 0.016381 (D_error: 0.001117, G_error: 0.015264)
====&gt; Epoch: 794 Average loss: 0.016495 (D_error: 0.001093, G_error: 0.015402)
====&gt; Epoch: 795 Average loss: 0.016381 (D_error: 0.001103, G_error: 0.015278)
====&gt; Epoch: 796 Average loss: 0.016782 (D_error: 0.001076, G_error: 0.015706)
====&gt; Epoch: 797 Average loss: 0.016743 (D_error: 0.001087, G_error: 0.015656)
====&gt; Epoch: 798 Average loss: 0.016343 (D_error: 0.001123, G_error: 0.015220)
====&gt; Epoch: 799 Average loss: 0.016686 (D_error: 0.001070, G_error: 0.015616)
====&gt; Epoch: 800 Average loss: 0.016511 (D_error: 0.001083, G_error: 0.015428)
====&gt; Epoch: 801 Average loss: 0.016853 (D_error: 0.001074, G_error: 0.015779)
====&gt; Epoch: 802 Average loss: 0.017017 (D_error: 0.001047, G_error: 0.015970)
====&gt; Epoch: 803 Average loss: 0.016908 (D_error: 0.001064, G_error: 0.015844)
====&gt; Epoch: 804 Average loss: 0.016797 (D_error: 0.001056, G_error: 0.015741)
====&gt; Epoch: 805 Average loss: 0.017032 (D_error: 0.001056, G_error: 0.015975)
====&gt; Epoch: 806 Average loss: 0.016978 (D_error: 0.001065, G_error: 0.015913)
====&gt; Epoch: 807 Average loss: 0.016789 (D_error: 0.001066, G_error: 0.015723)
====&gt; Epoch: 808 Average loss: 0.016825 (D_error: 0.001054, G_error: 0.015771)
====&gt; Epoch: 809 Average loss: 0.016908 (D_error: 0.001049, G_error: 0.015859)
====&gt; Epoch: 810 Average loss: 0.017095 (D_error: 0.001044, G_error: 0.016051)
====&gt; Epoch: 811 Average loss: 0.016765 (D_error: 0.001060, G_error: 0.015704)
====&gt; Epoch: 812 Average loss: 0.016862 (D_error: 0.001054, G_error: 0.015808)
====&gt; Epoch: 813 Average loss: 0.017178 (D_error: 0.001010, G_error: 0.016168)
====&gt; Epoch: 814 Average loss: 0.017496 (D_error: 0.001032, G_error: 0.016464)
====&gt; Epoch: 815 Average loss: 0.017207 (D_error: 0.001013, G_error: 0.016194)
====&gt; Epoch: 816 Average loss: 0.017506 (D_error: 0.001010, G_error: 0.016495)
====&gt; Epoch: 817 Average loss: 0.017318 (D_error: 0.001016, G_error: 0.016302)
====&gt; Epoch: 818 Average loss: 0.016957 (D_error: 0.001035, G_error: 0.015922)
====&gt; Epoch: 819 Average loss: 0.017379 (D_error: 0.001015, G_error: 0.016364)
====&gt; Epoch: 820 Average loss: 0.017353 (D_error: 0.001014, G_error: 0.016340)
====&gt; Epoch: 821 Average loss: 0.017207 (D_error: 0.001012, G_error: 0.016195)
====&gt; Epoch: 822 Average loss: 0.017210 (D_error: 0.001020, G_error: 0.016190)
====&gt; Epoch: 823 Average loss: 0.017428 (D_error: 0.001014, G_error: 0.016414)
====&gt; Epoch: 824 Average loss: 0.017569 (D_error: 0.001000, G_error: 0.016569)
====&gt; Epoch: 825 Average loss: 0.017374 (D_error: 0.000999, G_error: 0.016376)
====&gt; Epoch: 826 Average loss: 0.017331 (D_error: 0.000999, G_error: 0.016332)
====&gt; Epoch: 827 Average loss: 0.017332 (D_error: 0.000995, G_error: 0.016336)
====&gt; Epoch: 828 Average loss: 0.017729 (D_error: 0.000987, G_error: 0.016742)
====&gt; Epoch: 829 Average loss: 0.017286 (D_error: 0.000998, G_error: 0.016288)
====&gt; Epoch: 830 Average loss: 0.017629 (D_error: 0.000977, G_error: 0.016652)
====&gt; Epoch: 831 Average loss: 0.017286 (D_error: 0.001000, G_error: 0.016286)
====&gt; Epoch: 832 Average loss: 0.017273 (D_error: 0.000985, G_error: 0.016288)
====&gt; Epoch: 833 Average loss: 0.017813 (D_error: 0.000942, G_error: 0.016870)
====&gt; Epoch: 834 Average loss: 0.017563 (D_error: 0.000985, G_error: 0.016578)
====&gt; Epoch: 835 Average loss: 0.017146 (D_error: 0.001000, G_error: 0.016146)
====&gt; Epoch: 836 Average loss: 0.018046 (D_error: 0.000938, G_error: 0.017108)
====&gt; Epoch: 837 Average loss: 0.017831 (D_error: 0.000950, G_error: 0.016880)
====&gt; Epoch: 838 Average loss: 0.017451 (D_error: 0.000989, G_error: 0.016462)
====&gt; Epoch: 839 Average loss: 0.017577 (D_error: 0.000967, G_error: 0.016610)
====&gt; Epoch: 840 Average loss: 0.017918 (D_error: 0.000933, G_error: 0.016985)
====&gt; Epoch: 841 Average loss: 0.018356 (D_error: 0.000923, G_error: 0.017433)
====&gt; Epoch: 842 Average loss: 0.018065 (D_error: 0.000945, G_error: 0.017120)
====&gt; Epoch: 843 Average loss: 0.018186 (D_error: 0.000916, G_error: 0.017270)
====&gt; Epoch: 844 Average loss: 0.017746 (D_error: 0.000961, G_error: 0.016785)
====&gt; Epoch: 845 Average loss: 0.017779 (D_error: 0.000959, G_error: 0.016820)
====&gt; Epoch: 846 Average loss: 0.017581 (D_error: 0.000956, G_error: 0.016626)
====&gt; Epoch: 847 Average loss: 0.017678 (D_error: 0.000948, G_error: 0.016730)
====&gt; Epoch: 848 Average loss: 0.017710 (D_error: 0.000944, G_error: 0.016766)
====&gt; Epoch: 849 Average loss: 0.017966 (D_error: 0.000926, G_error: 0.017041)
====&gt; Epoch: 850 Average loss: 0.017944 (D_error: 0.000924, G_error: 0.017021)
====&gt; Epoch: 851 Average loss: 0.018316 (D_error: 0.000922, G_error: 0.017394)
====&gt; Epoch: 852 Average loss: 0.018293 (D_error: 0.000897, G_error: 0.017396)
====&gt; Epoch: 853 Average loss: 0.018304 (D_error: 0.000897, G_error: 0.017407)
====&gt; Epoch: 854 Average loss: 0.018467 (D_error: 0.000890, G_error: 0.017576)
====&gt; Epoch: 855 Average loss: 0.017864 (D_error: 0.000935, G_error: 0.016929)
====&gt; Epoch: 856 Average loss: 0.018485 (D_error: 0.000886, G_error: 0.017598)
====&gt; Epoch: 857 Average loss: 0.018245 (D_error: 0.000891, G_error: 0.017354)
====&gt; Epoch: 858 Average loss: 0.018199 (D_error: 0.000898, G_error: 0.017301)
====&gt; Epoch: 859 Average loss: 0.018080 (D_error: 0.000917, G_error: 0.017163)
====&gt; Epoch: 860 Average loss: 0.018515 (D_error: 0.000889, G_error: 0.017626)
====&gt; Epoch: 861 Average loss: 0.017898 (D_error: 0.000909, G_error: 0.016989)
====&gt; Epoch: 862 Average loss: 0.018490 (D_error: 0.000885, G_error: 0.017605)
====&gt; Epoch: 863 Average loss: 0.018039 (D_error: 0.000900, G_error: 0.017139)
====&gt; Epoch: 864 Average loss: 0.018305 (D_error: 0.000891, G_error: 0.017414)
====&gt; Epoch: 865 Average loss: 0.018655 (D_error: 0.000861, G_error: 0.017794)
====&gt; Epoch: 866 Average loss: 0.018577 (D_error: 0.000888, G_error: 0.017689)
====&gt; Epoch: 867 Average loss: 0.019088 (D_error: 0.000839, G_error: 0.018249)
====&gt; Epoch: 868 Average loss: 0.018363 (D_error: 0.000883, G_error: 0.017480)
====&gt; Epoch: 869 Average loss: 0.018296 (D_error: 0.000877, G_error: 0.017419)
====&gt; Epoch: 870 Average loss: 0.018432 (D_error: 0.000872, G_error: 0.017560)
====&gt; Epoch: 871 Average loss: 0.018077 (D_error: 0.000893, G_error: 0.017184)
====&gt; Epoch: 872 Average loss: 0.018355 (D_error: 0.000866, G_error: 0.017488)
====&gt; Epoch: 873 Average loss: 0.019297 (D_error: 0.000813, G_error: 0.018484)
====&gt; Epoch: 874 Average loss: 0.019041 (D_error: 0.000853, G_error: 0.018189)
====&gt; Epoch: 875 Average loss: 0.018716 (D_error: 0.000839, G_error: 0.017877)
====&gt; Epoch: 876 Average loss: 0.019059 (D_error: 0.000822, G_error: 0.018237)
====&gt; Epoch: 877 Average loss: 0.019406 (D_error: 0.000817, G_error: 0.018589)
====&gt; Epoch: 878 Average loss: 0.018307 (D_error: 0.000877, G_error: 0.017431)
====&gt; Epoch: 879 Average loss: 0.018734 (D_error: 0.000848, G_error: 0.017886)
====&gt; Epoch: 880 Average loss: 0.018598 (D_error: 0.000864, G_error: 0.017734)
====&gt; Epoch: 881 Average loss: 0.018625 (D_error: 0.000830, G_error: 0.017794)
====&gt; Epoch: 882 Average loss: 0.019158 (D_error: 0.000826, G_error: 0.018332)
====&gt; Epoch: 883 Average loss: 0.018358 (D_error: 0.000860, G_error: 0.017498)
====&gt; Epoch: 884 Average loss: 0.018641 (D_error: 0.000833, G_error: 0.017808)
====&gt; Epoch: 885 Average loss: 0.019314 (D_error: 0.000824, G_error: 0.018490)
====&gt; Epoch: 886 Average loss: 0.018699 (D_error: 0.000843, G_error: 0.017856)
====&gt; Epoch: 887 Average loss: 0.019177 (D_error: 0.000809, G_error: 0.018368)
====&gt; Epoch: 888 Average loss: 0.018828 (D_error: 0.000833, G_error: 0.017995)
====&gt; Epoch: 889 Average loss: 0.019337 (D_error: 0.000795, G_error: 0.018541)
====&gt; Epoch: 890 Average loss: 0.019331 (D_error: 0.000801, G_error: 0.018530)
====&gt; Epoch: 891 Average loss: 0.018881 (D_error: 0.000830, G_error: 0.018051)
====&gt; Epoch: 892 Average loss: 0.018503 (D_error: 0.000845, G_error: 0.017658)
====&gt; Epoch: 893 Average loss: 0.019009 (D_error: 0.000814, G_error: 0.018194)
====&gt; Epoch: 894 Average loss: 0.018869 (D_error: 0.000823, G_error: 0.018046)
====&gt; Epoch: 895 Average loss: 0.018865 (D_error: 0.000820, G_error: 0.018045)
====&gt; Epoch: 896 Average loss: 0.019237 (D_error: 0.000801, G_error: 0.018436)
====&gt; Epoch: 897 Average loss: 0.018795 (D_error: 0.000816, G_error: 0.017980)
====&gt; Epoch: 898 Average loss: 0.019276 (D_error: 0.000796, G_error: 0.018479)
====&gt; Epoch: 899 Average loss: 0.018862 (D_error: 0.000816, G_error: 0.018046)
====&gt; Epoch: 900 Average loss: 0.019257 (D_error: 0.000792, G_error: 0.018465)
====&gt; Epoch: 901 Average loss: 0.019100 (D_error: 0.000818, G_error: 0.018282)
====&gt; Epoch: 902 Average loss: 0.019576 (D_error: 0.000781, G_error: 0.018795)
====&gt; Epoch: 903 Average loss: 0.019156 (D_error: 0.000806, G_error: 0.018351)
====&gt; Epoch: 904 Average loss: 0.019025 (D_error: 0.000788, G_error: 0.018237)
====&gt; Epoch: 905 Average loss: 0.019332 (D_error: 0.000799, G_error: 0.018532)
====&gt; Epoch: 906 Average loss: 0.019896 (D_error: 0.000756, G_error: 0.019141)
====&gt; Epoch: 907 Average loss: 0.019750 (D_error: 0.000768, G_error: 0.018982)
====&gt; Epoch: 908 Average loss: 0.019488 (D_error: 0.000778, G_error: 0.018710)
====&gt; Epoch: 909 Average loss: 0.019550 (D_error: 0.000768, G_error: 0.018783)
====&gt; Epoch: 910 Average loss: 0.019202 (D_error: 0.000788, G_error: 0.018415)
====&gt; Epoch: 911 Average loss: 0.019547 (D_error: 0.000775, G_error: 0.018772)
====&gt; Epoch: 912 Average loss: 0.019405 (D_error: 0.000762, G_error: 0.018642)
====&gt; Epoch: 913 Average loss: 0.019493 (D_error: 0.000787, G_error: 0.018707)
====&gt; Epoch: 914 Average loss: 0.019494 (D_error: 0.000760, G_error: 0.018734)
====&gt; Epoch: 915 Average loss: 0.019380 (D_error: 0.000767, G_error: 0.018612)
====&gt; Epoch: 916 Average loss: 0.019736 (D_error: 0.000736, G_error: 0.019000)
====&gt; Epoch: 917 Average loss: 0.019186 (D_error: 0.000786, G_error: 0.018400)
====&gt; Epoch: 918 Average loss: 0.019954 (D_error: 0.000734, G_error: 0.019221)
====&gt; Epoch: 919 Average loss: 0.019747 (D_error: 0.000746, G_error: 0.019001)
====&gt; Epoch: 920 Average loss: 0.019515 (D_error: 0.000757, G_error: 0.018758)
====&gt; Epoch: 921 Average loss: 0.019451 (D_error: 0.000765, G_error: 0.018686)
====&gt; Epoch: 922 Average loss: 0.019706 (D_error: 0.000737, G_error: 0.018969)
====&gt; Epoch: 923 Average loss: 0.019887 (D_error: 0.000744, G_error: 0.019143)
====&gt; Epoch: 924 Average loss: 0.019677 (D_error: 0.000738, G_error: 0.018939)
====&gt; Epoch: 925 Average loss: 0.019066 (D_error: 0.000765, G_error: 0.018301)
====&gt; Epoch: 926 Average loss: 0.019576 (D_error: 0.000738, G_error: 0.018838)
====&gt; Epoch: 927 Average loss: 0.019340 (D_error: 0.000769, G_error: 0.018571)
====&gt; Epoch: 928 Average loss: 0.019075 (D_error: 0.000752, G_error: 0.018323)
====&gt; Epoch: 929 Average loss: 0.019497 (D_error: 0.000753, G_error: 0.018744)
====&gt; Epoch: 930 Average loss: 0.019240 (D_error: 0.000761, G_error: 0.018478)
====&gt; Epoch: 931 Average loss: 0.019187 (D_error: 0.000756, G_error: 0.018431)
====&gt; Epoch: 932 Average loss: 0.019931 (D_error: 0.000709, G_error: 0.019221)
====&gt; Epoch: 933 Average loss: 0.019703 (D_error: 0.000725, G_error: 0.018979)
====&gt; Epoch: 934 Average loss: 0.019070 (D_error: 0.000749, G_error: 0.018321)
====&gt; Epoch: 935 Average loss: 0.020179 (D_error: 0.000698, G_error: 0.019481)
====&gt; Epoch: 936 Average loss: 0.019413 (D_error: 0.000746, G_error: 0.018667)
====&gt; Epoch: 937 Average loss: 0.019848 (D_error: 0.000705, G_error: 0.019143)
====&gt; Epoch: 938 Average loss: 0.020090 (D_error: 0.000689, G_error: 0.019401)
====&gt; Epoch: 939 Average loss: 0.019908 (D_error: 0.000699, G_error: 0.019208)
====&gt; Epoch: 940 Average loss: 0.019440 (D_error: 0.000746, G_error: 0.018693)
====&gt; Epoch: 941 Average loss: 0.019857 (D_error: 0.000705, G_error: 0.019152)
====&gt; Epoch: 942 Average loss: 0.020073 (D_error: 0.000711, G_error: 0.019362)
====&gt; Epoch: 943 Average loss: 0.019334 (D_error: 0.000742, G_error: 0.018592)
====&gt; Epoch: 944 Average loss: 0.019147 (D_error: 0.000736, G_error: 0.018411)
====&gt; Epoch: 945 Average loss: 0.019512 (D_error: 0.000727, G_error: 0.018785)
====&gt; Epoch: 946 Average loss: 0.020130 (D_error: 0.000686, G_error: 0.019443)
====&gt; Epoch: 947 Average loss: 0.020006 (D_error: 0.000686, G_error: 0.019321)
====&gt; Epoch: 948 Average loss: 0.020159 (D_error: 0.000695, G_error: 0.019464)
====&gt; Epoch: 949 Average loss: 0.019694 (D_error: 0.000693, G_error: 0.019001)
====&gt; Epoch: 950 Average loss: 0.019898 (D_error: 0.000700, G_error: 0.019198)
====&gt; Epoch: 951 Average loss: 0.019665 (D_error: 0.000694, G_error: 0.018971)
====&gt; Epoch: 952 Average loss: 0.020068 (D_error: 0.000698, G_error: 0.019370)
====&gt; Epoch: 953 Average loss: 0.019377 (D_error: 0.000714, G_error: 0.018663)
====&gt; Epoch: 954 Average loss: 0.019853 (D_error: 0.000688, G_error: 0.019166)
====&gt; Epoch: 955 Average loss: 0.020183 (D_error: 0.000684, G_error: 0.019499)
====&gt; Epoch: 956 Average loss: 0.019622 (D_error: 0.000695, G_error: 0.018927)
====&gt; Epoch: 957 Average loss: 0.020230 (D_error: 0.000680, G_error: 0.019550)
====&gt; Epoch: 958 Average loss: 0.020961 (D_error: 0.000634, G_error: 0.020327)
====&gt; Epoch: 959 Average loss: 0.019568 (D_error: 0.000714, G_error: 0.018854)
====&gt; Epoch: 960 Average loss: 0.019764 (D_error: 0.000685, G_error: 0.019079)
====&gt; Epoch: 961 Average loss: 0.019346 (D_error: 0.000720, G_error: 0.018627)
====&gt; Epoch: 962 Average loss: 0.020074 (D_error: 0.000671, G_error: 0.019403)
====&gt; Epoch: 963 Average loss: 0.019425 (D_error: 0.000707, G_error: 0.018718)
====&gt; Epoch: 964 Average loss: 0.019740 (D_error: 0.000689, G_error: 0.019050)
====&gt; Epoch: 965 Average loss: 0.019695 (D_error: 0.000700, G_error: 0.018996)
====&gt; Epoch: 966 Average loss: 0.020212 (D_error: 0.000668, G_error: 0.019544)
====&gt; Epoch: 967 Average loss: 0.019705 (D_error: 0.000677, G_error: 0.019029)
====&gt; Epoch: 968 Average loss: 0.020234 (D_error: 0.000667, G_error: 0.019566)
====&gt; Epoch: 969 Average loss: 0.020022 (D_error: 0.000679, G_error: 0.019343)
====&gt; Epoch: 970 Average loss: 0.020326 (D_error: 0.000652, G_error: 0.019674)
====&gt; Epoch: 971 Average loss: 0.019696 (D_error: 0.000677, G_error: 0.019019)
====&gt; Epoch: 972 Average loss: 0.019912 (D_error: 0.000677, G_error: 0.019235)
====&gt; Epoch: 973 Average loss: 0.020276 (D_error: 0.000655, G_error: 0.019622)
====&gt; Epoch: 974 Average loss: 0.019610 (D_error: 0.000675, G_error: 0.018935)
====&gt; Epoch: 975 Average loss: 0.020053 (D_error: 0.000664, G_error: 0.019389)
====&gt; Epoch: 976 Average loss: 0.020027 (D_error: 0.000650, G_error: 0.019377)
====&gt; Epoch: 977 Average loss: 0.020330 (D_error: 0.000642, G_error: 0.019688)
====&gt; Epoch: 978 Average loss: 0.020542 (D_error: 0.000640, G_error: 0.019902)
====&gt; Epoch: 979 Average loss: 0.019661 (D_error: 0.000664, G_error: 0.018997)
====&gt; Epoch: 980 Average loss: 0.019466 (D_error: 0.000677, G_error: 0.018789)
====&gt; Epoch: 981 Average loss: 0.020210 (D_error: 0.000648, G_error: 0.019563)
====&gt; Epoch: 982 Average loss: 0.020386 (D_error: 0.000636, G_error: 0.019750)
====&gt; Epoch: 983 Average loss: 0.020114 (D_error: 0.000652, G_error: 0.019461)
====&gt; Epoch: 984 Average loss: 0.020021 (D_error: 0.000668, G_error: 0.019353)
====&gt; Epoch: 985 Average loss: 0.019793 (D_error: 0.000648, G_error: 0.019144)
====&gt; Epoch: 986 Average loss: 0.020108 (D_error: 0.000642, G_error: 0.019466)
====&gt; Epoch: 987 Average loss: 0.020117 (D_error: 0.000626, G_error: 0.019490)
====&gt; Epoch: 988 Average loss: 0.020841 (D_error: 0.000612, G_error: 0.020229)
====&gt; Epoch: 989 Average loss: 0.020217 (D_error: 0.000644, G_error: 0.019573)
====&gt; Epoch: 990 Average loss: 0.020450 (D_error: 0.000622, G_error: 0.019829)
====&gt; Epoch: 991 Average loss: 0.020369 (D_error: 0.000620, G_error: 0.019750)
====&gt; Epoch: 992 Average loss: 0.020185 (D_error: 0.000633, G_error: 0.019552)
====&gt; Epoch: 993 Average loss: 0.020123 (D_error: 0.000629, G_error: 0.019495)
====&gt; Epoch: 994 Average loss: 0.019911 (D_error: 0.000635, G_error: 0.019276)
====&gt; Epoch: 995 Average loss: 0.020053 (D_error: 0.000629, G_error: 0.019423)
====&gt; Epoch: 996 Average loss: 0.019957 (D_error: 0.000635, G_error: 0.019322)
====&gt; Epoch: 997 Average loss: 0.020462 (D_error: 0.000617, G_error: 0.019844)
====&gt; Epoch: 998 Average loss: 0.019696 (D_error: 0.000645, G_error: 0.019051)
====&gt; Epoch: 999 Average loss: 0.019881 (D_error: 0.000631, G_error: 0.019250)
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[46]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># torch.save(generator.state_dict(), &#39;./models/generator.pt&#39;)</span>
<span class="c1"># torch.save(discriminator.state_dict(), &#39;./models/discriminator.pt&#39;)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Loading</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">nz</span> <span class="o">=</span> <span class="mi">100</span><span class="c1"># dimension of random noise</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">nz</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">nz</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;./models/generator_100epochs.pt&#39;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)))</span>
<span class="n">discriminator</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;./models/discriminator_100epochs.pt&#39;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;All keys matched successfully&gt;
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[40]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load some of the images</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">img0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_GANs/gen_img0.png&#39;</span><span class="p">))</span>
<span class="n">img10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_GANs/gen_img10.png&#39;</span><span class="p">))</span>
<span class="n">img25</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_GANs/gen_img25.png&#39;</span><span class="p">))</span>
<span class="n">img50</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_GANs/gen_img50.png&#39;</span><span class="p">))</span>
<span class="n">img100</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_GANs/gen_img100.png&#39;</span><span class="p">))</span>
<span class="n">img250</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_GANs/gen_img250.png&#39;</span><span class="p">))</span>
<span class="n">img500</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_GANs/gen_img500.png&#39;</span><span class="p">))</span>
<span class="n">img750</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_GANs/gen_img750.png&#39;</span><span class="p">))</span>
<span class="n">img999</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;./plots_GANs/gen_img999.png&#39;</span><span class="p">))</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img0</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;img0&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img10</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;img10&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img25</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;img25&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;img50&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img100</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;img100&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img250</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;img250&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img500</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;img500&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img750</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;img750&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img999</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span> <span class="c1">#set colormap as &#39;gray&#39;</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;img999&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;After 1000 epochs&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[40]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Text(0.5, 0.98, &#39;After 1000 epochs&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_74_1.png" src="../_images/CASESTUDY_NN-day2_74_1.png" />
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">## Train a FFnet on the MNIST</span>
<span class="c1"># ./plots/lr0.001_3FFNet_2ReLU_Drop_momentum0.9_wdecay0.001_dampening0_nesterovFalse_HidDim128.png</span>
<span class="c1"># Testing a regular FFnet</span>
<span class="k">class</span> <span class="nc">FeedforwardNeuralNetModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedforwardNeuralNetModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Linear function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="c1"># Non-linearity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="c1"># Linear function (readout)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Linear function</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Training and Evaluation routines</span>
<span class="k">def</span> <span class="nf">train_FF</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is a standard training loop, which leaves some parts to be filled in.</span>
<span class="sd">    INPUT:</span>
<span class="sd">    :param model: an untrained pytorch model</span>
<span class="sd">    :param loss_fn: e.g. Cross Entropy loss of Mean Squared Error.</span>
<span class="sd">    :param optimizer: the model optimizer, initialized with a learning rate.</span>
<span class="sd">    :param training_set: The training data, in a dataloader for easy iteration.</span>
<span class="sd">    :param test_loader: The testing data, in a dataloader for easy iteration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optimizer: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimizer</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">num_epochs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># obviously, this is too many. I don&#39;t know what this author was thinking.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;n. of epochs: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">))</span>
    <span class="n">train_loss</span><span class="o">=</span><span class="p">[]</span>
    <span class="n">val_loss</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># loop through each data point in the training set</span>
        <span class="n">all_loss_train</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="c1"># run the model on the data</span>
            <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>

            <span class="c1"># Clear gradients w.r.t. parameters</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>

            <span class="c1"># Calculate the loss</span>
            <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># add an extra dimension to keep CrossEntropy happy.</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">targets</span><span class="p">)</span>

            <span class="c1"># Find the gradients of our loss via backpropogation</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># Adjust accordingly with the optimizer</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">all_loss_train</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">all_loss_train</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">all_loss_val</span><span class="o">=</span><span class="mi">0</span>
            <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>

                <span class="c1"># run the model on the data</span>
                <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
                <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># add an extra dimension to keep CrossEntropy happy.</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">targets</span><span class="p">)</span>
                <span class="n">all_loss_val</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">val_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">all_loss_val</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">))</span>


        <span class="c1"># Give status reports every 100 epochs</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">. Progress: </span><span class="si">{</span><span class="n">epoch</span><span class="o">/</span><span class="n">num_epochs</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s2">%. &quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; Training accuracy: </span><span class="si">{:.4f}</span><span class="s2">. Test accuracy: </span><span class="si">{:.4f}</span><span class="s2">. Loss Train: </span><span class="si">{:.4f}</span><span class="s2">. Loss Val: </span><span class="si">{:.4f}</span><span class="s2">. Time: </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">evaluate_FF</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">train_loader</span><span class="p">,</span><span class="n">verbose</span><span class="p">),</span>
                                                                                                         <span class="n">evaluate_FF</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">test_loader</span><span class="p">,</span><span class="n">verbose</span><span class="p">),</span>
                                                                                                         <span class="n">train_loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">val_loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">10</span><span class="o">*</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)))</span> <span class="c1">#TODO: implement the evaluate function to provide performance statistics during training.</span>
    <span class="c1"># Plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">evaluate_FF</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">evaluation_set</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the given model on the given dataset.</span>
<span class="sd">    Returns the percentage of correct classifications out of total classifications.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># this disables backpropogation, which makes the model run much more quickly.</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">evaluation_set</span><span class="p">:</span>

            <span class="c1"># run the model on the data</span>
            <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>
<span class="c1">#             if verbose:</span>
<span class="c1">#                 print(&#39;model_input.shape: {}&#39;.format(model_input.shape))</span>
<span class="c1">#                 print(&#39;targets.shape: {}&#39;.format(targets.shape))</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;out[:5]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">out</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;predicted[:5]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;targets[:5]: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
            <span class="n">total</span><span class="o">+=</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">correct</span><span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;total: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">total</span><span class="p">))</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;correct: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">correct</span><span class="p">))</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;100*float(correct/total): </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="nb">float</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">total</span><span class="p">)))</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">total</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy</span>


<span class="c1"># Best configuration for longer</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">hid_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">dampening</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">nesterov</span> <span class="o">=</span> <span class="kc">False</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">lr: </span><span class="si">{}</span><span class="s1">, momentum: </span><span class="si">{}</span><span class="s1">, weight_decay: </span><span class="si">{}</span><span class="s1">, dampening: </span><span class="si">{}</span><span class="s1">, nesterov: </span><span class="si">{}</span><span class="s1"> &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">dampening</span><span class="p">,</span> <span class="n">nesterov</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FeedforwardNeuralNetModel</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span><span class="n">hidden_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">ADAM</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span> <span class="c1"># This is absurdly high.</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">config_str</span> <span class="o">=</span> <span class="s1">&#39;lr&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_3FFNet_2ReLU_Drop_momentum&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">momentum</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_wdecay&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_dampening&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dampening</span><span class="p">)</span> <span class="o">+</span><span class="s1">&#39;_nesterov&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">nesterov</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_HidDim&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">)</span>
<span class="n">train_FF</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ADAM</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;./models/FFnet.pt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[46]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Loading the GAN trained until 100 epochs</span>

<span class="c1">## Redo for the best result architecture (likely sigmoid)</span>
<span class="k">class</span> <span class="nc">FeedforwardNeuralNetModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedforwardNeuralNetModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Linear function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="c1"># Non-linearity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="c1"># Linear function (readout)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Linear function</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">hid_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">nz</span> <span class="o">=</span> <span class="mi">100</span><span class="c1"># dimension of random noise</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">nz</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">nz</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;./models/generator_100epochs.pt&#39;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)))</span>
<span class="n">discriminator</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;./models/discriminator_100epochs.pt&#39;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)))</span>
<span class="n">model_FF</span> <span class="o">=</span> <span class="n">FeedforwardNeuralNetModel</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span><span class="n">hidden_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_FF</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;./models/FFnet.pt&#39;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[46]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;All keys matched successfully&gt;
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[47]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Generate 1000 samples: and classify some</span>
<span class="n">path_to_save</span> <span class="o">=</span> <span class="s1">&#39;./samples_GAN&#39;</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path_to_save</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path_to_save</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">generator</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">nz</span><span class="p">))))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">generated_img</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">generated_img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># save the generated torch tensor images</span>
    <span class="n">count</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">generated_img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span><span class="k">10</span> == 0 and count &lt; 25:
            <span class="n">save_image</span><span class="p">(</span><span class="n">generated_img</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">,:,:],</span> <span class="sa">f</span><span class="s2">&quot;./samples_GAN/sample</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.png&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;i: </span><span class="si">{}</span><span class="s1">, count: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">count</span><span class="p">))</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">generated_img</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">,:,:]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">model_input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="c1"># TODO: Turn the 28 by 28 image tensors into a 784 dimensional tensor.</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">model_FF</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;out: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">generated_img</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">,:,:]</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]),</span> <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">count</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;pred: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
torch.Size([1000, 1, 28, 28])
i: 0, count: 0
out: tensor([[3.8307e-08, 2.4883e-05, 1.1849e-07, 7.8436e-06, 4.8853e-01, 4.7663e-01,
         3.4533e-02, 2.3481e-07, 2.1284e-04, 6.8091e-05]])
i: 10, count: 1
out: tensor([[8.4217e-05, 4.3701e-05, 6.2595e-04, 4.0492e-08, 9.1073e-01, 7.1391e-05,
         8.1844e-02, 8.6675e-06, 6.1034e-03, 4.9043e-04]])
i: 20, count: 2
out: tensor([[5.4235e-05, 1.6426e-04, 1.1601e-01, 5.7772e-01, 5.4051e-07, 5.2946e-04,
         8.3468e-03, 1.8427e-09, 2.9717e-01, 3.7604e-10]])
i: 30, count: 3
out: tensor([[2.3284e-05, 3.5149e-05, 5.9822e-05, 2.6800e-02, 9.2633e-01, 1.1026e-05,
         2.5887e-07, 2.2036e-02, 1.5869e-04, 2.4547e-02]])
i: 40, count: 4
out: tensor([[9.9983e-01, 5.6842e-09, 1.5825e-04, 6.1666e-10, 2.4821e-08, 2.3318e-09,
         2.0921e-09, 2.6669e-09, 6.7770e-07, 9.1370e-06]])
i: 50, count: 5
out: tensor([[8.6453e-10, 4.3818e-07, 2.0774e-08, 1.4254e-03, 5.6368e-11, 9.9856e-01,
         8.2051e-07, 4.1332e-13, 1.1845e-05, 7.7518e-07]])
i: 60, count: 6
out: tensor([[2.0459e-05, 1.2781e-05, 7.6266e-02, 7.9050e-05, 2.3218e-08, 6.9513e-07,
         1.0992e-08, 1.4570e-04, 9.2289e-01, 5.8222e-04]])
i: 70, count: 7
out: tensor([[8.5822e-05, 1.1369e-04, 2.2185e-01, 7.0287e-01, 3.6165e-05, 1.3564e-02,
         2.2428e-03, 1.1616e-07, 5.9242e-02, 7.2552e-08]])
i: 80, count: 8
out: tensor([[5.8402e-05, 1.1920e-07, 1.0286e-06, 2.2805e-05, 3.1820e-07, 9.9978e-01,
         4.8329e-05, 4.0834e-11, 5.3351e-05, 3.2488e-05]])
i: 90, count: 9
out: tensor([[8.2471e-05, 4.3312e-05, 9.8849e-01, 1.1554e-03, 3.5335e-06, 3.0138e-04,
         6.6847e-05, 8.2654e-07, 9.8022e-03, 5.2289e-05]])
i: 100, count: 10
out: tensor([[1.0974e-06, 1.0474e-04, 9.8015e-01, 1.4693e-02, 9.4726e-09, 7.0491e-06,
         1.0825e-07, 3.8160e-08, 5.0316e-03, 1.6776e-05]])
i: 110, count: 11
out: tensor([[8.1653e-02, 7.9923e-07, 1.7779e-05, 3.6303e-08, 1.5599e-03, 2.0199e-07,
         9.1676e-01, 4.1076e-09, 2.9205e-06, 3.0228e-06]])
i: 120, count: 12
out: tensor([[4.0521e-05, 8.9474e-06, 1.5224e-02, 9.8162e-01, 6.0438e-10, 1.5773e-06,
         2.1270e-10, 2.5879e-03, 4.2212e-04, 9.8624e-05]])
i: 130, count: 13
out: tensor([[1.0570e-05, 8.7047e-05, 1.9966e-04, 1.4879e-07, 7.8656e-03, 1.7110e-06,
         2.6118e-07, 9.9631e-04, 9.9068e-01, 1.5670e-04]])
i: 140, count: 14
out: tensor([[9.8572e-01, 1.7017e-09, 4.7857e-03, 1.3438e-04, 7.3496e-08, 4.1876e-04,
         6.8921e-03, 4.6677e-10, 2.0427e-03, 5.9049e-06]])
i: 150, count: 15
out: tensor([[7.8059e-07, 1.6210e-10, 1.1144e-06, 5.6455e-13, 6.3012e-07, 5.6354e-08,
         1.0000e+00, 2.1855e-12, 1.6438e-10, 3.2938e-14]])
i: 160, count: 16
out: tensor([[9.9883e-15, 1.7545e-08, 8.0940e-15, 5.2639e-06, 1.7966e-10, 9.9999e-01,
         4.9566e-10, 4.9939e-13, 2.4430e-09, 5.5325e-07]])
i: 170, count: 17
out: tensor([[2.4508e-04, 1.0844e-05, 3.5294e-05, 1.2993e-02, 2.2842e-04, 5.9308e-04,
         2.1213e-03, 1.7227e-07, 9.8377e-01, 1.4154e-07]])
i: 180, count: 18
out: tensor([[9.5745e-01, 1.3716e-06, 4.6490e-05, 2.7003e-05, 8.5338e-04, 1.2380e-03,
         4.0038e-02, 4.2094e-08, 1.8079e-04, 1.6002e-04]])
i: 190, count: 19
out: tensor([[7.2766e-06, 4.8480e-08, 9.9867e-01, 1.0360e-03, 5.4199e-11, 1.0092e-07,
         5.0450e-09, 1.2755e-06, 2.8561e-04, 2.3180e-06]])
i: 200, count: 20
out: tensor([[9.9992e-01, 8.2528e-09, 7.5582e-05, 3.6717e-09, 9.0718e-11, 2.5288e-08,
         1.8842e-06, 3.0394e-09, 1.0774e-07, 9.3868e-07]])
i: 210, count: 21
out: tensor([[1.8106e-12, 3.1673e-08, 6.1448e-12, 9.5901e-06, 3.8083e-09, 9.9998e-01,
         7.3814e-09, 1.5449e-11, 1.1298e-07, 9.9353e-06]])
i: 220, count: 22
out: tensor([[3.5635e-08, 2.5547e-04, 7.9672e-01, 7.8141e-02, 2.7043e-11, 9.6663e-05,
         1.1649e-07, 8.9303e-06, 1.2478e-01, 4.9184e-09]])
i: 230, count: 23
out: tensor([[1.0820e-13, 5.7320e-10, 2.3945e-13, 8.9121e-06, 4.7155e-13, 9.9999e-01,
         1.7172e-09, 3.1953e-13, 6.9937e-08, 2.1932e-07]])
i: 240, count: 24
out: tensor([[2.9469e-05, 2.8193e-07, 1.9379e-05, 6.8732e-03, 2.2126e-09, 9.1837e-01,
         6.1194e-06, 6.7368e-07, 7.3783e-02, 9.1940e-04]])
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
&lt;ipython-input-47-04114edea52e&gt;:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = F.softmax(out)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_NN-day2_77_2.png" src="../_images/CASESTUDY_NN-day2_77_2.png" />
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># checking the pixel distributions</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># ax=ax.ravel()</span>
<span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Original MNIST pixel distribution&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">generated_img</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:,:]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;GAN&#39;s generated pixel distribution&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p><img alt="drawing" class="no-scaled-link" src="../_images/GAN_PixelDist.png" style="width: 800px;" /></p>
</div>
<div class="section" id="Question-4.2.1.">
<h2>Question 4.2.1.<a class="headerlink" href="#Question-4.2.1." title="Permalink to this headline">¶</a></h2>
<p><strong>Which generates more realistic images: your GAN, or your VAE? Why do you think this is?</strong></p>
</div>
<div class="section" id="Question-4.2.2.">
<h2>Question 4.2.2.<a class="headerlink" href="#Question-4.2.2." title="Permalink to this headline">¶</a></h2>
<p><strong>Does your GAN appear to generate all digits in equal number, or has it specialized in a smaller number of digits? If so, why might this be?</strong></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="NN-day3.html" class="btn btn-neutral float-right" title="LSTM Network" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="NN-day1.html" class="btn btn-neutral float-left" title="Estimating nitrogen concentrations in streams and rivers using NN" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Giuseppe Amatulli.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>