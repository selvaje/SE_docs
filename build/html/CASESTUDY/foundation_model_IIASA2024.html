

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Prithvi 100M model &mdash; Spatial Ecology&#39;s code documentation 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css?v=572af1d6" />
      <link rel="stylesheet" type="text/css" href="../_static/thebelab.css" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=d45e8c67"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/thebelab-helper.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Autoencoder (AE), Variational Autoencoder (VAE)" href="NN_Unsupervised_2024.html" />
    <link rel="prev" title="Using Multi-layer Perceptron and Convolutional Neural Networks for Satellite image classification - 2023" href="CNN_satelite_2023.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Spatial Ecology's code documentation
              <img src="../_static/SE_compact.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">COURSE TRAINERS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../COURSETRAINERS/trainers.html">Spatial Ecology course trainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">COURSES AROUND THE WORLD</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_wcsu_02-04_2021.html">Western Connecticut State University 2021</a></li>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_stock_uni_04-05_2021.html">Stockholm University 2021</a></li>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_geocomp_ml_04-05_2022.html">GeoComp &amp; ML 2022 course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_geocomp_modelling_10-11_2022.html">GeoComp &amp; Modelling 2022 course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_geocomp_ml_04-05_2023.html">GeoComp &amp; ML 2023 course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_geocomp_ml_04-05_2024.html">GeoComp &amp; ML 2024 course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_GEO-OPEN-HACK-2024_06_2024.html">GEO-OPEN-HACK-2024</a></li>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_geocomp_11-12_2024.html">GeoComp 2024 course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_geocomp_geoanlysis_04_2025.html">Geo Comp/Analysis 2025 course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../COURSESAROUNDTHEWORLD/course_geocomp_ml_09-11_2025.html">GeoComp &amp; ML 2025 course</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GEO DATA</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../GEODATA/geomorpho90m/geomorpho90m.html">Geomorpho90m: technical documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">LINUX VIRTUAL MACHINE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../VIRTUALMACHINE/Setting_Ubuntu24.04_for_Spatial_Ecology_course.html">Prepare Ubuntu 24.04 for Spatial Ecology courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../VIRTUALMACHINE/Setting_Colab_for_Spatial_Ecology_course.html">Prepare Colab for Spatial Ecology courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../VIRTUALMACHINE/Setting_OSGeoLive_for_Spatial_Ecology_course.html">Prepare OSGeoLive for Spatial Ecology courses</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">WEB SEMINARS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../WEBSEMINAR/webseminar.html">Raster/Vector Processing using GDAL/OGR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WEBSEMINAR/webseminar.html#image-processing-using-pktools">Image Processing using Pktools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WEBSEMINAR/webseminar.html#introduction-to-grass-gis">Introduction to GRASS GIS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WEBSEMINAR/webseminar.html#geocomputation-with-high-performance-computing">GeoComputation with High Performance Computing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">BASH</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../BASH/bashintro_osgeo.html">Linux Operation System as a base for Spatial Ecology Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../BASH/bashinter_osgeo.html">Manipulate text files in bash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../BASH/bashxargs_osgeo.html">Multi-core bash</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AWK</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../AWK/awk.html">AWK Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GDAL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../GDAL/gdal_osgeo.html">Use GDAL/OGR for raster/vector operations - osgeo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GDAL/gdal_colab.html">Use GDAL/OGR for raster/vector operations - colab</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PKTOOLS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../PKTOOLS/pktools_osgeo.html">Use PKTOOLS for raster/vector operations - osgeo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PKTOOLS/pktools_colab.html">Use PKTOOLS for raster/vector operations - colab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PKTOOLS/pyjeo_introduction1.html">Introduction to pyjeo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PKTOOLS/pyjeo_introduction2.html">pyjeo: an open source image processing library in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PKTOOLS/pyjeo_pktools.html">Performing raster and vector operations in Python using pyjeo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PKTOOLS/pyjeo_upscaling_surf.html">Scaling-up: batch processing on the cluster with pyjeo</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">R</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../R/R_Intro.html">R Introduction</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PYTHON</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/PythonEnvs.html">Python environments or how to survive to your journey in the geodata space</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/Python_Intro.html">Introduction to Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/Geo_Python.html">Python &amp; GeoComputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/Python_data_analysis_SM.html">Python data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/Python_geospatial_data_analysis_SM.html">Python geospatial data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/RasterIO_Intro.html">RasterIO for dummies: a brief intro to a <em>pythonic</em> raster library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/RasterIO_Intro.html#2.-Preparing-the-dataset-for-next-ML-exercises-via-rasterio">2. Preparing the dataset for next ML exercises via rasterio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/RasterIO_Intro.html#3.-Beyond-the-basics">3. Beyond the basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PYTHON/OGCSQL.html">Generalities about OGC Geospatial extensions for SQL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GRASS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../GRASS/grass_intro.html">GRASS Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GRASS/grass_newproject.html">Start a new GRASS project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GRASS/grass_hydro.html">Using GRASS for stream-network extraction and basins delineation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GRASS/grass_hydro_colab.html">Using GRASS for stream-network extraction and basins delineation in Colab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GRASS/SDM1_MWood_gecomp4GRASS.html">SDM1 : Montane woodcreper - Gecomputation for the Random Forest model using GRASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GRASS/SDM1_MWood_GRASSmodel.html">SDM1 : Montane woodcreper - Random Forest Model using GRASS</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">HIGH PERFORMANCE COMPUTING</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../HPC/hpc_setting.html">Geocomputation at High Performance Computing Cluster (HPC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../HPC/hpc_setting_grass.html">Use of GRASS in HPC</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ASSIGNMENTS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ASSIGNMENTS/assignment_fall2022_solutions.html">Assignments Fall 2022</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CASE STUDY</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="SDM1_MWood_gecomp.html">SDM1 : Montane woodcreper - Gecomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDM1_MWood_Rmodel.html">SDM1 : Montane woodcreper - Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDM1_MWood_gecomp4GRASS.html">SDM1 : Montane woodcreper - Gecomputation for the Random Forest model using GRASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDM1_MWood_GRASSmodel.html">SDM1 : Montane woodcreper - Random Forest Model using GRASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDM2_Vath_Rmodel.html">SDM2 : Varied Thrush - Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="manipulate_GSIM.html">Manipulate GSIM files</a></li>
<li class="toctree-l1"><a class="reference internal" href="Data_type_GTiff.html">Data type in GTiff</a></li>
<li class="toctree-l1"><a class="reference internal" href="temporal_interpolation.html">Temporal interpolation of landsat images</a></li>
<li class="toctree-l1"><a class="reference internal" href="DTW.html">Dynamic Time Warping</a></li>
<li class="toctree-l1"><a class="reference internal" href="pred_NP.html">Estimating nitrogen and phosphorus concentrations in streams and rivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="NN-day1.html">Estimating nitrogen concentrations in streams and rivers using NN</a></li>
<li class="toctree-l1"><a class="reference internal" href="NN-day2.html">Autoencoder (AE), Variational Autoencoder (VAE) and Generative Adversarial Network (GAN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="NN-day3.html">LSTM Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_01DataExplore.html">Estimation of tree height using GEDI dataset - Data explore</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_02Predictors_extraction.html">Estimation of tree height using GEDI dataset - Predictors extraction at point location</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_03RF_pred.html">Estimation of tree height using GEDI dataset - Random Forest prediction</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_04SVM_pred_2022.html">Estimation of tree height using GEDI dataset - Support Vector Machine for Regression (SVR) - 2022</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_04SVM_pred_2023.html">Estimation of tree height using GEDI dataset - Support Vector Machine for Regression (SVR) - 2023</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_04SVM_pred_2024.html">Estimation of tree height using GEDI dataset - Support Vector Machine for Regression (SVR) - 2024</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_04SVM_pred_2024.html#Exercise:-explore-the-other-parameters-offered-by-the-SVM-library-and-try-to-make-the-model-better.-Some-suggestions:">Exercise: explore the other parameters offered by the SVM library and try to make the model better. Some suggestions:</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_05Perceptron_pred_2022.html">Estimation of tree height using GEDI dataset - Perceptron 1 - 2022</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_05Perceptron_intro_2023.html">Estimation of tree height using GEDI dataset - Perceptron 1 - 2023</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_05Perceptron_intro_2024.html">Estimation of tree height using GEDI dataset - Perceptron - 2024</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_06Perceptron_pred_2023.html">Estimation of tree height using GEDI dataset - Perceptron tree prediction - 2023</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_06Perceptron_complete_2024.html">Estimation of tree height using GEDI dataset - Perceptron complete - 2024</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_05Perceptron_pred_clean_2022.html">Estimation of tree height using GEDI dataset - Clean Data - Perceptron 2 - 2022</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_06NeuralNets_pred.html">Estimation of tree height using GEDI dataset - Neural Network 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tree_Height_07FeedForward_Networks_2024.html">Estimation of tree height using GEDI dataset - Neural Network 1 - 2024</a></li>
<li class="toctree-l1"><a class="reference internal" href="NNs_pt3_SHAP.html">Neural Nets (pt.3), Interpretability and Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="CNN_satelite_2022.html">Using Multi-layer Perceptron and Convolutional Neural Networks for Satellite image classification - 2022.</a></li>
<li class="toctree-l1"><a class="reference internal" href="CNN_satelite_2023.html">Using Multi-layer Perceptron and Convolutional Neural Networks for Satellite image classification - 2023</a></li>
<li class="toctree-l1"><a class="reference internal" href="CNN_satelite_2023.html#Using-CNNs-for-a-image-dataset">Using CNNs for a image dataset</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Prithvi 100M model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Getting-started-with-Prithvi---Reconstruction">Getting started with Prithvi - Reconstruction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Get-model-files">Get model files</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Treat-it-as-a-module">Treat it as a module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Relevant-imports">Relevant imports</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Define-some-functions-for-visualization">Define some functions for visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Loading-the-model">Loading the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Let's-try-it-out!">Let’s try it out!</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Lets-call-the-model!">Lets call the model!</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Lets-use-these-to-build-a-nice-output-visualization">Lets use these to build a nice output visualization</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Inference-with-finetuned-Prithvi">Inference with finetuned Prithvi</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Let's-grab-an-image-to-do-inference-on">Let’s grab an image to do inference on</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Inference with finetuned Prithvi</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#Proposed-exercises">Proposed exercises</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Finetuning-for-your-use-case">Finetuning for your use case</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Running-features-through-the-segmentation-head">Running features through the segmentation head</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Finetuning---MMSeg">Finetuning - MMSeg</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="NN_Unsupervised_2024.html">Autoencoder (AE), Variational Autoencoder (VAE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="NN_Unsupervised_2024.html#Implementing-an-Autoencoder">Implementing an Autoencoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="NN_Unsupervised_2024.html#Autoencoding-MNIST">Autoencoding MNIST</a></li>
<li class="toctree-l1"><a class="reference internal" href="NN_Unsupervised_2024.html#Section-2">Section 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="NN_Unsupervised_2024.html#Section-3---Generative-Models">Section 3 - Generative Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="LSTMs_tutorial.html">Using LSTM for time-series predictions</a></li>
<li class="toctree-l1"><a class="reference internal" href="CNN_satelite_with_GPT_code_2023.html">Using GPT to implement a Convolutional Neural Networks for Satellite image classification.</a></li>
<li class="toctree-l1"><a class="reference internal" href="Classification_pyjeo_sklearn_2023.html">Classification in Python using pyjeo and sklearn</a></li>
<li class="toctree-l1"><a class="reference internal" href="GEEviaPython_2023.html">Google Earth Engine use via Python, containers and other mythical beasts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Students Projects</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../STUDENTSPROJECTS/index.html">1. 2021 SWEDEN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../STUDENTSPROJECTS/index.html#matera">2. 2022 MATERA</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">OUTDOOR</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../OUTDOOR/outdoor_orientering.html">Do not get lost in the wilderness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../OUTDOOR/outdoor_info.html">Shelter locations close to Gioia del Colle (BA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../OUTDOOR/bike_accomodation.html">Accomodation in Gioia del Colle (BA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../OUTDOOR/puglia_discover.html">Discover Puglia by bike!</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">TALKS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../TALKS/intelligent_modelling.html">Intelligent modelling in time and space: combine GeoComputation and Machine Learning for  environmental application.</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ADMIN</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ADMIN/00_pktools_gdrive_install.html">Install pktools on the gdrive and be able to use from any Colab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ADMIN/video.html">Video tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ADMIN/Compiling_OTB.html">Compiling OTB from source</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spatial Ecology's code documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">&lt;no title&gt;</a></li>
      <li class="breadcrumb-item active">Prithvi 100M model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/CASESTUDY/foundation_model_IIASA2024.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Prithvi-100M-model">
<h1>Prithvi 100M model<a class="headerlink" href="#Prithvi-100M-model" title="Link to this heading"></a></h1>
<p>This notebook will demonstrate basic usage of the Prithvi ViT model.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spatial-ecology.net/docs/source/lectures/lect_20240626_FoundationModelsforGeoData.pdf">Lecture</a></p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>wget https://raw.githubusercontent.com/selvaje/SE_data/master/exercise/foundation_model_IIASA2024.ipynb
wget https://raw.githubusercontent.com/selvaje/SE_data/master/exercise/foundation_model_IIASA2024.py
</pre></div>
</div>
<p>Useful links:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/ibm-nasa-geospatial">Hugginface</a> page for this project</p></li>
<li><p><a class="reference external" href="https://github.com/NASA-IMPACT/hls-foundation-os/tree/main">Github</a> page</p></li>
</ul>
<section id="Getting-started-with-Prithvi---Reconstruction">
<h2>Getting started with Prithvi - Reconstruction<a class="headerlink" href="#Getting-started-with-Prithvi---Reconstruction" title="Link to this heading"></a></h2>
<section id="Get-model-files">
<h3>Get model files<a class="headerlink" href="#Get-model-files" title="Link to this heading"></a></h3>
<p>To get started, clone the HuggingFace repository for Prithvi 100M, running the command below</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make sure you have git-lfs installed (https://git-lfs.com)</span>
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M
<span class="c1"># rename to a valid python module name</span>
mv<span class="w"> </span>Prithvi-100M<span class="w"> </span>prithvi
</pre></div>
</div>
<p>Alternatively, you can directly download the <a class="reference external" href="https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/tree/main#:~:text=Prithvi_100M.pt,pickle">weights</a> and <a class="reference external" href="https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/Prithvi.py">model class</a> and <a class="reference external" href="https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/Prithvi_100M_config.yaml">configuration file</a> from the repository and place them inside a directory named<code class="docutils literal notranslate"><span class="pre">prithvi</span></code>.</p>
<p>A third alternative is to leverage the <code class="docutils literal notranslate"><span class="pre">huggingface_hub</span></code> library to download these files directly through code. <code class="docutils literal notranslate"><span class="pre">%pip</span> <span class="pre">install</span> <span class="pre">huggingface_hub</span></code></p>
</section>
<section id="Treat-it-as-a-module">
<h3>Treat it as a module<a class="headerlink" href="#Treat-it-as-a-module" title="Link to this heading"></a></h3>
<p>Next, lets add an <code class="docutils literal notranslate"><span class="pre">__init__.py</span></code> file to the downloaded directory, so we can treat it as a module and import the <code class="docutils literal notranslate"><span class="pre">MaskedAutoencoderViT</span></code> class from it. Simply create an empty file inside the <code class="docutils literal notranslate"><span class="pre">prithvi</span></code> directory named <code class="docutils literal notranslate"><span class="pre">__init__.py</span></code> by running the code below</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;prithvi/__init__.py&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Relevant-imports">
<h3>Relevant imports<a class="headerlink" href="#Relevant-imports" title="Link to this heading"></a></h3>
<p>To run this notebook, besides following the installation steps in the <a class="reference external" href="./README.md">README</a>, make sure to install <a class="reference external" href="https://jupyter.org/install">jupyter</a></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">rasterio</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">yaml</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">prithvi.Prithvi</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaskedAutoencoderViT</span>

<span class="n">NO_DATA</span> <span class="o">=</span> <span class="o">-</span><span class="mi">9999</span>
<span class="n">NO_DATA_FLOAT</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">PERCENTILES</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">99.9</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/gpfs/gibbs/project/dijk/ahf38/conda_envs/geo_comp2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
/gpfs/gibbs/project/dijk/ahf38/conda_envs/geo_comp2/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /gpfs/gibbs/project/dijk/ahf38/conda_envs/geo_comp2/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f&#34;Failed to load image Python extension: {e}&#34;)
</pre></div></div>
</div>
</section>
<section id="Define-some-functions-for-visualization">
<h3>Define some functions for visualization<a class="headerlink" href="#Define-some-functions-for-visualization" title="Link to this heading"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">load_raster</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">rasterio</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="k">as</span> <span class="n">src</span><span class="p">:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;img.shape: &#39;</span><span class="p">,</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># load first 6 bands</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>

        <span class="c1"># Handling No Data Values</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">img</span> <span class="o">==</span> <span class="n">NO_DATA</span><span class="p">,</span> <span class="n">NO_DATA_FLOAT</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>

        <span class="c1"># Cropping</span>
        <span class="k">if</span> <span class="n">crop</span><span class="p">:</span>
            <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:,</span> <span class="o">-</span><span class="n">crop</span><span class="p">[</span><span class="mi">0</span><span class="p">]:,</span> <span class="o">-</span><span class="n">crop</span><span class="p">[</span><span class="mi">1</span><span class="p">]:]</span>
    <span class="k">return</span> <span class="n">img</span>

<span class="k">def</span><span class="w"> </span><span class="nf">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">raster</span><span class="p">,</span> <span class="n">ref_img</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ref_img</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ref_img</span> <span class="o">=</span> <span class="n">raster</span>
    <span class="n">channels</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Loop through each channel (band) in the raster</span>
    <span class="k">for</span> <span class="n">channel</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">raster</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">valid_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">ref_img</span><span class="p">[</span><span class="n">channel</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">valid_mask</span><span class="p">[</span><span class="n">ref_img</span><span class="p">[</span><span class="n">channel</span><span class="p">]</span> <span class="o">==</span> <span class="n">NO_DATA_FLOAT</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">ref_img</span><span class="p">[</span><span class="n">channel</span><span class="p">][</span><span class="n">valid_mask</span><span class="p">],</span> <span class="n">PERCENTILES</span><span class="p">)</span> <span class="c1"># Calculate the minimum and maximum values at specified percentiles from the valid data</span>
        <span class="n">normalized_raster</span> <span class="o">=</span> <span class="p">(</span><span class="n">raster</span><span class="p">[</span><span class="n">channel</span><span class="p">]</span> <span class="o">-</span> <span class="n">mins</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">maxs</span> <span class="o">-</span> <span class="n">mins</span><span class="p">)</span> <span class="c1"># Normalize the raster channel to the range [0, 1] using the calculated mins and maxs</span>
        <span class="n">normalized_raster</span><span class="p">[</span><span class="o">~</span><span class="n">valid_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Set the pixels that are not valid to 0 in the normalized raster</span>
        <span class="n">clipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">normalized_raster</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Clip the values to ensure they are within the range [0, 1]</span>
        <span class="n">channels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clipped</span><span class="p">)</span>
    <span class="n">clipped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span>
    <span class="n">channels_last</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">clipped</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">rgb</span> <span class="o">=</span> <span class="n">channels_last</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">rgb</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plot_image_mask_reconstruction</span><span class="p">(</span><span class="n">normalized</span><span class="p">,</span> <span class="n">mask_img</span><span class="p">,</span> <span class="n">pred_img</span><span class="p">):</span>
    <span class="c1"># Mix visible and predicted patches</span>
    <span class="n">rec_img</span> <span class="o">=</span> <span class="n">normalized</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">rec_img</span><span class="p">[</span><span class="n">mask_img</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_img</span><span class="p">[</span><span class="n">mask_img</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># binary mask: 0 is keep, 1 is remove. Masked regions are replaced by &#39;pred&#39; values</span>

    <span class="n">mask_img_np</span> <span class="o">=</span> <span class="n">mask_img</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span>

    <span class="n">rec_img_np</span> <span class="o">=</span> <span class="p">(</span><span class="n">rec_img</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span> <span class="o">*</span> <span class="n">stds</span><span class="p">)</span> <span class="o">+</span> <span class="n">means</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">subplot</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
        <span class="n">subplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data</span><span class="p">))</span>
    <span class="n">masked_img_np</span> <span class="o">=</span> <span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">masked_img_np</span><span class="p">[</span><span class="n">mask_img_np</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">masked_img_np</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">rec_img_np</span><span class="p">,</span> <span class="n">ref_img</span><span class="o">=</span><span class="n">input_data</span><span class="p">))</span>
</pre></div>
</div>
</div>
</section>
<section id="Loading-the-model">
<h3>Loading the model<a class="headerlink" href="#Loading-the-model" title="Link to this heading"></a></h3>
<p>Assuming you have the relevant files under this directory</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights</span>
<span class="n">weights_path</span> <span class="o">=</span> <span class="s2">&quot;./prithvi/Prithvi_100M.pt&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">weights_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># read model config</span>
<span class="n">model_cfg_path</span> <span class="o">=</span> <span class="s2">&quot;./prithvi/Prithvi_100M_config.yaml&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_cfg_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">model_config</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">model_args</span><span class="p">,</span> <span class="n">train_args</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">[</span><span class="s2">&quot;model_args&quot;</span><span class="p">],</span> <span class="n">model_config</span><span class="p">[</span><span class="s2">&quot;train_params&quot;</span><span class="p">]</span>

<span class="c1"># let us use only 1 frame for now (the model was trained on 3 frames)</span>
<span class="n">model_args</span><span class="p">[</span><span class="s2">&quot;num_frames&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># instantiate model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MaskedAutoencoderViT</span><span class="p">(</span><span class="o">**</span><span class="n">model_args</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># load weights into model</span>
<span class="c1"># strict=false since we are loading with only 1 frame, but the warning is expected</span>
<span class="k">del</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;pos_embed&#39;</span><span class="p">]</span>
<span class="k">del</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;decoder_pos_embed&#39;</span><span class="p">]</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print(model)</span>
</pre></div>
</div>
</div>
</section>
<section id="Let's-try-it-out!">
<h3>Let’s try it out!<a class="headerlink" href="#Let's-try-it-out!" title="Link to this heading"></a></h3>
<p>We can access the images directly from the HuggingFace space thanks to rasterio</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">raster_path</span> <span class="o">=</span> <span class="s2">&quot;https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-demo/resolve/main/HLS.L30.T13REN.2018013T172747.v2.0.B02.B03.B04.B05.B06.B07_cropped.tif&quot;</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="n">raster_path</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input data shape is </span><span class="si">{</span><span class="n">input_data</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">raster_for_visualization</span> <span class="o">=</span> <span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">raster_for_visualization</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
img.shape:  (6, 500, 540)
Input data shape is (6, 224, 224)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.image.AxesImage at 0x146a9d76e3a0&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_foundation_model_IIASA2024_13_2.png" src="../_images/CASESTUDY_foundation_model_IIASA2024_13_2.png" />
</div>
</div>
<section id="Lets-call-the-model!">
<h4>Lets call the model!<a class="headerlink" href="#Lets-call-the-model!" title="Link to this heading"></a></h4>
<p>We pass:</p>
<ul class="simple">
<li><p>The normalized input image, cropped to size (224, 224)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mask_ratio</span></code>: The proportion of pixels that will be masked</p></li>
</ul>
<p>The model returns a tuple with:</p>
<ul class="simple">
<li><p>loss</p></li>
<li><p>reconstructed image</p></li>
<li><p>mask used</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># statistics used to normalize images before passing to the model</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_args</span><span class="p">[</span><span class="s2">&quot;data_mean&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_args</span><span class="p">[</span><span class="s2">&quot;data_std&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">preprocess_image</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="c1"># normalize image</span>
    <span class="n">normalized</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">normalized</span> <span class="o">=</span> <span class="p">((</span><span class="n">image</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span> <span class="o">/</span> <span class="n">stds</span><span class="p">)</span>
    <span class="n">normalized</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">normalized</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalized</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">normalized</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">normalized</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">normalized</span> <span class="o">=</span> <span class="n">preprocess_image</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">mask_ratio</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">normalized</span><span class="p">,</span> <span class="n">mask_ratio</span><span class="o">=</span><span class="n">mask_ratio</span><span class="p">)</span>

    <span class="c1"># Let&#39;s take a look at the shape of the model&#39;s output</span>
    <span class="c1"># This is the flat array of patches. Number of patches: image size=(224x224); patch size=(16x16); number of patches = (224/16)^2=196</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;pred.shape: &#39;</span><span class="p">,</span><span class="n">pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># [batch, patches, dimmension]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;mask.shape: &#39;</span><span class="p">,</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Undo the patching, back to the original pixel space</span>
    <span class="n">mask_img</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">unpatchify</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">pred_img</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">unpatchify</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
pred.shape:  torch.Size([1, 196, 1536])
mask.shape:  torch.Size([1, 196])
</pre></div></div>
</div>
</section>
<section id="Lets-use-these-to-build-a-nice-output-visualization">
<h4>Lets use these to build a nice output visualization<a class="headerlink" href="#Lets-use-these-to-build-a-nice-output-visualization" title="Link to this heading"></a></h4>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_image_mask_reconstruction</span><span class="p">(</span><span class="n">normalized</span><span class="p">,</span> <span class="n">mask_img</span><span class="p">,</span> <span class="n">pred_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_foundation_model_IIASA2024_18_0.png" src="../_images/CASESTUDY_foundation_model_IIASA2024_18_0.png" />
</div>
</div>
</section>
</section>
</section>
<section id="Inference-with-finetuned-Prithvi">
<h2>Inference with finetuned Prithvi<a class="headerlink" href="#Inference-with-finetuned-Prithvi" title="Link to this heading"></a></h2>
<p>This time, lets use the huggingface hub library to directly download the files for the finetuned model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># %pip install huggingface_hub</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">mmcv</span><span class="w"> </span><span class="kn">import</span> <span class="n">Config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mmseg.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_segmentor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mmseg.datasets.pipelines</span><span class="w"> </span><span class="kn">import</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">LoadImageFromFile</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mmseg.apis</span><span class="w"> </span><span class="kn">import</span> <span class="n">init_segmentor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">model_inference</span><span class="w"> </span><span class="kn">import</span> <span class="n">inference_segmentor</span><span class="p">,</span> <span class="n">process_test_pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">hf_hub_download</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grab the config and model weights from huggingface</span>
<span class="n">config_path</span><span class="o">=</span><span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;ibm-nasa-geospatial/Prithvi-100M-sen1floods11&quot;</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;sen1floods11_Prithvi_100M.py&quot;</span><span class="p">)</span>
<span class="n">ckpt</span><span class="o">=</span><span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;ibm-nasa-geospatial/Prithvi-100M-sen1floods11&quot;</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;sen1floods11_Prithvi_100M.pth&#39;</span><span class="p">)</span>
<span class="c1"># finetuned_model = init_segmentor(Config.fromfile(config_path), ckpt, device=&quot;cpu&quot;)</span>
<span class="n">finetuned_model</span> <span class="o">=</span> <span class="n">init_segmentor</span><span class="p">(</span><span class="n">Config</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="n">config_path</span><span class="p">),</span> <span class="n">ckpt</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/gpfs/gibbs/project/dijk/ahf38/conda_envs/geo_comp2/lib/python3.8/site-packages/mmseg/models/decode_heads/decode_head.py:104: UserWarning: For binary segmentation, we suggest using`out_channels = 1` to define the outputchannels of segmentor, and use `threshold`to convert seg_logist into a predictionapplying a threshold
  warnings.warn(&#39;For binary segmentation, we suggest using&#39;
/gpfs/gibbs/project/dijk/ahf38/conda_envs/geo_comp2/lib/python3.8/site-packages/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  warnings.warn(
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
load checkpoint from local path: /home/ahf38/.cache/huggingface/hub/models--ibm-nasa-geospatial--Prithvi-100M-sen1floods11/snapshots/220f62f00f6a31a70daac7babf139e4bf265f1c0/sen1floods11_Prithvi_100M.pth
</pre></div></div>
</div>
<section id="Let's-grab-an-image-to-do-inference-on">
<h3>Let’s grab an image to do inference on<a class="headerlink" href="#Let's-grab-an-image-to-do-inference-on" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">!wget</span> <span class="pre">https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-sen1floods11-demo/resolve/main/Spain_7370579_S2Hand.tif</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">!wget</span> <span class="pre">https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-sen1floods11-demo/resolve/main/India_900498_S2Hand.tif.tif</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">!wget</span> <span class="pre">https://github.com/cloudtostreet/Sen1Floods11/blob/master/sample/S1/Spain_7370579_S1Hand.tif</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will load and print the imge we want to do inference with</span>
<span class="n">input_data_inference</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="s2">&quot;Spain_7370579_S2Hand.tif&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image input shape is </span><span class="si">{</span><span class="n">input_data_inference</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">raster_for_visualization</span> <span class="o">=</span> <span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">raster_for_visualization</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
img.shape:  (13, 512, 512)
Image input shape is (6, 512, 512)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.image.AxesImage at 0x152dd56c4280&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_foundation_model_IIASA2024_25_2.png" src="../_images/CASESTUDY_foundation_model_IIASA2024_25_2.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s take a look at the definition of the model&#39;s pipeline</span>
<span class="n">custom_test_pipeline</span> <span class="o">=</span> <span class="n">process_test_pipeline</span><span class="p">(</span><span class="n">finetuned_model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">pipeline</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;custom_test_pipeline: &#39;</span><span class="p">,</span><span class="n">custom_test_pipeline</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">inference_segmentor</span><span class="p">(</span><span class="n">finetuned_model</span><span class="p">,</span> <span class="s2">&quot;Spain_7370579_S2Hand.tif&quot;</span><span class="p">,</span> <span class="n">custom_test_pipeline</span><span class="o">=</span><span class="n">custom_test_pipeline</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
custom_test_pipeline:  [{&#39;type&#39;: &#39;LoadGeospatialImageFromFile&#39;, &#39;to_float32&#39;: False, &#39;nodata&#39;: -9999, &#39;nodata_replace&#39;: 0}, {&#39;type&#39;: &#39;BandsExtract&#39;, &#39;bands&#39;: [1, 2, 3, 8, 11, 12]}, {&#39;type&#39;: &#39;ConstantMultiply&#39;, &#39;constant&#39;: 0.0001}, {&#39;type&#39;: &#39;ToTensor&#39;, &#39;keys&#39;: [&#39;img&#39;]}, {&#39;type&#39;: &#39;TorchPermute&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;order&#39;: (2, 0, 1)}, {&#39;type&#39;: &#39;TorchNormalize&#39;, &#39;means&#39;: [0.14245495, 0.13921481, 0.12434631, 0.31420089, 0.20743526, 0.12046503], &#39;stds&#39;: [0.04036231, 0.04186983, 0.05267646, 0.0822221, 0.06834774, 0.05294205]}, {&#39;type&#39;: &#39;Reshape&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;new_shape&#39;: (6, 1, -1, -1), &#39;look_up&#39;: {&#39;2&#39;: 1, &#39;3&#39;: 2}}, {&#39;type&#39;: &#39;CastTensor&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;new_type&#39;: &#39;torch.FloatTensor&#39;}, {&#39;type&#39;: &#39;CollectTestList&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;meta_keys&#39;: [&#39;img_info&#39;, &#39;filename&#39;, &#39;ori_filename&#39;, &#39;img&#39;, &#39;img_shape&#39;, &#39;ori_shape&#39;, &#39;pad_shape&#39;, &#39;scale_factor&#39;, &#39;img_norm_cfg&#39;]}]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The output of the model is a binary mask of same size of the input image</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;result[0].shape: &#39;</span><span class="p">,</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
result[0].shape:  (512, 512)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s take a look at the model&#39;s prediction</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">input_data_inference</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="s2">&quot;Spain_7370579_S2Hand.tif&quot;</span><span class="p">)</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">))</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;jet&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="k">for</span> <span class="n">subplot</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">subplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
img.shape:  (13, 512, 512)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_foundation_model_IIASA2024_28_1.png" src="../_images/CASESTUDY_foundation_model_IIASA2024_28_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inference with a second image</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;USA_430764_S2Hand.tif&quot;</span>
<span class="n">input_data_inference</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image input shape is </span><span class="si">{</span><span class="n">input_data_inference</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">raster_for_visualization</span> <span class="o">=</span> <span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">raster_for_visualization</span><span class="p">)</span>
<span class="c1"># adapt this pipeline for Tif files with &gt; 3 images</span>
<span class="n">custom_test_pipeline</span> <span class="o">=</span> <span class="n">process_test_pipeline</span><span class="p">(</span><span class="n">finetuned_model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">pipeline</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;custom_test_pipeline: &#39;</span><span class="p">,</span><span class="n">custom_test_pipeline</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">inference_segmentor</span><span class="p">(</span><span class="n">finetuned_model</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">custom_test_pipeline</span><span class="o">=</span><span class="n">custom_test_pipeline</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">input_data_inference</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">))</span>
<span class="c1"># ax[1].imshow(result[0], norm=norm, cmap=&quot;jet&quot;)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">))</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;jet&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="k">for</span> <span class="n">subplot</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">subplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
img.shape:  (13, 512, 512)
Image input shape is (6, 512, 512)
custom_test_pipeline:  [{&#39;type&#39;: &#39;LoadGeospatialImageFromFile&#39;, &#39;to_float32&#39;: False, &#39;nodata&#39;: -9999, &#39;nodata_replace&#39;: 0}, {&#39;type&#39;: &#39;BandsExtract&#39;, &#39;bands&#39;: [1, 2, 3, 8, 11, 12]}, {&#39;type&#39;: &#39;ConstantMultiply&#39;, &#39;constant&#39;: 0.0001}, {&#39;type&#39;: &#39;ToTensor&#39;, &#39;keys&#39;: [&#39;img&#39;]}, {&#39;type&#39;: &#39;TorchPermute&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;order&#39;: (2, 0, 1)}, {&#39;type&#39;: &#39;TorchNormalize&#39;, &#39;means&#39;: [0.14245495, 0.13921481, 0.12434631, 0.31420089, 0.20743526, 0.12046503], &#39;stds&#39;: [0.04036231, 0.04186983, 0.05267646, 0.0822221, 0.06834774, 0.05294205]}, {&#39;type&#39;: &#39;Reshape&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;new_shape&#39;: (6, 1, -1, -1), &#39;look_up&#39;: {&#39;2&#39;: 1, &#39;3&#39;: 2}}, {&#39;type&#39;: &#39;CastTensor&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;new_type&#39;: &#39;torch.FloatTensor&#39;}, {&#39;type&#39;: &#39;CollectTestList&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;meta_keys&#39;: [&#39;img_info&#39;, &#39;filename&#39;, &#39;ori_filename&#39;, &#39;img&#39;, &#39;img_shape&#39;, &#39;ori_shape&#39;, &#39;pad_shape&#39;, &#39;scale_factor&#39;, &#39;img_norm_cfg&#39;]}]
img.shape:  (13, 512, 512)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_foundation_model_IIASA2024_29_1.png" src="../_images/CASESTUDY_foundation_model_IIASA2024_29_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_foundation_model_IIASA2024_29_2.png" src="../_images/CASESTUDY_foundation_model_IIASA2024_29_2.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inference with the a third image</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;India_900498_S2Hand.tif&quot;</span>
<span class="n">input_data_inference</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image input shape is </span><span class="si">{</span><span class="n">input_data_inference</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">raster_for_visualization</span> <span class="o">=</span> <span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">raster_for_visualization</span><span class="p">)</span>
<span class="c1"># adapt this pipeline for Tif files with &gt; 3 images</span>
<span class="n">custom_test_pipeline</span> <span class="o">=</span> <span class="n">process_test_pipeline</span><span class="p">(</span><span class="n">finetuned_model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">pipeline</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;custom_test_pipeline: &#39;</span><span class="p">,</span><span class="n">custom_test_pipeline</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">inference_segmentor</span><span class="p">(</span><span class="n">finetuned_model</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">custom_test_pipeline</span><span class="o">=</span><span class="n">custom_test_pipeline</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">input_data_inference</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">))</span>
<span class="c1"># ax[1].imshow(result[0], norm=norm, cmap=&quot;jet&quot;)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">))</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;jet&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="k">for</span> <span class="n">subplot</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">subplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
img.shape:  (13, 512, 512)
Image input shape is (6, 512, 512)
custom_test_pipeline:  [{&#39;type&#39;: &#39;LoadGeospatialImageFromFile&#39;, &#39;to_float32&#39;: False, &#39;nodata&#39;: -9999, &#39;nodata_replace&#39;: 0}, {&#39;type&#39;: &#39;BandsExtract&#39;, &#39;bands&#39;: [1, 2, 3, 8, 11, 12]}, {&#39;type&#39;: &#39;ConstantMultiply&#39;, &#39;constant&#39;: 0.0001}, {&#39;type&#39;: &#39;ToTensor&#39;, &#39;keys&#39;: [&#39;img&#39;]}, {&#39;type&#39;: &#39;TorchPermute&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;order&#39;: (2, 0, 1)}, {&#39;type&#39;: &#39;TorchNormalize&#39;, &#39;means&#39;: [0.14245495, 0.13921481, 0.12434631, 0.31420089, 0.20743526, 0.12046503], &#39;stds&#39;: [0.04036231, 0.04186983, 0.05267646, 0.0822221, 0.06834774, 0.05294205]}, {&#39;type&#39;: &#39;Reshape&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;new_shape&#39;: (6, 1, -1, -1), &#39;look_up&#39;: {&#39;2&#39;: 1, &#39;3&#39;: 2}}, {&#39;type&#39;: &#39;CastTensor&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;new_type&#39;: &#39;torch.FloatTensor&#39;}, {&#39;type&#39;: &#39;CollectTestList&#39;, &#39;keys&#39;: [&#39;img&#39;], &#39;meta_keys&#39;: [&#39;img_info&#39;, &#39;filename&#39;, &#39;ori_filename&#39;, &#39;img&#39;, &#39;img_shape&#39;, &#39;ori_shape&#39;, &#39;pad_shape&#39;, &#39;scale_factor&#39;, &#39;img_norm_cfg&#39;]}]
img.shape:  (13, 512, 512)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_foundation_model_IIASA2024_30_1.png" src="../_images/CASESTUDY_foundation_model_IIASA2024_30_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_foundation_model_IIASA2024_30_2.png" src="../_images/CASESTUDY_foundation_model_IIASA2024_30_2.png" />
</div>
</div>
</section>
</section>
<section id="id1">
<h2>Inference with finetuned Prithvi<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<p>Let’s explore a second finetuned model - Crop Classification</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">config_path</span><span class="o">=</span><span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification&quot;</span><span class="p">,</span>
                            <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;multi_temporal_crop_classification_Prithvi_100M.py&quot;</span><span class="p">)</span>
<span class="n">ckpt</span><span class="o">=</span><span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification&quot;</span><span class="p">,</span>
                     <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;multi_temporal_crop_classification_Prithvi_100M.pth&#39;</span><span class="p">)</span>
<span class="n">finetuned_model</span> <span class="o">=</span> <span class="n">init_segmentor</span><span class="p">(</span><span class="n">Config</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="n">config_path</span><span class="p">),</span> <span class="n">ckpt</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
load checkpoint from local path: /home/ahf38/.cache/huggingface/hub/models--ibm-nasa-geospatial--Prithvi-100M-multi-temporal-crop-classification/snapshots/3b8de5aa922c79b4cf69d497732fcf22f0edd8c6/multi_temporal_crop_classification_Prithvi_100M.pth
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[40]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load a sample image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.patches</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mpatches</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;chip_102_345_merged.tif&quot;</span>
<span class="n">input_data_inference</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image input shape is </span><span class="si">{</span><span class="n">input_data_inference</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># adapt this pipeline for Tif files with &gt; 3 images</span>
<span class="n">custom_test_pipeline</span> <span class="o">=</span> <span class="n">process_test_pipeline</span><span class="p">(</span><span class="n">finetuned_model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">pipeline</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">inference_segmentor</span><span class="p">(</span><span class="n">finetuned_model</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">custom_test_pipeline</span><span class="o">=</span><span class="n">custom_test_pipeline</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;result.shape: &#39;</span><span class="p">,</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">input_data_inference</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;tab20&quot;</span><span class="p">)</span>
<span class="c1"># ax[2].imshow(enhance_raster_for_visualization(input_data_inference))</span>
<span class="c1"># ax[2].imshow(result[0], cmap=&quot;jet&quot;, alpha=0.3, norm=norm)</span>
<span class="k">for</span> <span class="n">subplot</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">subplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="c1"># Turn off axis for all subplots</span>
<span class="k">for</span> <span class="n">subplot</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">subplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="c1"># Define the legend handles</span>
<span class="n">legend_labels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Natural Vegetation&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Forest&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Corn&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Soybeans&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Wetlands&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Developed/Barren&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Open Water&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Winter Wheat&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Alfalfa&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Fallow/Idle Cropland&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Cotton&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Sorghum&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Other&quot;</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">tab20</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">13</span><span class="p">)]</span>
<span class="n">handles</span> <span class="o">=</span> <span class="p">[</span><span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">legend_labels</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">13</span><span class="p">)]</span>

<span class="c1"># Add the legend to ax[2]</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">handles</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span><span class="n">bbox_to_anchor</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
img.shape:  (18, 224, 224)
Image input shape is (6, 224, 224)
result.shape:  (224, 224)
img.shape:  (18, 224, 224)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[40]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.legend.Legend at 0x146a6436be20&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_foundation_model_IIASA2024_33_2.png" src="../_images/CASESTUDY_foundation_model_IIASA2024_33_2.png" />
</div>
</div>
</section>
</section>
<section id="Proposed-exercises">
<h1>Proposed exercises<a class="headerlink" href="#Proposed-exercises" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p>Use some of the images listed below to run the flood prediction model and/or the crop classification models</p></li>
<li><p>Download a new image for your region and run these models. Does it match your expectation?</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Images download from the demo (https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-demo)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">input_data_inference</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="s2">&quot;./temporal/HLS.L30.T18TVL.2018029T154533.v2.0.B02.B03.B04.B05.B06.B07_cropped.tif&quot;</span><span class="p">)</span>
<span class="n">input_data_inference2</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="s2">&quot;./temporal/HLS.L30.T18TVL.2018141T154435.v2.0.B02.B03.B04.B05.B06.B07_cropped.tif&quot;</span><span class="p">)</span>
<span class="n">input_data_inference3</span> <span class="o">=</span> <span class="n">load_raster</span><span class="p">(</span><span class="s2">&quot;./temporal/HLS.L30.T18TVL.2018189T154446.v2.0.B02.B03.B04.B05.B06.B07_cropped.tif&quot;</span><span class="p">)</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference2</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">enhance_raster_for_visualization</span><span class="p">(</span><span class="n">input_data_inference3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.image.AxesImage at 0x148d7d86a4f0&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/CASESTUDY_foundation_model_IIASA2024_35_1.png" src="../_images/CASESTUDY_foundation_model_IIASA2024_35_1.png" />
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ADD YOUR CODE TO RUN INFERENCE HERE</span>
</pre></div>
</div>
</div>
<section id="Finetuning-for-your-use-case">
<h2>Finetuning for your use case<a class="headerlink" href="#Finetuning-for-your-use-case" title="Link to this heading"></a></h2>
<p>To finetune, you can now write a PyTorch loop as usual to train on your dataset. Simply extract the backbone from the model with some surgery and run only the model features forward, with no masking!</p>
<p>In general some reccomendations are:</p>
<ul class="simple">
<li><p>At least in the beggining, experiment with freezing the backbone. This will give you much faster iteration through experiments.</p></li>
<li><p>Err on the side of a smaller learning rate</p></li>
<li><p>With an unfrozen encoder, regularization is your friend! (Weight decay, dropout, batchnorm…)</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># if going with plain pytorch:</span>
<span class="c1"># - remember to normalize images beforehand (find the normalization statistics in the config file)</span>
<span class="c1"># - turn off masking by passing mask_ratio = 0</span>
<span class="n">normalized</span> <span class="o">=</span> <span class="n">preprocess_image</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="n">features</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward_encoder</span><span class="p">(</span><span class="n">normalized</span><span class="p">,</span> <span class="n">mask_ratio</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;features.shape: &#39;</span><span class="p">,</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
features.shape:  torch.Size([1, 197, 768])
</pre></div></div>
</div>
<p>These are the standard output of a ViT.</p>
<ul class="simple">
<li><p>Dim 1: Batch size</p></li>
<li><p>Dim 2: [<code class="docutils literal notranslate"><span class="pre">cls_token</span></code>] + tokens representing flattened image</p></li>
<li><p>Dim 3: embedding dimension</p></li>
</ul>
<p>First reshape features into “image-like” shape:</p>
<ul class="simple">
<li><p>Drop cls_token</p></li>
<li><p>reshape into HxW shape</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encoder features have shape </span><span class="si">{</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># drop cls token</span>
<span class="n">reshaped_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>

<span class="c1"># reshape</span>
<span class="n">feature_img_side_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">reshaped_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">reshaped_features</span> <span class="o">=</span> <span class="n">reshaped_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">feature_img_side_length</span><span class="p">,</span> <span class="n">feature_img_side_length</span><span class="p">,</span> <span class="n">model_args</span><span class="p">[</span><span class="s2">&quot;embed_dim&quot;</span><span class="p">])</span>
<span class="c1"># channels first</span>
<span class="n">reshaped_features</span> <span class="o">=</span> <span class="n">reshaped_features</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encoder features have new shape </span><span class="si">{</span><span class="n">reshaped_features</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Encoder features have shape torch.Size([1, 197, 768])
Encoder features have new shape torch.Size([1, 768, 14, 14])
</pre></div></div>
</div>
<p>A simple segmentation head can consist of a few upscaling blocks + a final head for classification</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">upscaling_block</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="n">embed_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_args</span><span class="p">[</span><span class="s2">&quot;embed_dim&quot;</span><span class="p">]</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
<span class="n">segmentation_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="o">*</span><span class="p">[</span>
    <span class="n">upscaling_block</span><span class="p">(</span><span class="n">embed_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">embed_dims</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="n">embed_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">))</span>
</pre></div>
</div>
</div>
<section id="Running-features-through-the-segmentation-head">
<h3>Running features through the segmentation head<a class="headerlink" href="#Running-features-through-the-segmentation-head" title="Link to this heading"></a></h3>
<p>We now get an output of shape [batch_size, num_classes, height, width]</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;len(embed_dims): &#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">embed_dims</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;segmentation_head: &#39;</span><span class="p">,</span><span class="n">segmentation_head</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;segmentation_head(reshaped_features).shape: &#39;</span><span class="p">,</span> <span class="n">segmentation_head</span><span class="p">(</span><span class="n">reshaped_features</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
len(embed_dims):  5
segmentation_head:  Sequential(
  (0): Sequential(
    (0): Upsample(scale_factor=2.0, mode=nearest)
    (1): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): ReLU()
  )
  (1): Sequential(
    (0): Upsample(scale_factor=2.0, mode=nearest)
    (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): ReLU()
  )
  (2): Sequential(
    (0): Upsample(scale_factor=2.0, mode=nearest)
    (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): ReLU()
  )
  (3): Sequential(
    (0): Upsample(scale_factor=2.0, mode=nearest)
    (1): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): ReLU()
  )
  (4): Conv2d(48, 2, kernel_size=(1, 1), stride=(1, 1))
)
segmentation_head(reshaped_features).shape:  torch.Size([1, 2, 224, 224])
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># new_model = nn.Sequential(model, segmentation_head)</span>
<span class="c1"># print(new_model)</span>
</pre></div>
</div>
</div>
</section>
<section id="Finetuning---MMSeg">
<h3>Finetuning - MMSeg<a class="headerlink" href="#Finetuning---MMSeg" title="Link to this heading"></a></h3>
<p>Alternatively, finetune using the MMSegmentation extension we have opensourced.</p>
<ul class="simple">
<li><p>No model surgery required</p></li>
<li><p>No need to write boilerplate training code</p></li>
<li><p>Integrations with Tensorboard, MLFlow, …</p></li>
<li><p>Segmentation evaluation metrics / losses built in</p></li>
</ul>
<ol class="arabic simple">
<li><p>Build your config file. Look <a class="reference external" href="./configs/">here</a> for examples, the <a class="reference external" href="./README.md">ReadME</a> for some docs and <a class="reference external" href="https://mmsegmentation.readthedocs.io/en/0.x/tutorials/config.html">MMSeg</a> for more general tutorials.</p></li>
<li><p>Collect your dataset in the format determined by MMSeg</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mim</span> <span class="pre">train</span> <span class="pre">mmsegmentation</span> <span class="pre">&lt;path</span> <span class="pre">to</span> <span class="pre">my</span> <span class="pre">config&gt;</span></code></p></li>
</ol>
<p>This is what the model looks like in the MMSeg configuration code.</p>
<p>All this composition we did above is done for you!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;TemporalEncoderDecoder&quot;</span><span class="p">,</span>
    <span class="n">frozen_backbone</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">backbone</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;TemporalViTEncoder&quot;</span><span class="p">,</span>
        <span class="n">pretrained</span><span class="o">=</span><span class="n">pretrained_weights_path</span><span class="p">,</span>
        <span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">,</span>
        <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
        <span class="n">num_frames</span><span class="o">=</span><span class="n">num_frames</span><span class="p">,</span>
        <span class="n">tubelet_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">in_chans</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">bands</span><span class="p">),</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">depth</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span>
        <span class="n">norm_pix_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">neck</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;ConvTransformerTokensToEmbeddingNeck&quot;</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="n">num_frames</span><span class="o">*</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">output_embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">drop_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">Hp</span><span class="o">=</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">,</span>
        <span class="n">Wp</span><span class="o">=</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">decode_head</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;FCNHead&quot;</span><span class="p">,</span>
        <span class="n">in_index</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span>
        <span class="n">channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">num_convs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">concat_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">dropout_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">norm_cfg</span><span class="o">=</span><span class="n">norm_cfg</span><span class="p">,</span>
        <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">loss_decode</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;CrossEntropyLoss&quot;</span><span class="p">,</span>
            <span class="n">use_sigmoid</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">loss_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">ce_weights</span><span class="p">,</span>
            <span class="n">avg_non_ignore</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">),</span>
    <span class="p">),</span>
    <span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="CNN_satelite_2023.html" class="btn btn-neutral float-left" title="Using Multi-layer Perceptron and Convolutional Neural Networks for Satellite image classification - 2023" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="NN_Unsupervised_2024.html" class="btn btn-neutral float-right" title="Autoencoder (AE), Variational Autoencoder (VAE)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Giuseppe Amatulli.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>